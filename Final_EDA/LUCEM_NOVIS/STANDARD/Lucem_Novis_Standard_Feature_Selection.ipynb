{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52c5e7a8-ae35-4fda-a757-0950c338dc14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Book 9: Lucem Novis Standard Feature Selection Pipeline\n\n> **Lucem Novis variant**: Visit history features (Book 6, `visit_` prefix) are excluded from this pipeline. All other features are included. See Mercy Standard for the full-feature version.\n \n## Hybrid Two-Phase Approach\n \n| Phase | Method | Purpose |\n|-------|--------|---------|\n| **Phase 1** | Cluster-Based Reduction | Remove redundant/correlated features |\n| **Phase 2** | Iterative SHAP Winnowing | Iterative removal via 2-of-3 criteria |\n| **Phase 3** | CV Stability Analysis | Validate selection across 5 folds |\n| **Phase 4** | Parsimony-Aware Selection | Automated best-iteration pick + CV filter |\n| **Phase 5** | Production Model Training | Final model with relaxed XGBoost params |\n \n## Key Features\n- **Dynamic clustering threshold** via silhouette score (not fixed 0.7)\n- **2:1 SHAP weighting** for positive cases (model handles imbalance via scale_pos_weight)\n- **5-fold CV stability analysis** - identify features robust across different train/val splits\n- **Test set holdout** - test metrics evaluated only once at final model (no peeking)\n- **Granular checkpoints** - stop anytime and resume without starting over\n- **Automatic validation gates** - stops when performance degrades\n \n## Checkpoint System\nCheckpoints saved after each step using portable formats (JSON for models, Parquet for DataFrames).\nKill notebook anytime, re-run to resume. Backward compatible with legacy pickle checkpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa108e27-cd60-46b7-aa97-028c53bef37b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d24c6c3b-e6f9-4b4f-8f56-43f6b76abf91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION - Modify these as needed\n",
    "# ============================================================================\n",
    "\n",
    "# Resume behavior: Set to True to check for and resume from checkpoints\n",
    "# Set to False to start fresh (will prompt to confirm clearing checkpoints)\n",
    "AUTO_RESUME = True\n",
    "\n",
    "# =============================================================================\n",
    "# PHASE 2 REMOVAL CRITERIA (from original CRC_ITER1_MODEL-PREVALENCE.py)\n",
    "# =============================================================================\n",
    "# Feature must meet AT LEAST 2 of 3 criteria to be eligible for removal:\n",
    "#   1. Near-zero SHAP importance (< ZERO_SHAP_THRESHOLD)\n",
    "#   2. Negative-biased ratio (< NEG_BIAS_RATIO_THRESHOLD)\n",
    "#   3. Bottom percentile by SHAP (< BOTTOM_PERCENTILE)\n",
    "# =============================================================================\n",
    "ZERO_SHAP_THRESHOLD = 0.0005     # Near-zero importance threshold (raised from 0.0002)\n",
    "NEG_BIAS_RATIO_THRESHOLD = 0.25  # Negative-bias ratio threshold (raised from 0.15)\n",
    "BOTTOM_PERCENTILE = 12           # Bottom percentile threshold (raised from 8)\n",
    "\n",
    "# Iteration limits - gradual reduction to prevent model destabilization\n",
    "MAX_REMOVALS_EARLY = 10          # Cap for iterations 1-5 (was 30, caused crashes)\n",
    "MAX_REMOVALS_LATE = 5            # Cap for iterations 6+ (finer control)\n",
    "LATE_PHASE_ITERATION = 5         # Switch to finer control after this iteration\n",
    "MIN_FEATURES_THRESHOLD = 25      # Target final feature count\n",
    "\n",
    "# Phase 1 validation gate\n",
    "PHASE1_MAX_VAL_DROP = 0.10       # Allow up to 10% drop in Phase 1\n",
    "\n",
    "# Visualization thresholds (reference lines in plots, not used for stopping)\n",
    "MAX_VAL_AUPRC_DROP = 0.05        # 5% drop reference line\n",
    "MAX_GAP_INCREASE = 0.02          # Gap increase reference line\n",
    "\n",
    "# Clustering configuration\n",
    "# Use silhouette optimization, but constrain to reasonable range\n",
    "# Target: ~40-60 clusters for ~170 features (allows meaningful cluster reduction)\n",
    "MIN_CLUSTERING_THRESHOLD = 0.60  # Floor - prevents too many tiny clusters\n",
    "MAX_CLUSTERING_THRESHOLD = 0.85  # Ceiling - prevents too few giant clusters\n",
    "TARGET_CLUSTER_RANGE = (40, 70)  # Prefer thresholds giving this many clusters\n",
    "\n",
    "# Threshold search range\n",
    "THRESHOLD_MIN = 0.50\n",
    "THRESHOLD_MAX = 0.90\n",
    "THRESHOLD_STEP = 0.05\n",
    "\n",
    "# Random seed for reproducibility\n",
    "RANDOM_SEED = 217\n",
    "\n",
    "# Output directories\n",
    "CHECKPOINT_DIR = \"checkpoints\"\n",
    "OUTPUT_DIR = \"feature_selection_outputs\"\n",
    "\n",
    "# Number of CV folds for feature selection stability\n",
    "N_CV_FOLDS = 5\n",
    "CV_FEATURE_THRESHOLD = 0.60  # Keep features appearing in >= 60% of folds (3/5)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CONFIGURATION (Original Methodology)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"AUTO_RESUME: {AUTO_RESUME}\")\n",
    "print(f\"\\nRemoval Criteria (must meet 2+ of 3):\")\n",
    "print(f\"  ZERO_SHAP_THRESHOLD: {ZERO_SHAP_THRESHOLD}\")\n",
    "print(f\"  NEG_BIAS_RATIO_THRESHOLD: {NEG_BIAS_RATIO_THRESHOLD}\")\n",
    "print(f\"  BOTTOM_PERCENTILE: {BOTTOM_PERCENTILE}%\")\n",
    "print(f\"\\nIteration Limits:\")\n",
    "print(f\"  MAX_REMOVALS_EARLY (iter 1-{LATE_PHASE_ITERATION}): {MAX_REMOVALS_EARLY}\")\n",
    "print(f\"  MAX_REMOVALS_LATE (iter {LATE_PHASE_ITERATION+1}+): {MAX_REMOVALS_LATE}\")\n",
    "print(f\"  MIN_FEATURES_THRESHOLD: {MIN_FEATURES_THRESHOLD}\")\n",
    "print(f\"\\nClustering:\")\n",
    "print(f\"  MIN_CLUSTERING_THRESHOLD: {MIN_CLUSTERING_THRESHOLD}\")\n",
    "print(f\"  MAX_CLUSTERING_THRESHOLD: {MAX_CLUSTERING_THRESHOLD}\")\n",
    "print(f\"  TARGET_CLUSTER_RANGE: {TARGET_CLUSTER_RANGE}\")\n",
    "print(f\"\\nOther:\")\n",
    "print(f\"  RANDOM_SEED: {RANDOM_SEED}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3076388-7587-46a0-a63b-23af0ed120b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Imports and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8786f9f9-b2f0-4f8a-a3d0-2deb85ab7e9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram\n",
    "from scipy.spatial.distance import squareform\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score, average_precision_score, roc_auc_score, brier_score_loss\n",
    ")\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import shap\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Verify Parquet support (required for checkpoint system)\n",
    "try:\n",
    "    import pyarrow\n",
    "    print(f\"PyArrow version: {pyarrow.__version__}\")\n",
    "except ImportError:\n",
    "    raise ImportError(\n",
    "        \"PyArrow is required for Parquet checkpoint support. \"\n",
    "        \"Install with: pip install pyarrow\"\n",
    "    )\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"America/Chicago\")\n",
    "\n",
    "# Get catalog from environment\n",
    "trgt_cat = os.environ.get('trgt_cat', 'dev')\n",
    "\n",
    "# Track overall pipeline start time\n",
    "PIPELINE_START_TIME = time.time()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ENVIRONMENT INITIALIZED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Timestamp: {datetime.now()}\")\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Target catalog: {trgt_cat}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3b402b3-4163-4c20-b428-de0d48b3f22d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION - Modify these as needed\n",
    "# ============================================================================\n",
    "\n",
    "# Resume behavior: Set to True to check for and resume from checkpoints\n",
    "# Set to False to start fresh (will prompt to confirm clearing checkpoints)\n",
    "AUTO_RESUME = True\n",
    "\n",
    "# =============================================================================\n",
    "# PHASE 2 REMOVAL CRITERIA (from original CRC_ITER1_MODEL-PREVALENCE.py)\n",
    "# =============================================================================\n",
    "# Feature must meet AT LEAST 2 of 3 criteria to be eligible for removal:\n",
    "#   1. Near-zero SHAP importance (< ZERO_SHAP_THRESHOLD)\n",
    "#   2. Negative-biased ratio (< NEG_BIAS_RATIO_THRESHOLD)\n",
    "#   3. Bottom percentile by SHAP (< BOTTOM_PERCENTILE)\n",
    "# =============================================================================\n",
    "ZERO_SHAP_THRESHOLD = 0.0005     # Near-zero importance threshold (raised from 0.0002)\n",
    "NEG_BIAS_RATIO_THRESHOLD = 0.25  # Negative-bias ratio threshold (raised from 0.15)\n",
    "BOTTOM_PERCENTILE = 12           # Bottom percentile threshold (raised from 8)\n",
    "\n",
    "# Iteration limits - gradual reduction to prevent model destabilization\n",
    "MAX_REMOVALS_EARLY = 10          # Cap for iterations 1-5 (was 30, caused crashes)\n",
    "MAX_REMOVALS_LATE = 5            # Cap for iterations 6+ (finer control)\n",
    "LATE_PHASE_ITERATION = 5         # Switch to finer control after this iteration\n",
    "MIN_FEATURES_THRESHOLD = 25      # Target final feature count\n",
    "\n",
    "# Phase 1 validation gate\n",
    "PHASE1_MAX_VAL_DROP = 0.10       # Allow up to 10% drop in Phase 1\n",
    "\n",
    "# Visualization thresholds (reference lines in plots, not used for stopping)\n",
    "MAX_VAL_AUPRC_DROP = 0.05        # 5% drop reference line\n",
    "MAX_GAP_INCREASE = 0.02          # Gap increase reference line\n",
    "\n",
    "# Clustering configuration\n",
    "# Use silhouette optimization, but constrain to reasonable range\n",
    "# Target: ~40-60 clusters for ~170 features (allows meaningful cluster reduction)\n",
    "MIN_CLUSTERING_THRESHOLD = 0.60  # Floor - prevents too many tiny clusters\n",
    "MAX_CLUSTERING_THRESHOLD = 0.85  # Ceiling - prevents too few giant clusters\n",
    "TARGET_CLUSTER_RANGE = (40, 70)  # Prefer thresholds giving this many clusters\n",
    "\n",
    "# Threshold search range\n",
    "THRESHOLD_MIN = 0.50\n",
    "THRESHOLD_MAX = 0.90\n",
    "THRESHOLD_STEP = 0.05\n",
    "\n",
    "# Random seed for reproducibility\n",
    "RANDOM_SEED = 217\n",
    "\n",
    "# Output directories\n",
    "CHECKPOINT_DIR = \"checkpoints\"\n",
    "OUTPUT_DIR = \"feature_selection_outputs\"\n",
    "\n",
    "# Number of CV folds for feature selection stability\n",
    "N_CV_FOLDS = 5\n",
    "CV_FEATURE_THRESHOLD = 0.6  # Keep features appearing in >= 60% of folds (3/5)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CONFIGURATION (Original Methodology)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"AUTO_RESUME: {AUTO_RESUME}\")\n",
    "print(f\"\\nRemoval Criteria (must meet 2+ of 3):\")\n",
    "print(f\"  ZERO_SHAP_THRESHOLD: {ZERO_SHAP_THRESHOLD}\")\n",
    "print(f\"  NEG_BIAS_RATIO_THRESHOLD: {NEG_BIAS_RATIO_THRESHOLD}\")\n",
    "print(f\"  BOTTOM_PERCENTILE: {BOTTOM_PERCENTILE}%\")\n",
    "print(f\"\\nIteration Limits:\")\n",
    "print(f\"  MAX_REMOVALS_EARLY (iter 1-{LATE_PHASE_ITERATION}): {MAX_REMOVALS_EARLY}\")\n",
    "print(f\"  MAX_REMOVALS_LATE (iter {LATE_PHASE_ITERATION+1}+): {MAX_REMOVALS_LATE}\")\n",
    "print(f\"  MIN_FEATURES_THRESHOLD: {MIN_FEATURES_THRESHOLD}\")\n",
    "print(f\"\\nClustering:\")\n",
    "print(f\"  MIN_CLUSTERING_THRESHOLD: {MIN_CLUSTERING_THRESHOLD}\")\n",
    "print(f\"  MAX_CLUSTERING_THRESHOLD: {MAX_CLUSTERING_THRESHOLD}\")\n",
    "print(f\"  TARGET_CLUSTER_RANGE: {TARGET_CLUSTER_RANGE}\")\n",
    "print(f\"\\nOther:\")\n",
    "print(f\"  RANDOM_SEED: {RANDOM_SEED}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cf1d019-c099-4b71-a217-eb9a878c2a49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Checkpoint Management Functions\n",
    " \n",
    "Checkpoints use portable, version-stable formats:\n",
    "- **XGBoost models**: JSON format via `model.save_model()` (portable across XGBoost versions)\n",
    "- **DataFrames**: Parquet format (efficient, schema-preserving)\n",
    "- **NumPy arrays**: `.npy` format\n",
    "- **Simple types**: JSON format\n",
    "- **Complex objects**: Pickle fallback (scipy linkage matrices, etc.)\n",
    " \n",
    "Each checkpoint is stored as a directory with typed files + metadata.\n",
    "Legacy pickle checkpoints (`.pkl` files) are still supported for backward compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb70d128-6a46-4737-8316-ea5f4c2f5ce1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def ensure_directories():\n",
    "    \"\"\"Create checkpoint and output directories if they don't exist.\"\"\"\n",
    "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    print(f\"✓ Directories verified: {CHECKPOINT_DIR}/, {OUTPUT_DIR}/\")\n",
    "\n",
    "def get_checkpoint_dir(name):\n",
    "    \"\"\"Get directory path for a checkpoint (each checkpoint is a directory).\"\"\"\n",
    "    return os.path.join(CHECKPOINT_DIR, name)\n",
    "\n",
    "def save_checkpoint(name, data):\n",
    "    \"\"\"\n",
    "    Save a checkpoint with format-appropriate storage:\n",
    "    - XGBoost models: JSON format (portable across versions)\n",
    "    - DataFrames: Parquet format (efficient, portable)\n",
    "    - Other data: JSON for simple types, pickle for complex types\n",
    "    \"\"\"\n",
    "    checkpoint_dir = get_checkpoint_dir(name)\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    metadata = {'keys': []}\n",
    "\n",
    "    for key, value in data.items():\n",
    "        metadata['keys'].append(key)\n",
    "\n",
    "        if isinstance(value, XGBClassifier):\n",
    "            # Save XGBoost model as JSON\n",
    "            model_path = os.path.join(checkpoint_dir, f\"{key}.json\")\n",
    "            value.save_model(model_path)\n",
    "\n",
    "        elif isinstance(value, pd.DataFrame):\n",
    "            # Save DataFrame as Parquet\n",
    "            parquet_path = os.path.join(checkpoint_dir, f\"{key}.parquet\")\n",
    "            value.to_parquet(parquet_path, index=False)\n",
    "\n",
    "        elif isinstance(value, np.ndarray):\n",
    "            # Save numpy arrays as .npy\n",
    "            npy_path = os.path.join(checkpoint_dir, f\"{key}.npy\")\n",
    "            np.save(npy_path, value)\n",
    "\n",
    "        elif isinstance(value, (list, dict, str, int, float, bool, type(None))):\n",
    "            # Save simple types as JSON\n",
    "            json_path = os.path.join(checkpoint_dir, f\"{key}.json\")\n",
    "            with open(json_path, 'w') as f:\n",
    "                json.dump(value, f, indent=2, default=str)\n",
    "\n",
    "        else:\n",
    "            # Fallback to pickle for complex types (scipy linkage, etc.)\n",
    "            pkl_path = os.path.join(checkpoint_dir, f\"{key}.pkl\")\n",
    "            with open(pkl_path, 'wb') as f:\n",
    "                pickle.dump(value, f)\n",
    "\n",
    "    # Save metadata\n",
    "    with open(os.path.join(checkpoint_dir, \"_metadata.json\"), 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "\n",
    "    print(f\"✓ CHECKPOINT SAVED: {name}\")\n",
    "\n",
    "def load_checkpoint(name):\n",
    "    \"\"\"Load a checkpoint by name. Returns None if not found.\"\"\"\n",
    "    checkpoint_dir = get_checkpoint_dir(name)\n",
    "\n",
    "    # Check for new format (directory-based)\n",
    "    if os.path.isdir(checkpoint_dir):\n",
    "        metadata_path = os.path.join(checkpoint_dir, \"_metadata.json\")\n",
    "        if not os.path.exists(metadata_path):\n",
    "            return None\n",
    "\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "\n",
    "        data = {}\n",
    "        for key in metadata['keys']:\n",
    "            # Try each format in order\n",
    "            json_path = os.path.join(checkpoint_dir, f\"{key}.json\")\n",
    "            parquet_path = os.path.join(checkpoint_dir, f\"{key}.parquet\")\n",
    "            npy_path = os.path.join(checkpoint_dir, f\"{key}.npy\")\n",
    "            pkl_path = os.path.join(checkpoint_dir, f\"{key}.pkl\")\n",
    "\n",
    "            if key == 'model' and os.path.exists(json_path):\n",
    "                # Load XGBoost model\n",
    "                model = XGBClassifier()\n",
    "                model.load_model(json_path)\n",
    "                data[key] = model\n",
    "            elif os.path.exists(parquet_path):\n",
    "                # Load DataFrame\n",
    "                data[key] = pd.read_parquet(parquet_path)\n",
    "            elif os.path.exists(npy_path):\n",
    "                # Load numpy array\n",
    "                data[key] = np.load(npy_path)\n",
    "            elif os.path.exists(json_path):\n",
    "                # Load JSON\n",
    "                with open(json_path, 'r') as f:\n",
    "                    data[key] = json.load(f)\n",
    "            elif os.path.exists(pkl_path):\n",
    "                # Load pickle\n",
    "                with open(pkl_path, 'rb') as f:\n",
    "                    data[key] = pickle.load(f)\n",
    "\n",
    "        print(f\"✓ CHECKPOINT LOADED: {name}\")\n",
    "        return data\n",
    "\n",
    "    # Fallback: check for legacy pickle format\n",
    "    legacy_path = os.path.join(CHECKPOINT_DIR, f\"{name}.pkl\")\n",
    "    if os.path.exists(legacy_path):\n",
    "        print(f\"⚠ Loading legacy pickle checkpoint: {name}\")\n",
    "        with open(legacy_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        print(f\"✓ CHECKPOINT LOADED: {name} (legacy format)\")\n",
    "        return data\n",
    "\n",
    "    return None\n",
    "\n",
    "def checkpoint_exists(name):\n",
    "    \"\"\"Check if a checkpoint exists (new or legacy format).\"\"\"\n",
    "    checkpoint_dir = get_checkpoint_dir(name)\n",
    "    legacy_path = os.path.join(CHECKPOINT_DIR, f\"{name}.pkl\")\n",
    "    return os.path.isdir(checkpoint_dir) or os.path.exists(legacy_path)\n",
    "\n",
    "def list_checkpoints():\n",
    "    \"\"\"List all existing checkpoints (new and legacy formats).\"\"\"\n",
    "    if not os.path.exists(CHECKPOINT_DIR):\n",
    "        return []\n",
    "\n",
    "    checkpoints = set()\n",
    "\n",
    "    # New format: directories\n",
    "    for item in os.listdir(CHECKPOINT_DIR):\n",
    "        item_path = os.path.join(CHECKPOINT_DIR, item)\n",
    "        if os.path.isdir(item_path) and os.path.exists(os.path.join(item_path, \"_metadata.json\")):\n",
    "            checkpoints.add(item)\n",
    "\n",
    "    # Legacy format: .pkl files\n",
    "    for f in os.listdir(CHECKPOINT_DIR):\n",
    "        if f.endswith('.pkl'):\n",
    "            checkpoints.add(f.replace('.pkl', ''))\n",
    "\n",
    "    return sorted(checkpoints)\n",
    "\n",
    "def get_latest_checkpoint():\n",
    "    \"\"\"Determine the latest checkpoint and what step to resume from.\"\"\"\n",
    "    checkpoints = list_checkpoints()\n",
    "    if not checkpoints:\n",
    "        return None, \"start\"\n",
    "\n",
    "    # Define checkpoint order\n",
    "    checkpoint_order = [\n",
    "        \"step1_1_data\",\n",
    "        \"step1_2_correlation\",\n",
    "        \"step1_3_clusters\",\n",
    "        \"step1_4_baseline_model\",\n",
    "        \"step1_5_shap_phase1\",\n",
    "        \"step1_6_cluster_representatives\",\n",
    "        \"step1_7_phase1_complete\"\n",
    "    ]\n",
    "\n",
    "    # Check Phase 1 checkpoints\n",
    "    latest_phase1 = None\n",
    "    for cp in checkpoint_order:\n",
    "        if cp in checkpoints:\n",
    "            latest_phase1 = cp\n",
    "\n",
    "    # Check Phase 2 iteration checkpoints\n",
    "    phase2_iters = [cp for cp in checkpoints if cp.startswith(\"step2_iter\")]\n",
    "    if phase2_iters:\n",
    "        # Find highest iteration number\n",
    "        iter_nums = []\n",
    "        for cp in phase2_iters:\n",
    "            try:\n",
    "                # Extract iteration number from names like \"step2_iter3_complete\"\n",
    "                parts = cp.split(\"_\")\n",
    "                iter_num = int(parts[1].replace(\"iter\", \"\"))\n",
    "                iter_nums.append((iter_num, cp))\n",
    "            except:\n",
    "                pass\n",
    "        if iter_nums:\n",
    "            max_iter = max(iter_nums, key=lambda x: x[0])\n",
    "            return max_iter[1], f\"phase2_iter{max_iter[0]}\"\n",
    "\n",
    "    if latest_phase1:\n",
    "        return latest_phase1, latest_phase1\n",
    "\n",
    "    return None, \"start\"\n",
    "\n",
    "def clear_checkpoints():\n",
    "    \"\"\"Clear all checkpoints to start fresh (handles both new and legacy formats).\"\"\"\n",
    "    import shutil\n",
    "\n",
    "    if os.path.exists(CHECKPOINT_DIR):\n",
    "        for item in os.listdir(CHECKPOINT_DIR):\n",
    "            item_path = os.path.join(CHECKPOINT_DIR, item)\n",
    "            if os.path.isdir(item_path):\n",
    "                # New format: remove entire checkpoint directory\n",
    "                shutil.rmtree(item_path)\n",
    "            elif item.endswith('.pkl'):\n",
    "                # Legacy format: remove pickle file\n",
    "                os.remove(item_path)\n",
    "        print(\"✓ All checkpoints cleared\")\n",
    "\n",
    "def display_checkpoint_status():\n",
    "    \"\"\"Display current checkpoint status.\"\"\"\n",
    "    checkpoints = list_checkpoints()\n",
    "    latest, resume_point = get_latest_checkpoint()\n",
    "\n",
    "    print(\"=\"*70)\n",
    "    print(\"CHECKPOINT STATUS\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    if not checkpoints:\n",
    "        print(\"No checkpoints found. Starting fresh.\")\n",
    "    else:\n",
    "        print(f\"Found {len(checkpoints)} checkpoint(s):\")\n",
    "        for cp in checkpoints:\n",
    "            print(f\"  - {cp}\")\n",
    "        print(f\"\\nLatest: {latest}\")\n",
    "        print(f\"Resume point: {resume_point}\")\n",
    "\n",
    "    print(\"=\"*70)\n",
    "    return latest, resume_point\n",
    "\n",
    "# Initialize directories\n",
    "ensure_directories()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d00cce5f-3d1c-4d2a-a0b9-41d8c7c7d3f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Check for Existing Checkpoints and Resume Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce64f108-1dc4-4e44-a234-2f3c29a9dd38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display checkpoint status and determine resume point\n",
    "latest_checkpoint, resume_point = display_checkpoint_status()\n",
    "\n",
    "# Determine whether to resume or start fresh\n",
    "if latest_checkpoint and AUTO_RESUME:\n",
    "    print(f\"\\n>>> AUTO_RESUME is ON. Resuming from: {resume_point}\")\n",
    "    START_FRESH = False\n",
    "elif latest_checkpoint and not AUTO_RESUME:\n",
    "    print(f\"\\n>>> Checkpoints exist but AUTO_RESUME is OFF.\")\n",
    "    print(\">>> Set AUTO_RESUME = True to resume, or run clear_checkpoints() to start fresh.\")\n",
    "    START_FRESH = True\n",
    "else:\n",
    "    print(\"\\n>>> No checkpoints found. Starting fresh.\")\n",
    "    START_FRESH = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53dcf48a-978e-41e3-8140-b6d50a8bb0d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83b8713f-f629-4a1c-89b8-7d02adbd46ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def print_stage(stage_name, stage_num=None, total_stages=None):\n",
    "    \"\"\"Print a prominent stage marker with timestamp.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    if stage_num and total_stages:\n",
    "        header = f\"[{timestamp}] STAGE {stage_num}/{total_stages}: {stage_name}\"\n",
    "    else:\n",
    "        header = f\"[{timestamp}] {stage_name}\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(header)\n",
    "    print(\"=\"*70)\n",
    "\n",
    "def print_progress(message, indent=2):\n",
    "    \"\"\"Print a progress message with timestamp.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    prefix = \" \" * indent\n",
    "    print(f\"{prefix}[{timestamp}] {message}\")\n",
    "\n",
    "def train_xgboost_model(X_train, y_train, X_val, y_val, feature_cols, scale_pos_weight):\n",
    "    \"\"\"\n",
    "    Train an XGBoost model with conservative hyperparameters.\n",
    "    Returns the trained model.\n",
    "    \"\"\"\n",
    "    print_progress(f\"Training XGBoost with {len(feature_cols)} features...\")\n",
    "    print_progress(f\"Train: {len(y_train):,} obs, {int(y_train.sum()):,} events | Val: {len(y_val):,} obs, {int(y_val.sum()):,} events\")\n",
    "\n",
    "    # Conservative parameters from original CRC_ITER1_MODEL-PREVALENCE.py\n",
    "    # These prevent overfitting on the highly imbalanced data\n",
    "    params = {\n",
    "        'max_depth': 3,              # Very shallow trees (original: 2)\n",
    "        'min_child_weight': 50,      # Require substantial support\n",
    "        'gamma': 2.0,                # High min loss reduction (original: 2.0)\n",
    "        'subsample': 0.5,            # Low row sampling (original: 0.5)\n",
    "        'colsample_bytree': 0.5,     # Low column sampling (original: 0.5)\n",
    "        'colsample_bylevel': 0.5,    # Additional column regularization\n",
    "        'reg_alpha': 5.0,            # L1 regularization (original: 5.0)\n",
    "        'reg_lambda': 50.0,          # L2 regularization (original: 50.0)\n",
    "        'learning_rate': 0.005,      # Very slow learning (original: 0.005)\n",
    "        'n_estimators': 2000,\n",
    "        'scale_pos_weight': scale_pos_weight,\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'aucpr',\n",
    "        'random_state': RANDOM_SEED,\n",
    "        'early_stopping_rounds': 150  # More patience (original: 150)\n",
    "    }\n",
    "\n",
    "    start_time = time.time()\n",
    "    model = XGBClassifier(**params)\n",
    "    model.fit(\n",
    "        X_train[feature_cols], y_train,\n",
    "        eval_set=[(X_val[feature_cols], y_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "    elapsed = time.time() - start_time\n",
    "    print_progress(f\"Model trained in {elapsed:.1f}s (best iteration: {model.best_iteration})\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X, y, feature_cols, split_name=\"\"):\n",
    "    \"\"\"\n",
    "    Evaluate model and return metrics dictionary.\n",
    "    \"\"\"\n",
    "    y_pred = model.predict_proba(X[feature_cols])[:, 1]\n",
    "\n",
    "    auprc = average_precision_score(y, y_pred)\n",
    "    auroc = roc_auc_score(y, y_pred)\n",
    "    brier = brier_score_loss(y, y_pred)\n",
    "    baseline_rate = y.mean()\n",
    "\n",
    "    metrics = {\n",
    "        'auprc': auprc,\n",
    "        'auroc': auroc,\n",
    "        'brier': brier,\n",
    "        'baseline_rate': baseline_rate,\n",
    "        'lift': auprc / baseline_rate if baseline_rate > 0 else 0,\n",
    "        'n_samples': len(y),\n",
    "        'n_events': int(y.sum())\n",
    "    }\n",
    "\n",
    "    if split_name:\n",
    "        print(f\"  {split_name}: AUPRC={auprc:.4f} ({metrics['lift']:.1f}x lift), AUROC={auroc:.4f}\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def compute_shap_values(model, X_val, y_val, feature_cols):\n",
    "    \"\"\"\n",
    "    Compute SHAP values separately for positive and negative cases.\n",
    "    Returns importance_pos, importance_neg, importance_combined, importance_ratio.\n",
    "    \"\"\"\n",
    "    overall_start = time.time()\n",
    "    print_progress(\"Starting SHAP computation...\")\n",
    "\n",
    "    # Get indices for positive and negative cases\n",
    "    pos_mask = y_val == 1\n",
    "    neg_mask = y_val == 0\n",
    "\n",
    "    n_pos = pos_mask.sum()\n",
    "    n_neg = neg_mask.sum()\n",
    "\n",
    "    print_progress(f\"Positive cases: {n_pos:,} | Negative cases: {n_neg:,}\")\n",
    "\n",
    "    # Create explainer\n",
    "    print_progress(\"Creating TreeExplainer...\")\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "\n",
    "    # Compute SHAP for positive cases\n",
    "    print_progress(f\"Computing SHAP for {n_pos:,} positive cases (this may take a while)...\")\n",
    "    start = time.time()\n",
    "    X_pos = X_val.loc[pos_mask, feature_cols]\n",
    "    shap_values_pos = explainer.shap_values(X_pos)\n",
    "    elapsed_pos = time.time() - start\n",
    "    print_progress(f\"Positive cases complete in {elapsed_pos:.1f}s ({n_pos/elapsed_pos:.0f} cases/sec)\")\n",
    "\n",
    "    # Compute SHAP for negative cases (use all cases for more stable estimates)\n",
    "    X_neg = X_val.loc[neg_mask, feature_cols]\n",
    "    n_neg_shap = len(X_neg)\n",
    "\n",
    "    print_progress(f\"Computing SHAP for {n_neg_shap:,} negative cases (this may take a while)...\")\n",
    "    start = time.time()\n",
    "    shap_values_neg = explainer.shap_values(X_neg)\n",
    "    elapsed_neg = time.time() - start\n",
    "    \n",
    "    print_progress(f\"Negative cases complete in {elapsed_neg:.1f}s ({n_neg_shap/elapsed_neg:.0f} cases/sec)\")\n",
    "\n",
    "\n",
    "    # Calculate importance metrics\n",
    "    print_progress(\"Calculating importance metrics...\")\n",
    "    importance_pos = np.abs(shap_values_pos).mean(axis=0)\n",
    "    importance_neg = np.abs(shap_values_neg).mean(axis=0)\n",
    "\n",
    "    # 2:1 weighting for positive cases\n",
    "    importance_combined = (importance_pos * 2 + importance_neg) / 3\n",
    "\n",
    "    # Ratio: higher = more predictive of positive cases\n",
    "    importance_ratio = importance_pos / (importance_neg + 1e-10)\n",
    "\n",
    "    # Create DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_cols,\n",
    "        'SHAP_Positive': importance_pos,\n",
    "        'SHAP_Negative': importance_neg,\n",
    "        'SHAP_Combined': importance_combined,\n",
    "        'SHAP_Ratio': importance_ratio\n",
    "    }).sort_values('SHAP_Combined', ascending=False)\n",
    "\n",
    "    importance_df['Rank'] = range(1, len(importance_df) + 1)\n",
    "\n",
    "    total_elapsed = time.time() - overall_start\n",
    "    print_progress(f\"SHAP computation complete in {total_elapsed:.1f}s total\")\n",
    "\n",
    "    return importance_df\n",
    "\n",
    "def update_tracking_csv(iteration_data):\n",
    "    \"\"\"\n",
    "    Append iteration results to tracking CSV.\n",
    "    \"\"\"\n",
    "    csv_path = os.path.join(OUTPUT_DIR, \"lucem_novis_standard_iteration_tracking.csv\")\n",
    "\n",
    "    df_new = pd.DataFrame([iteration_data])\n",
    "\n",
    "    if os.path.exists(csv_path):\n",
    "        df_existing = pd.read_csv(csv_path)\n",
    "        # Remove existing row for this iteration if re-running\n",
    "        phase = iteration_data.get('phase', '')\n",
    "        iteration = iteration_data.get('iteration', '')\n",
    "        df_existing = df_existing[~((df_existing['phase'] == phase) & (df_existing['iteration'] == iteration))]\n",
    "        df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "    else:\n",
    "        df_combined = df_new\n",
    "\n",
    "    df_combined.to_csv(csv_path, index=False)\n",
    "    print(f\"  ✓ Tracking CSV updated: {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69e1202e-c4a9-4ae4-9bf1-3b6443e578b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "# PHASE 1: Cluster-Based Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b389c7db-bbed-4881-a59a-580ba1427e98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1.1: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "843026fa-9580-4a3f-acde-91b489ca0569",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check if we can skip this step\n",
    "if checkpoint_exists(\"step1_1_data\") and not START_FRESH:\n",
    "    print_stage(\"STEP 1.1: LOAD DATA (from checkpoint)\", 1, 7)\n",
    "    print_progress(\"Loading from checkpoint: step1_1_data\")\n",
    "    data_checkpoint = load_checkpoint(\"step1_1_data\")\n",
    "    df_pandas = data_checkpoint['df_pandas']\n",
    "    feature_cols = data_checkpoint['feature_cols']\n",
    "    scale_pos_weight = data_checkpoint['scale_pos_weight']\n",
    "    print_progress(f\"Loaded {len(df_pandas):,} observations, {len(feature_cols)} features\")\n",
    "else:\n",
    "    print_stage(\"STEP 1.1: LOAD DATA\", 1, 7)\n",
    "\n",
    "    # Load wide feature table with SPLIT column\n",
    "    print_progress(\"Querying data from Spark (this may take a few minutes for large datasets)...\")\n",
    "\n",
    "    load_start = time.time()\n",
    "    df_spark = spark.sql(f'''\n",
    "    SELECT *\n",
    "    FROM {trgt_cat}.clncl_ds.herald_eda_train_wide_cleaned\n",
    "    ''')\n",
    "\n",
    "    # Convert to Pandas\n",
    "    print_progress(\"Converting Spark DataFrame to Pandas (may take a while for large datasets)...\")\n",
    "    df_pandas = df_spark.toPandas()\n",
    "    print_progress(f\"Data loaded in {time.time() - load_start:.1f}s\")\n",
    "\n",
    "    # Convert datetime\n",
    "    df_pandas['END_DTTM'] = pd.to_datetime(df_pandas['END_DTTM'])\n",
    "\n",
    "    # Identify feature columns (exclude identifiers, target, and split)\n",
    "    # ICD10_CODE and ICD10_GROUP were already excluded in Book 8's clean table\n",
    "    exclude_cols = ['PAT_ID', 'END_DTTM', 'FUTURE_CRC_EVENT', 'SPLIT']\n",
    "    feature_cols = [c for c in df_pandas.columns if c not in exclude_cols]\n",
    "\n",
    "    # Exclude months_since_cohort_entry: encodes observation time (study design\n",
    "    # artifact), not clinical signal. Longer observation = more diagnosis\n",
    "    # opportunities, creating prevalence bias. Same rationale as CEA/FOBT removal.\n",
    "    if 'months_since_cohort_entry' in feature_cols:\n",
    "        feature_cols.remove('months_since_cohort_entry')\n",
    "        print('  Excluded months_since_cohort_entry (observation time / prevalence bias)')\n",
    "\n",
    "    # LUCEM NOVIS: Exclude all visit history features (Book 6)\n",
    "    visit_features = [c for c in feature_cols if c.startswith('visit_')]\n",
    "    feature_cols = [c for c in feature_cols if not c.startswith('visit_')]\n",
    "    print(f\"  Excluded {len(visit_features)} visit features (Lucem Novis variant)\")\n",
    "\n",
    "    # Calculate scale_pos_weight from training data\n",
    "    train_mask = df_pandas['SPLIT'] == 'train'\n",
    "    n_neg = (df_pandas.loc[train_mask, 'FUTURE_CRC_EVENT'] == 0).sum()\n",
    "    n_pos = (df_pandas.loc[train_mask, 'FUTURE_CRC_EVENT'] == 1).sum()\n",
    "    if n_pos == 0:\n",
    "        raise ValueError(\"No positive cases in training data. Cannot compute scale_pos_weight.\")\n",
    "    scale_pos_weight = n_neg / n_pos\n",
    "\n",
    "    print(f\"\\nData loaded:\")\n",
    "    print(f\"  Total observations: {len(df_pandas):,}\")\n",
    "    print(f\"  Features: {len(feature_cols)}\")\n",
    "    print(f\"  Train: {train_mask.sum():,}\")\n",
    "    print(f\"  Val: {(df_pandas['SPLIT'] == 'val').sum():,}\")\n",
    "    print(f\"  Test: {(df_pandas['SPLIT'] == 'test').sum():,}\")\n",
    "    print(f\"  Scale pos weight: {scale_pos_weight:.1f}\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    save_checkpoint(\"step1_1_data\", {\n",
    "        'df_pandas': df_pandas,\n",
    "        'feature_cols': feature_cols,\n",
    "        'scale_pos_weight': scale_pos_weight\n",
    "    })\n",
    "\n",
    "print(f\"\\n✓ Data ready: {len(df_pandas):,} observations, {len(feature_cols)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91033f6f-79dc-4bbb-9787-95a5fc92ac69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1.1b: Create CV Folds for Feature Selection Stability\n",
    " \n",
    "To ensure feature selection stability, we run the selection process across multiple\n",
    "cross-validation folds. Features that are consistently selected across folds are more\n",
    "likely to be genuinely important rather than artifacts of a particular train/val split.\n",
    " \n",
    "**Approach:**\n",
    "- Test set (Q6) remains fixed as temporal holdout\n",
    "- Q0-Q5 data is split into 5 folds using StratifiedGroupKFold\n",
    "- Feature selection runs on each fold\n",
    "- Final features = those selected in ≥3/5 folds (60% threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc945ef5-9105-4ae9-b3ed-200b9324c379",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "# Check if we can skip this step\n",
    "if checkpoint_exists(\"step1_1b_cv_folds\") and not START_FRESH:\n",
    "    print(\">>> Loading from checkpoint: step1_1b_cv_folds\")\n",
    "    cv_checkpoint = load_checkpoint(\"step1_1b_cv_folds\")\n",
    "    cv_fold_assignments = cv_checkpoint['cv_fold_assignments']\n",
    "else:\n",
    "    print(\"=\"*70)\n",
    "    print(\"STEP 1.1b: CREATE CV FOLDS FOR FEATURE SELECTION\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Separate test (Q6) from train/val pool (existing split)\n",
    "    test_mask = df_pandas['SPLIT'] == 'test'\n",
    "    trainval_mask = df_pandas['SPLIT'].isin(['train', 'val'])\n",
    "\n",
    "    print(f\"Test set (fixed): {test_mask.sum():,} observations\")\n",
    "    print(f\"Train/Val pool: {trainval_mask.sum():,} observations\")\n",
    "\n",
    "    # Get patient-level labels for SGKF with MULTI-CLASS stratification by cancer type\n",
    "    # This matches Book 0's approach: 0=negative, 1=C18, 2=C19, 3=C20\n",
    "    trainval_df = df_pandas[trainval_mask].copy()\n",
    "\n",
    "    # Get patient-level outcome and cancer type\n",
    "    patient_outcome = trainval_df.groupby('PAT_ID')['FUTURE_CRC_EVENT'].max().reset_index()\n",
    "    patient_outcome.columns = ['PAT_ID', 'is_positive']\n",
    "\n",
    "    # For positive patients, get their cancer type from the cohort table\n",
    "    # ICD10_GROUP is not in the wide_cleaned table (excluded as outcome-related in Book 8)\n",
    "    # so we join it from the cohort table only for stratification purposes\n",
    "    cancer_type_df = spark.sql(f'''\n",
    "        SELECT DISTINCT PAT_ID, ICD10_GROUP\n",
    "        FROM {trgt_cat}.clncl_ds.herald_eda_train_final_cohort\n",
    "        WHERE FUTURE_CRC_EVENT = 1 AND ICD10_GROUP IS NOT NULL\n",
    "    ''').toPandas()\n",
    "\n",
    "    trainval_pats = set(trainval_df['PAT_ID'].unique())\n",
    "    positive_patients = cancer_type_df[cancer_type_df['PAT_ID'].isin(trainval_pats)].drop_duplicates('PAT_ID')\n",
    "\n",
    "    # Merge to get cancer type for positive patients\n",
    "    patient_labels = patient_outcome.merge(positive_patients, on='PAT_ID', how='left')\n",
    "\n",
    "    # Create multi-class stratification label: 0=neg, 1=C18, 2=C19, 3=C20\n",
    "    cancer_type_map = {'C18': 1, 'C19': 2, 'C20': 3}\n",
    "    patient_labels['strat_label'] = patient_labels.apply(\n",
    "        lambda row: cancer_type_map.get(row['ICD10_GROUP'], 0) if row['is_positive'] == 1 else 0,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    print(f\"Unique patients in train/val: {len(patient_labels):,}\")\n",
    "    print(f\"Positive patients: {patient_labels['is_positive'].sum():,}\")\n",
    "    print(f\"Stratification classes: {patient_labels['strat_label'].value_counts().sort_index().to_dict()}\")\n",
    "\n",
    "    # Create 3-fold SGKF\n",
    "    sgkf = StratifiedGroupKFold(n_splits=N_CV_FOLDS, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "    X_dummy = np.zeros(len(patient_labels))\n",
    "    y = patient_labels['strat_label'].values  # Multi-class: 0=neg, 1=C18, 2=C19, 3=C20\n",
    "    groups = patient_labels['PAT_ID'].values\n",
    "\n",
    "    # Store fold assignments for each patient (as lists for JSON serialization)\n",
    "    # Use string keys because JSON converts int keys to strings\n",
    "    cv_fold_assignments = {}\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(sgkf.split(X_dummy, y, groups)):\n",
    "        train_patients_list = patient_labels.iloc[train_idx]['PAT_ID'].tolist()\n",
    "        val_patients_list = patient_labels.iloc[val_idx]['PAT_ID'].tolist()\n",
    "\n",
    "        cv_fold_assignments[str(fold_idx)] = {\n",
    "            'train_patients': train_patients_list,  # Stored as list for JSON compatibility\n",
    "            'val_patients': val_patients_list\n",
    "        }\n",
    "\n",
    "        # Calculate stats (convert to set for efficient lookup)\n",
    "        train_patients = set(train_patients_list)\n",
    "        val_patients = set(val_patients_list)\n",
    "        train_obs = trainval_df[trainval_df['PAT_ID'].isin(train_patients)]\n",
    "        val_obs = trainval_df[trainval_df['PAT_ID'].isin(val_patients)]\n",
    "\n",
    "        print(f\"\\nFold {fold_idx + 1}:\")\n",
    "        print(f\"  Train: {len(train_patients):,} patients, {len(train_obs):,} observations\")\n",
    "        print(f\"  Val:   {len(val_patients):,} patients, {len(val_obs):,} observations\")\n",
    "        print(f\"  Train event rate: {train_obs['FUTURE_CRC_EVENT'].mean()*100:.4f}%\")\n",
    "        print(f\"  Val event rate:   {val_obs['FUTURE_CRC_EVENT'].mean()*100:.4f}%\")\n",
    "\n",
    "        # Show cancer type distribution in each fold (verifies multi-class stratification)\n",
    "        train_labels = patient_labels[patient_labels['PAT_ID'].isin(train_patients)]['strat_label'].value_counts().sort_index()\n",
    "        val_labels = patient_labels[patient_labels['PAT_ID'].isin(val_patients)]['strat_label'].value_counts().sort_index()\n",
    "        print(f\"  Train cancer types: { {k: train_labels.get(k, 0) for k in [0,1,2,3]} }\")\n",
    "        print(f\"  Val cancer types:   { {k: val_labels.get(k, 0) for k in [0,1,2,3]} }\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    save_checkpoint(\"step1_1b_cv_folds\", {\n",
    "        'cv_fold_assignments': cv_fold_assignments\n",
    "    })\n",
    "\n",
    "print(f\"\\n✓ CV folds created: {N_CV_FOLDS} folds for feature selection stability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0415f4b9-a296-4400-b636-75d280fe15e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1.2: Compute Correlation Matrix (Training Data Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2fcf7f0-90d7-471a-9de3-1381a6af9381",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Diagnostic: check for non-numeric columns in feature_cols\n",
    "# Run this cell in Databricks BEFORE Step 1.2 of Book 9\n",
    "print(f\"Total feature columns: {len(feature_cols)}\")\n",
    "print(f\"\\nNon-numeric columns in feature_cols:\")\n",
    "found = False\n",
    "for col in feature_cols:\n",
    "    dtype = df_pandas[col].dtype\n",
    "    if not pd.api.types.is_numeric_dtype(df_pandas[col]):\n",
    "        print(f\"  {col}: {dtype}\")\n",
    "        found = True\n",
    "if not found:\n",
    "    print(\"  (none)\")\n",
    "\n",
    "print(f\"\\nAll dtypes in feature_cols:\")\n",
    "print(df_pandas[feature_cols].dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f044a17-df74-4585-a84e-0c18ac2dfb92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check if we can skip this step\n",
    "if checkpoint_exists(\"step1_2_correlation\") and not START_FRESH:\n",
    "    print_stage(\"STEP 1.2: COMPUTE CORRELATION MATRIX (from checkpoint)\", 2, 7)\n",
    "    print_progress(\"Loading from checkpoint: step1_2_correlation\")\n",
    "    corr_checkpoint = load_checkpoint(\"step1_2_correlation\")\n",
    "    corr_matrix = corr_checkpoint['corr_matrix']\n",
    "    dist_matrix = corr_checkpoint['dist_matrix']\n",
    "    # Use filtered feature list from checkpoint, or fall back to correlation matrix columns\n",
    "    feature_cols = corr_checkpoint.get('feature_cols', list(corr_matrix.columns))\n",
    "    print_progress(f\"Loaded correlation matrix: {corr_matrix.shape}\")\n",
    "else:\n",
    "    print_stage(\"STEP 1.2: COMPUTE CORRELATION MATRIX\", 2, 7)\n",
    "\n",
    "    # Filter to training data only\n",
    "    train_mask = df_pandas['SPLIT'] == 'train'\n",
    "    df_train = df_pandas.loc[train_mask, feature_cols]\n",
    "\n",
    "    print_progress(f\"Initial feature count: {len(feature_cols)}\")\n",
    "\n",
    "    # ==========================================================================\n",
    "    # REMOVE ZERO-VARIANCE FEATURES\n",
    "    # ==========================================================================\n",
    "    # Spearman correlation is undefined for constant features (zero variance),\n",
    "    # which creates NaN values in the correlation matrix. This happens when\n",
    "    # the patient-level split moves different patients to train, causing some\n",
    "    # features that had variation in one cohort to become constant in another.\n",
    "    # ==========================================================================\n",
    "\n",
    "    variances = df_train.var()\n",
    "    zero_var_features = variances[variances == 0].index.tolist()\n",
    "\n",
    "    if zero_var_features:\n",
    "        print_progress(f\"WARNING: Removing {len(zero_var_features)} zero-variance features:\")\n",
    "        for f in zero_var_features[:10]:  # Show first 10\n",
    "            print_progress(f\"  - {f}\")\n",
    "        if len(zero_var_features) > 10:\n",
    "            print_progress(f\"  ... and {len(zero_var_features) - 10} more\")\n",
    "\n",
    "        # Update feature_cols to exclude zero-variance features\n",
    "        feature_cols = [c for c in feature_cols if c not in zero_var_features]\n",
    "        df_train = df_train[feature_cols]\n",
    "\n",
    "    print_progress(f\"Features after variance filter: {len(feature_cols)}\")\n",
    "\n",
    "    # ==========================================================================\n",
    "    # IMPUTE MISSING VALUES FOR CORRELATION ONLY\n",
    "    # ==========================================================================\n",
    "    # Spearman correlation with pairwise-complete observations can produce\n",
    "    # inconsistent matrices when missingness patterns differ across features.\n",
    "    # We impute a COPY of the training data using clinically sensible defaults\n",
    "    # (matching the original CORRELATION_HIERARCHICAL_FEATURE_CLUSTERING.py).\n",
    "    # The original df_pandas is NOT modified — XGBoost and SHAP use raw NaNs.\n",
    "    # ==========================================================================\n",
    "\n",
    "    print_progress(\"Imputing missing values for correlation computation (original data unchanged)...\")\n",
    "    df_train_imputed = df_train.copy()\n",
    "\n",
    "    imputation_counts = {'time_since': 0, 'binary': 0, 'count': 0, 'continuous': 0}\n",
    "\n",
    "    for col_name in feature_cols:\n",
    "        n_missing = df_train_imputed[col_name].isnull().sum()\n",
    "        if n_missing == 0:\n",
    "            continue\n",
    "\n",
    "        # Skip non-numeric columns (shouldn't be here, but safety check)\n",
    "        if not pd.api.types.is_numeric_dtype(df_train_imputed[col_name]):\n",
    "            print(f\"WARNING: Non-numeric column in features: '{col_name}' (dtype={df_train_imputed[col_name].dtype}\")\n",
    "\n",
    "        # 1. TIME-SINCE FEATURES: fill with max+1 (\"never observed\")\n",
    "        if 'days_since' in col_name.lower() or col_name.endswith('_DAYS') or 'recency' in col_name.lower():\n",
    "            max_val = df_train_imputed[col_name].max()\n",
    "            fill_val = max_val + 1 if not pd.isna(max_val) else 730\n",
    "            df_train_imputed[col_name] = df_train_imputed[col_name].fillna(fill_val)\n",
    "            imputation_counts['time_since'] += 1\n",
    "\n",
    "        # 2. BINARY FLAGS: fill with 0 (\"flag not present\")\n",
    "        elif set(df_train_imputed[col_name].dropna().unique()).issubset({0, 1, 0.0, 1.0}):\n",
    "            df_train_imputed[col_name] = df_train_imputed[col_name].fillna(0)\n",
    "            imputation_counts['binary'] += 1\n",
    "\n",
    "        # 3. COUNT FEATURES: fill with 0 (\"no events\")\n",
    "        elif (df_train_imputed[col_name].dropna() >= 0).all() and (df_train_imputed[col_name].dropna() % 1 == 0).all():\n",
    "            df_train_imputed[col_name] = df_train_imputed[col_name].fillna(0)\n",
    "            imputation_counts['count'] += 1\n",
    "\n",
    "        # 4. CONTINUOUS FEATURES: fill with median (\"typical patient\")\n",
    "        else:\n",
    "            median_val = df_train_imputed[col_name].median()\n",
    "            df_train_imputed[col_name] = df_train_imputed[col_name].fillna(median_val)\n",
    "            imputation_counts['continuous'] += 1\n",
    "\n",
    "    remaining = df_train_imputed[feature_cols].isnull().sum().sum()\n",
    "    print_progress(f\"  Time-since features (max+1): {imputation_counts['time_since']}\")\n",
    "    print_progress(f\"  Binary flags (0): {imputation_counts['binary']}\")\n",
    "    print_progress(f\"  Count features (0): {imputation_counts['count']}\")\n",
    "    print_progress(f\"  Continuous features (median): {imputation_counts['continuous']}\")\n",
    "    print_progress(f\"  Remaining missing values: {remaining}\")\n",
    "    if remaining > 0:\n",
    "        print_progress(\"  WARNING: Some missing values could not be classified — will use pairwise deletion\")\n",
    "\n",
    "    print_progress(f\"Computing Spearman correlation on {len(df_train_imputed):,} observations x {len(feature_cols)} features...\")\n",
    "    print_progress(\"This is O(n*m^2) - may take several minutes for large feature sets...\")\n",
    "\n",
    "    start = time.time()\n",
    "    corr_matrix = df_train_imputed.corr(method='spearman')\n",
    "    elapsed = time.time() - start\n",
    "    print_progress(f\"Correlation matrix computed in {elapsed:.1f}s\")\n",
    "\n",
    "    # Free the imputed copy — no longer needed\n",
    "    del df_train_imputed\n",
    "\n",
    "    # Check for any remaining NaN (shouldn't happen after imputation + variance filter)\n",
    "    nan_count = corr_matrix.isna().sum().sum()\n",
    "    if nan_count > 0:\n",
    "        print_progress(f\"WARNING: {nan_count} NaN values remain in correlation matrix\")\n",
    "        # Drop features that still have NaN correlations\n",
    "        nan_features = corr_matrix.columns[corr_matrix.isna().all()].tolist()\n",
    "        if nan_features:\n",
    "            print_progress(f\"Dropping {len(nan_features)} features with all-NaN correlations\")\n",
    "            corr_matrix = corr_matrix.drop(index=nan_features, columns=nan_features)\n",
    "            feature_cols = [c for c in feature_cols if c not in nan_features]\n",
    "\n",
    "    # Convert to distance matrix: distance = 1 - |correlation|\n",
    "    dist_matrix = 1 - corr_matrix.abs()\n",
    "\n",
    "    # Save checkpoint (include filtered feature_cols)\n",
    "    save_checkpoint(\"step1_2_correlation\", {\n",
    "        'corr_matrix': corr_matrix,\n",
    "        'dist_matrix': dist_matrix,\n",
    "        'feature_cols': feature_cols\n",
    "    })\n",
    "\n",
    "print(f\"\\n✓ Correlation matrix ready: {corr_matrix.shape}\")\n",
    "print(f\"✓ Feature count after filtering: {len(feature_cols)}\")\n",
    "\n",
    "# Ensure feature_cols is synced with correlation matrix (safety check)\n",
    "if len(feature_cols) != len(dist_matrix.columns):\n",
    "    print(f\"WARNING: feature_cols ({len(feature_cols)}) != dist_matrix columns ({len(dist_matrix.columns)})\")\n",
    "    print(\"Syncing feature_cols with distance matrix columns...\")\n",
    "    feature_cols = list(dist_matrix.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4e329d9-5b41-47c6-9453-9cd82cea98c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1.3: Dynamic Threshold Selection via Silhouette Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0efad1f-754b-4d3c-a5c5-e1a2d8e2d2e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check if we can skip this step\n",
    "if checkpoint_exists(\"step1_3_clusters\") and not START_FRESH:\n",
    "    print_stage(\"STEP 1.3: DYNAMIC THRESHOLD SELECTION (from checkpoint)\", 3, 7)\n",
    "    print_progress(\"Loading from checkpoint: step1_3_clusters\")\n",
    "    cluster_checkpoint = load_checkpoint(\"step1_3_clusters\")\n",
    "    linkage_matrix = cluster_checkpoint['linkage_matrix']\n",
    "    chosen_threshold = cluster_checkpoint['chosen_threshold']\n",
    "    cluster_labels = cluster_checkpoint['cluster_labels']\n",
    "    cluster_df = cluster_checkpoint['cluster_df']\n",
    "    threshold_results = cluster_checkpoint['threshold_results']\n",
    "    print_progress(f\"Loaded clustering with threshold {chosen_threshold}, {len(np.unique(cluster_labels))} clusters\")\n",
    "else:\n",
    "    print_stage(\"STEP 1.3: DYNAMIC THRESHOLD SELECTION\", 3, 7)\n",
    "\n",
    "    # Convert distance matrix to condensed form for hierarchical clustering\n",
    "    print_progress(\"Converting distance matrix to condensed form...\")\n",
    "    dist_values = dist_matrix.values.copy()\n",
    "\n",
    "    # Diagnostic: check for issues\n",
    "    print_progress(f\"  Matrix shape: {dist_values.shape}\")\n",
    "    nan_count = np.isnan(dist_values).sum()\n",
    "    inf_count = np.isinf(dist_values).sum()\n",
    "    print_progress(f\"  NaN count: {nan_count}, Inf count: {inf_count}\")\n",
    "\n",
    "    # Fix any remaining NaN/Inf values (set to max distance = 1.0)\n",
    "    if nan_count > 0 or inf_count > 0:\n",
    "        print_progress(\"  Replacing NaN/Inf with 1.0 (max distance)\")\n",
    "        dist_values = np.nan_to_num(dist_values, nan=1.0, posinf=1.0, neginf=1.0)\n",
    "\n",
    "    # Force symmetry by averaging with transpose (handles floating-point precision)\n",
    "    dist_values = (dist_values + dist_values.T) / 2\n",
    "\n",
    "    # Ensure diagonal is exactly 0\n",
    "    np.fill_diagonal(dist_values, 0)\n",
    "\n",
    "    # Verify symmetry\n",
    "    max_asymmetry = np.max(np.abs(dist_values - dist_values.T))\n",
    "    print_progress(f\"  Max asymmetry after fix: {max_asymmetry}\")\n",
    "\n",
    "    condensed_dist = squareform(dist_values)\n",
    "\n",
    "    # Perform hierarchical clustering\n",
    "    print_progress(\"Computing hierarchical clustering (average linkage)...\")\n",
    "    cluster_start = time.time()\n",
    "    linkage_matrix = linkage(condensed_dist, method='average')\n",
    "    print_progress(f\"Hierarchical clustering complete in {time.time() - cluster_start:.1f}s\")\n",
    "\n",
    "    # Test different thresholds\n",
    "    thresholds = np.arange(THRESHOLD_MIN, THRESHOLD_MAX + THRESHOLD_STEP, THRESHOLD_STEP)\n",
    "    threshold_results = []\n",
    "\n",
    "    print_progress(f\"Testing {len(thresholds)} thresholds from {THRESHOLD_MIN} to {THRESHOLD_MAX}...\")\n",
    "    print(\"-\"*60)\n",
    "\n",
    "    for thresh in thresholds:\n",
    "        # Get cluster labels at this threshold\n",
    "        labels = fcluster(linkage_matrix, t=thresh, criterion='distance')\n",
    "        n_clusters = len(np.unique(labels))\n",
    "\n",
    "        # Count singletons\n",
    "        cluster_sizes = pd.Series(labels).value_counts()\n",
    "        n_singletons = (cluster_sizes == 1).sum()\n",
    "\n",
    "        # Compute silhouette score (only if we have 2+ clusters and not all singletons)\n",
    "        if n_clusters > 1 and n_clusters < len(feature_cols):\n",
    "            try:\n",
    "                sil_score = silhouette_score(dist_values, labels, metric='precomputed')\n",
    "            except:\n",
    "                sil_score = -1\n",
    "        else:\n",
    "            sil_score = -1\n",
    "\n",
    "        threshold_results.append({\n",
    "            'threshold': thresh,\n",
    "            'n_clusters': n_clusters,\n",
    "            'n_singletons': n_singletons,\n",
    "            'pct_singletons': n_singletons / n_clusters * 100,\n",
    "            'silhouette': sil_score\n",
    "        })\n",
    "\n",
    "        print(f\"  Threshold {thresh:.2f}: {n_clusters} clusters, {n_singletons} singletons ({n_singletons/n_clusters*100:.0f}%), silhouette={sil_score:.3f}\")\n",
    "\n",
    "    threshold_df = pd.DataFrame(threshold_results)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # SMART THRESHOLD SELECTION\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Priority order:\n",
    "    # 1. Thresholds within MIN/MAX range that give TARGET_CLUSTER_RANGE clusters\n",
    "    # 2. Best silhouette within those constraints\n",
    "    # 3. Fallback: threshold closest to target cluster count\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    print(f\"\\n  Threshold constraints:\")\n",
    "    print(f\"    Min threshold: {MIN_CLUSTERING_THRESHOLD}\")\n",
    "    print(f\"    Max threshold: {MAX_CLUSTERING_THRESHOLD}\")\n",
    "    print(f\"    Target clusters: {TARGET_CLUSTER_RANGE[0]}-{TARGET_CLUSTER_RANGE[1]}\")\n",
    "\n",
    "    # Filter to valid range\n",
    "    constrained = threshold_df[\n",
    "        (threshold_df['threshold'] >= MIN_CLUSTERING_THRESHOLD) &\n",
    "        (threshold_df['threshold'] <= MAX_CLUSTERING_THRESHOLD)\n",
    "    ].copy()\n",
    "\n",
    "    if len(constrained) == 0:\n",
    "        print(f\"\\n  ⚠ No thresholds in range [{MIN_CLUSTERING_THRESHOLD}, {MAX_CLUSTERING_THRESHOLD}]\")\n",
    "        print(f\"  Using middle of range as fallback\")\n",
    "        chosen_threshold = (MIN_CLUSTERING_THRESHOLD + MAX_CLUSTERING_THRESHOLD) / 2\n",
    "    else:\n",
    "        # Prefer thresholds giving target cluster count\n",
    "        in_target = constrained[\n",
    "            (constrained['n_clusters'] >= TARGET_CLUSTER_RANGE[0]) &\n",
    "            (constrained['n_clusters'] <= TARGET_CLUSTER_RANGE[1])\n",
    "        ]\n",
    "\n",
    "        if len(in_target) > 0:\n",
    "            # Among those in target range, pick best silhouette (if positive)\n",
    "            positive_sil = in_target[in_target['silhouette'] > 0]\n",
    "            if len(positive_sil) > 0:\n",
    "                best_idx = positive_sil['silhouette'].idxmax()\n",
    "                chosen_threshold = positive_sil.loc[best_idx, 'threshold']\n",
    "                print(f\"\\n  Found {len(in_target)} thresholds in target cluster range\")\n",
    "                print(f\"  Best silhouette in range: {positive_sil.loc[best_idx, 'silhouette']:.3f}\")\n",
    "            else:\n",
    "                # No positive silhouette, just pick middle of valid thresholds\n",
    "                chosen_threshold = in_target['threshold'].median()\n",
    "                print(f\"\\n  No positive silhouette scores, using median threshold\")\n",
    "        else:\n",
    "            # No threshold gives target clusters - pick closest to target range\n",
    "            target_mid = (TARGET_CLUSTER_RANGE[0] + TARGET_CLUSTER_RANGE[1]) / 2\n",
    "            constrained['dist_to_target'] = abs(constrained['n_clusters'] - target_mid)\n",
    "            best_idx = constrained['dist_to_target'].idxmin()\n",
    "            chosen_threshold = constrained.loc[best_idx, 'threshold']\n",
    "            print(f\"\\n  No threshold in target cluster range\")\n",
    "            print(f\"  Closest to target ({target_mid:.0f} clusters): threshold {chosen_threshold}\")\n",
    "\n",
    "    print(f\"\\n>>> CHOSEN THRESHOLD: {chosen_threshold}\")\n",
    "\n",
    "    # Get final cluster assignments\n",
    "    cluster_labels = fcluster(linkage_matrix, t=chosen_threshold, criterion='distance')\n",
    "\n",
    "    # Create cluster DataFrame\n",
    "    cluster_df = pd.DataFrame({\n",
    "        'Feature': feature_cols,\n",
    "        'Cluster': cluster_labels\n",
    "    })\n",
    "\n",
    "    n_clusters = len(np.unique(cluster_labels))\n",
    "    cluster_sizes = cluster_df['Cluster'].value_counts()\n",
    "\n",
    "    print(f\"\\nCluster summary at threshold {chosen_threshold}:\")\n",
    "    print(f\"  Total clusters: {n_clusters}\")\n",
    "    print(f\"  Singletons: {(cluster_sizes == 1).sum()}\")\n",
    "    print(f\"  Largest cluster: {cluster_sizes.max()} features\")\n",
    "    print(f\"  Mean cluster size: {cluster_sizes.mean():.1f}\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    save_checkpoint(\"step1_3_clusters\", {\n",
    "        'linkage_matrix': linkage_matrix,\n",
    "        'chosen_threshold': chosen_threshold,\n",
    "        'cluster_labels': cluster_labels,\n",
    "        'cluster_df': cluster_df,\n",
    "        'threshold_results': threshold_results\n",
    "    })\n",
    "\n",
    "print(f\"\\n✓ Clustering complete: {len(cluster_df['Cluster'].unique())} clusters at threshold {chosen_threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e7ea406-9692-4025-a8d1-42706e6599b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Visualize Clustering Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "485742fb-abf3-46a8-8b50-5610e574237c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot threshold analysis\n",
    "threshold_df = pd.DataFrame(threshold_results)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Silhouette scores\n",
    "axes[0].plot(threshold_df['threshold'], threshold_df['silhouette'], 'bo-')\n",
    "axes[0].axvline(x=chosen_threshold, color='r', linestyle='--', label=f'Chosen: {chosen_threshold}')\n",
    "axes[0].set_xlabel('Threshold')\n",
    "axes[0].set_ylabel('Silhouette Score')\n",
    "axes[0].set_title('Silhouette Score vs Threshold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Number of clusters\n",
    "axes[1].plot(threshold_df['threshold'], threshold_df['n_clusters'], 'go-')\n",
    "axes[1].axvline(x=chosen_threshold, color='r', linestyle='--', label=f'Chosen: {chosen_threshold}')\n",
    "axes[1].set_xlabel('Threshold')\n",
    "axes[1].set_ylabel('Number of Clusters')\n",
    "axes[1].set_title('Cluster Count vs Threshold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Singleton percentage\n",
    "axes[2].plot(threshold_df['threshold'], threshold_df['pct_singletons'], 'mo-')\n",
    "axes[2].axvline(x=chosen_threshold, color='r', linestyle='--', label=f'Chosen: {chosen_threshold}')\n",
    "axes[2].set_xlabel('Threshold')\n",
    "axes[2].set_ylabel('% Singletons')\n",
    "axes[2].set_title('Singleton Percentage vs Threshold')\n",
    "axes[2].legend()\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'lucem_novis_standard_threshold_analysis.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Saved: {OUTPUT_DIR}/lucem_novis_standard_threshold_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9270ad6f-1341-42f3-8748-1e5b6f3185e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1.4: Train Baseline Model (All Features)\n",
    " \n",
    "Train XGBoost model with all features to establish baseline performance.\n",
    "Evaluation is performed on **train and validation sets only** - test set is held out\n",
    "until final model evaluation to prevent information leakage during feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88676c7e-6463-4560-ae97-9fb7cfd27f52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check if we can skip this step\n",
    "if checkpoint_exists(\"step1_4_baseline_model\") and not START_FRESH:\n",
    "    print_stage(\"STEP 1.4: TRAIN BASELINE MODEL (from checkpoint)\", 4, 7)\n",
    "    print_progress(\"Loading from checkpoint: step1_4_baseline_model\")\n",
    "    baseline_checkpoint = load_checkpoint(\"step1_4_baseline_model\")\n",
    "    baseline_model = baseline_checkpoint['model']\n",
    "    baseline_metrics = baseline_checkpoint['metrics']\n",
    "    print_progress(f\"Loaded baseline model: Val AUPRC = {baseline_metrics['val']['auprc']:.4f}\")\n",
    "else:\n",
    "    print_stage(\"STEP 1.4: TRAIN BASELINE MODEL (ALL FEATURES)\", 4, 7)\n",
    "\n",
    "    # Prepare data splits\n",
    "    train_mask = df_pandas['SPLIT'] == 'train'\n",
    "    val_mask = df_pandas['SPLIT'] == 'val'\n",
    "    test_mask = df_pandas['SPLIT'] == 'test'\n",
    "\n",
    "    X_train = df_pandas.loc[train_mask].copy()\n",
    "    y_train = df_pandas.loc[train_mask, 'FUTURE_CRC_EVENT'].copy()\n",
    "\n",
    "    X_val = df_pandas.loc[val_mask].copy()\n",
    "    y_val = df_pandas.loc[val_mask, 'FUTURE_CRC_EVENT'].copy()\n",
    "\n",
    "    X_test = df_pandas.loc[test_mask].copy()\n",
    "    y_test = df_pandas.loc[test_mask, 'FUTURE_CRC_EVENT'].copy()\n",
    "\n",
    "    print(f\"Training baseline model with {len(feature_cols)} features...\")\n",
    "    print(f\"  Train: {len(y_train):,} obs, {y_train.sum():,} events\")\n",
    "    print(f\"  Val: {len(y_val):,} obs, {y_val.sum():,} events\")\n",
    "    print(f\"  Test: {len(y_test):,} obs, {y_test.sum():,} events\")\n",
    "\n",
    "    start = time.time()\n",
    "    baseline_model = train_xgboost_model(\n",
    "        X_train, y_train, X_val, y_val,\n",
    "        feature_cols, scale_pos_weight\n",
    "    )\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"✓ Model trained in {elapsed:.1f}s (best iteration: {baseline_model.best_iteration})\")\n",
    "\n",
    "    # Evaluate on train/val only (test is held out until final evaluation)\n",
    "    print(\"\\nBaseline performance:\")\n",
    "    baseline_metrics = {\n",
    "        'train': evaluate_model(baseline_model, X_train, y_train, feature_cols, \"Train\"),\n",
    "        'val': evaluate_model(baseline_model, X_val, y_val, feature_cols, \"Val\")\n",
    "    }\n",
    "\n",
    "    # Calculate train-val gap\n",
    "    baseline_metrics['train_val_gap'] = baseline_metrics['train']['auprc'] - baseline_metrics['val']['auprc']\n",
    "    print(f\"  Train-Val Gap: {baseline_metrics['train_val_gap']:.4f}\")\n",
    "    print(\"  (Test set held out - evaluated only at final model)\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    save_checkpoint(\"step1_4_baseline_model\", {\n",
    "        'model': baseline_model,\n",
    "        'metrics': baseline_metrics\n",
    "    })\n",
    "\n",
    "    # Update tracking CSV\n",
    "    update_tracking_csv({\n",
    "        'phase': 'phase1',\n",
    "        'iteration': 'baseline',\n",
    "        'n_features': len(feature_cols),\n",
    "        'n_removed': 0,\n",
    "        'train_auprc': baseline_metrics['train']['auprc'],\n",
    "        'val_auprc': baseline_metrics['val']['auprc'],\n",
    "        'train_val_gap': baseline_metrics['train_val_gap'],\n",
    "        'val_drop_from_baseline': 0.0,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    })\n",
    "\n",
    "print(f\"\\n✓ Baseline model ready: Val AUPRC = {baseline_metrics['val']['auprc']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17721b28-8436-4931-9688-2e030d3fbe96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1.5: Compute SHAP with 2:1 Positive Weighting\n",
    " \n",
    "### Why SHAP is Computed on Validation Data\n",
    " \n",
    "We compute SHAP values on the validation set rather than the training set for two reasons:\n",
    " \n",
    "1. **Avoid overfitting to training quirks**: SHAP on training data would reflect feature\n",
    "   contributions to training patterns, including any noise the model memorized. Validation\n",
    "   data provides a cleaner signal of generalizable feature importance.\n",
    " \n",
    "2. **Consistent with early stopping**: The model was trained with early stopping on validation\n",
    "   performance, so its learned structure is already optimized for validation. SHAP on\n",
    "   validation reflects the model's actual generalization behavior.\n",
    " \n",
    "**Tradeoff**: The validation set has fewer positive cases (~1/3 of training), so importance\n",
    "estimates have higher variance for rare features. We mitigate this by keeping all positive\n",
    "cases in the SHAP computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ba941fd-8d91-4d87-8e68-ebce66795f30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check if we can skip this step\n",
    "if checkpoint_exists(\"step1_5_shap_phase1\") and not START_FRESH:\n",
    "    print_stage(\"STEP 1.5: COMPUTE SHAP VALUES (from checkpoint)\", 5, 7)\n",
    "    print_progress(\"Loading from checkpoint: step1_5_shap_phase1\")\n",
    "    shap_checkpoint = load_checkpoint(\"step1_5_shap_phase1\")\n",
    "    importance_df = shap_checkpoint['importance_df']\n",
    "    print_progress(f\"Loaded SHAP importance for {len(importance_df)} features\")\n",
    "else:\n",
    "    print_stage(\"STEP 1.5: COMPUTE SHAP VALUES (2:1 POSITIVE WEIGHTING)\", 5, 7)\n",
    "    print_progress(\"NOTE: SHAP computation is the slowest step - may take 10-30+ minutes\")\n",
    "\n",
    "    # Use validation set for SHAP\n",
    "    val_mask = df_pandas['SPLIT'] == 'val'\n",
    "    X_val = df_pandas.loc[val_mask].copy()\n",
    "    y_val = df_pandas.loc[val_mask, 'FUTURE_CRC_EVENT'].copy()\n",
    "\n",
    "    importance_df = compute_shap_values(baseline_model, X_val, y_val, feature_cols)\n",
    "\n",
    "    # Display top features\n",
    "    print(\"\\nTop 20 features by SHAP_Combined:\")\n",
    "    print(importance_df[['Rank', 'Feature', 'SHAP_Combined', 'SHAP_Ratio']].head(20).to_string(index=False))\n",
    "\n",
    "    # Save checkpoint\n",
    "    save_checkpoint(\"step1_5_shap_phase1\", {\n",
    "        'importance_df': importance_df\n",
    "    })\n",
    "\n",
    "print(f\"\\n✓ SHAP values computed for {len(importance_df)} features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2a6e437-693f-4839-bee0-cd7a7ab08643",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1.6: Select Cluster Representatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "468e609b-dca2-4827-b86c-3c03378f40e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check if we can skip this step\n",
    "if checkpoint_exists(\"step1_6_cluster_representatives\") and not START_FRESH:\n",
    "    print_stage(\"STEP 1.6: SELECT CLUSTER REPRESENTATIVES (from checkpoint)\", 6, 7)\n",
    "    print_progress(\"Loading from checkpoint: step1_6_cluster_representatives\")\n",
    "    rep_checkpoint = load_checkpoint(\"step1_6_cluster_representatives\")\n",
    "    selected_features = rep_checkpoint['selected_features']\n",
    "    selection_df = rep_checkpoint['selection_df']\n",
    "    print_progress(f\"Loaded {len(selected_features)} selected features\")\n",
    "else:\n",
    "    print_stage(\"STEP 1.6: SELECT CLUSTER REPRESENTATIVES\", 6, 7)\n",
    "\n",
    "    # Merge cluster assignments with SHAP importance\n",
    "    cluster_importance = cluster_df.merge(importance_df, on='Feature')\n",
    "\n",
    "    selected_features = []\n",
    "    selection_records = []\n",
    "\n",
    "    # For each cluster, select representative(s)\n",
    "    for cluster_id in sorted(cluster_importance['Cluster'].unique()):\n",
    "        cluster_features = cluster_importance[cluster_importance['Cluster'] == cluster_id].copy()\n",
    "        cluster_size = len(cluster_features)\n",
    "\n",
    "        # Sort by importance_ratio (descending) - prefer features predictive of positives\n",
    "        cluster_features = cluster_features.sort_values('SHAP_Ratio', ascending=False)\n",
    "\n",
    "        # Determine how many to keep (adaptive: drop at most 1-2 per cluster)\n",
    "        if cluster_size <= 2:\n",
    "            n_keep = cluster_size  # Keep all for tiny clusters\n",
    "        elif cluster_size <= 4:\n",
    "            n_keep = cluster_size - 1  # Drop at most 1\n",
    "        else:\n",
    "            n_keep = cluster_size - 2  # Drop at most 2 for larger clusters\n",
    "\n",
    "        # Select top feature(s)\n",
    "        kept = cluster_features.head(n_keep)\n",
    "\n",
    "        for _, row in kept.iterrows():\n",
    "            selected_features.append(row['Feature'])\n",
    "            selection_records.append({\n",
    "                'Feature': row['Feature'],\n",
    "                'Cluster': cluster_id,\n",
    "                'Cluster_Size': cluster_size,\n",
    "                'SHAP_Combined': row['SHAP_Combined'],\n",
    "                'SHAP_Ratio': row['SHAP_Ratio'],\n",
    "                'Selection_Reason': f\"Top by SHAP_Ratio in cluster of {cluster_size}\"\n",
    "            })\n",
    "\n",
    "    selection_df = pd.DataFrame(selection_records)\n",
    "\n",
    "    print(f\"\\nCluster representative selection:\")\n",
    "    print(f\"  Original features: {len(feature_cols)}\")\n",
    "    print(f\"  Clusters: {len(cluster_importance['Cluster'].unique())}\")\n",
    "    print(f\"  Selected features: {len(selected_features)}\")\n",
    "    print(f\"  Reduction: {len(feature_cols) - len(selected_features)} features removed ({(len(feature_cols) - len(selected_features))/len(feature_cols)*100:.1f}%)\")\n",
    "\n",
    "    # Show selection summary by cluster size\n",
    "    print(\"\\nSelection by cluster size:\")\n",
    "    for size_cat in ['1', '2-3', '4-7', '8+']:\n",
    "        if size_cat == '1':\n",
    "            mask = selection_df['Cluster_Size'] == 1\n",
    "        elif size_cat == '2-3':\n",
    "            mask = (selection_df['Cluster_Size'] >= 2) & (selection_df['Cluster_Size'] <= 3)\n",
    "        elif size_cat == '4-7':\n",
    "            mask = (selection_df['Cluster_Size'] >= 4) & (selection_df['Cluster_Size'] <= 7)\n",
    "        else:\n",
    "            mask = selection_df['Cluster_Size'] >= 8\n",
    "\n",
    "        n_selected = mask.sum()\n",
    "        if n_selected > 0:\n",
    "            print(f\"  Cluster size {size_cat}: {n_selected} features selected\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    save_checkpoint(\"step1_6_cluster_representatives\", {\n",
    "        'selected_features': selected_features,\n",
    "        'selection_df': selection_df\n",
    "    })\n",
    "\n",
    "print(f\"\\n✓ Selected {len(selected_features)} cluster representatives\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e5b9311-3f9e-40b9-b932-240253979269",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1.7: Phase 1 Validation Gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a71d4702-9f84-4309-a3c6-d6ac0cf0a677",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check if we can skip this step\n",
    "if checkpoint_exists(\"step1_7_phase1_complete\") and not START_FRESH:\n",
    "    print_stage(\"STEP 1.7: PHASE 1 VALIDATION GATE (from checkpoint)\", 7, 7)\n",
    "    print_progress(\"Loading from checkpoint: step1_7_phase1_complete\")\n",
    "    phase1_checkpoint = load_checkpoint(\"step1_7_phase1_complete\")\n",
    "    phase1_features = phase1_checkpoint['phase1_features']\n",
    "    phase1_metrics = phase1_checkpoint['phase1_metrics']\n",
    "    phase1_passed = phase1_checkpoint['phase1_passed']\n",
    "    print_progress(f\"Phase 1 complete: {len(phase1_features)} features, passed={phase1_passed}\")\n",
    "else:\n",
    "    print_stage(\"STEP 1.7: PHASE 1 VALIDATION GATE\", 7, 7)\n",
    "\n",
    "    # Prepare data\n",
    "    train_mask = df_pandas['SPLIT'] == 'train'\n",
    "    val_mask = df_pandas['SPLIT'] == 'val'\n",
    "    test_mask = df_pandas['SPLIT'] == 'test'\n",
    "\n",
    "    X_train = df_pandas.loc[train_mask].copy()\n",
    "    y_train = df_pandas.loc[train_mask, 'FUTURE_CRC_EVENT'].copy()\n",
    "\n",
    "    X_val = df_pandas.loc[val_mask].copy()\n",
    "    y_val = df_pandas.loc[val_mask, 'FUTURE_CRC_EVENT'].copy()\n",
    "\n",
    "    X_test = df_pandas.loc[test_mask].copy()\n",
    "    y_test = df_pandas.loc[test_mask, 'FUTURE_CRC_EVENT'].copy()\n",
    "\n",
    "    print(f\"Training model with {len(selected_features)} selected features...\")\n",
    "\n",
    "    # Train model with selected features\n",
    "    phase1_model = train_xgboost_model(\n",
    "        X_train, y_train, X_val, y_val,\n",
    "        selected_features, scale_pos_weight\n",
    "    )\n",
    "\n",
    "    # Evaluate on train/val only (test is held out until final evaluation)\n",
    "    print(\"\\nPhase 1 reduced model performance:\")\n",
    "    phase1_metrics = {\n",
    "        'train': evaluate_model(phase1_model, X_train, y_train, selected_features, \"Train\"),\n",
    "        'val': evaluate_model(phase1_model, X_val, y_val, selected_features, \"Val\")\n",
    "    }\n",
    "\n",
    "    phase1_metrics['train_val_gap'] = phase1_metrics['train']['auprc'] - phase1_metrics['val']['auprc']\n",
    "\n",
    "    # Calculate drops from baseline\n",
    "    val_drop = (baseline_metrics['val']['auprc'] - phase1_metrics['val']['auprc']) / baseline_metrics['val']['auprc']\n",
    "    gap_change = phase1_metrics['train_val_gap'] - baseline_metrics['train_val_gap']\n",
    "\n",
    "    print(f\"\\nValidation Gate Check:\")\n",
    "    print(f\"  Baseline Val AUPRC: {baseline_metrics['val']['auprc']:.4f}\")\n",
    "    print(f\"  Phase 1 Val AUPRC:  {phase1_metrics['val']['auprc']:.4f}\")\n",
    "    print(f\"  Val AUPRC Drop:     {val_drop*100:.2f}% (threshold: {PHASE1_MAX_VAL_DROP*100}%)\")\n",
    "    print(f\"  Train-Val Gap Change: {gap_change:.4f}\")\n",
    "\n",
    "    # Check if passed\n",
    "    phase1_passed = val_drop <= PHASE1_MAX_VAL_DROP\n",
    "\n",
    "    if phase1_passed:\n",
    "        print(f\"\\n✓ PHASE 1 VALIDATION GATE: PASSED\")\n",
    "        phase1_features = selected_features\n",
    "    else:\n",
    "        print(f\"\\n⚠ PHASE 1 VALIDATION GATE: FAILED\")\n",
    "        print(\"  Consider: keeping top 2 per cluster, or raising threshold\")\n",
    "        # For now, proceed with selected features but flag the issue\n",
    "        phase1_features = selected_features\n",
    "\n",
    "    # Save checkpoint\n",
    "    save_checkpoint(\"step1_7_phase1_complete\", {\n",
    "        'phase1_features': phase1_features,\n",
    "        'phase1_metrics': phase1_metrics,\n",
    "        'phase1_passed': phase1_passed,\n",
    "        'val_drop': val_drop,\n",
    "        'gap_change': gap_change\n",
    "    })\n",
    "\n",
    "    # Update tracking CSV\n",
    "    update_tracking_csv({\n",
    "        'phase': 'phase1',\n",
    "        'iteration': 'cluster_reduction',\n",
    "        'n_features': len(phase1_features),\n",
    "        'n_removed': len(feature_cols) - len(phase1_features),\n",
    "        'train_auprc': phase1_metrics['train']['auprc'],\n",
    "        'val_auprc': phase1_metrics['val']['auprc'],\n",
    "        'train_val_gap': phase1_metrics['train_val_gap'],\n",
    "        'val_drop_from_baseline': val_drop,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    })\n",
    "\n",
    "print(f\"\\n✓ Phase 1 complete: {len(phase1_features)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62a0f16a-e477-4072-b6ed-80fe80c61428",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8dddb1f-6ce7-4c81-8558-ccddfcce0e5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "# PHASE 2: Iterative SHAP Winnowing\n",
    "---\n",
    " \n",
    "Iteratively remove low-importance features while monitoring validation performance.\n",
    " \n",
    "**Key controls:**\n",
    "- **Validation gates**: Stop if val AUPRC drops >5% or train-val gap increases >0.02\n",
    "- **Protected features**: Never remove features in top 50% by SHAP ratio\n",
    "- **Test holdout**: All decisions based on validation metrics only; test evaluated at final model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1d84c68-e4c2-432d-ba52-a10455022730",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Phase 2 Iteration Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c64c3e9-80e1-4408-acca-5f7752822ea6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print_stage(\"PHASE 2: ITERATIVE SHAP WINNOWING\")\n",
    "\n",
    "# Initialize Phase 2\n",
    "current_features = phase1_features.copy()\n",
    "iteration = 0\n",
    "stop_reason = None\n",
    "prev_metrics = phase1_metrics.copy()  # Track previous iteration for comparison\n",
    "\n",
    "# Check for existing Phase 2 checkpoints to resume\n",
    "phase2_checkpoints = [cp for cp in list_checkpoints() if cp.startswith(\"step2_iter\") and cp.endswith(\"_complete\")]\n",
    "if phase2_checkpoints and not START_FRESH:\n",
    "    # Find the latest complete iteration\n",
    "    iter_nums = []\n",
    "    for cp in phase2_checkpoints:\n",
    "        try:\n",
    "            iter_num = int(cp.split(\"_\")[1].replace(\"iter\", \"\"))\n",
    "            iter_nums.append(iter_num)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    if iter_nums:\n",
    "        last_iter = max(iter_nums)\n",
    "        print_progress(f\"Found Phase 2 checkpoint at iteration {last_iter}. Resuming...\")\n",
    "\n",
    "        last_checkpoint = load_checkpoint(f\"step2_iter{last_iter}_complete\")\n",
    "        current_features = last_checkpoint['current_features']\n",
    "        iteration = last_iter\n",
    "\n",
    "        # Update prev_metrics from checkpoint for comparison\n",
    "        if 'metrics' in last_checkpoint:\n",
    "            prev_metrics = last_checkpoint['metrics'].copy()\n",
    "\n",
    "        # Check if we should stop\n",
    "        if last_checkpoint.get('stop_triggered', False):\n",
    "            stop_reason = last_checkpoint.get('stop_reason', 'Unknown')\n",
    "            print_progress(f\"Previous iteration triggered stop: {stop_reason}\")\n",
    "\n",
    "print_progress(f\"Starting Phase 2 from iteration {iteration}\")\n",
    "print_progress(f\"Current features: {len(current_features)}\")\n",
    "print_progress(f\"Max removals: {MAX_REMOVALS_EARLY} (iter 1-{LATE_PHASE_ITERATION}), {MAX_REMOVALS_LATE} (iter {LATE_PHASE_ITERATION+1}+)\")\n",
    "print_progress(f\"Min features threshold: {MIN_FEATURES_THRESHOLD}\")\n",
    "print_progress(\"Each iteration: Train model → Compute SHAP → Identify removals → Validate\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c08ef65-3afa-40ae-8cef-7de963394f1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Run Phase 2 Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a353abf5-e5b4-4c6d-9321-da04729bbb18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Prepare data splits (needed for iterations)\n",
    "train_mask = df_pandas['SPLIT'] == 'train'\n",
    "val_mask = df_pandas['SPLIT'] == 'val'\n",
    "test_mask = df_pandas['SPLIT'] == 'test'\n",
    "\n",
    "X_train = df_pandas.loc[train_mask].copy()\n",
    "y_train = df_pandas.loc[train_mask, 'FUTURE_CRC_EVENT'].copy()\n",
    "\n",
    "X_val = df_pandas.loc[val_mask].copy()\n",
    "y_val = df_pandas.loc[val_mask, 'FUTURE_CRC_EVENT'].copy()\n",
    "\n",
    "X_test = df_pandas.loc[test_mask].copy()\n",
    "y_test = df_pandas.loc[test_mask, 'FUTURE_CRC_EVENT'].copy()\n",
    "\n",
    "# Main iteration loop\n",
    "while stop_reason is None:\n",
    "    iteration += 1\n",
    "    iter_start_time = time.time()\n",
    "\n",
    "    # Determine max removals for this iteration (gradual reduction)\n",
    "    max_removals_this_iter = MAX_REMOVALS_EARLY if iteration <= LATE_PHASE_ITERATION else MAX_REMOVALS_LATE\n",
    "\n",
    "    print_stage(f\"PHASE 2 - ITERATION {iteration}\")\n",
    "    print_progress(f\"Current features: {len(current_features)}\")\n",
    "    print_progress(f\"Target: Remove up to {max_removals_this_iter} low-value features\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # Step 2.1: Train Model\n",
    "    # =========================================================================\n",
    "    checkpoint_name = f\"step2_iter{iteration}_model\"\n",
    "\n",
    "    if checkpoint_exists(checkpoint_name) and not START_FRESH:\n",
    "        print_progress(f\"Step 2.{iteration}.1: Loading model from checkpoint...\")\n",
    "        model_checkpoint = load_checkpoint(checkpoint_name)\n",
    "        iter_model = model_checkpoint['model']\n",
    "        iter_metrics = model_checkpoint['metrics']\n",
    "        # Evaluate test if not in checkpoint (for older checkpoints)\n",
    "        if 'test' not in iter_metrics:\n",
    "            iter_metrics['test'] = evaluate_model(iter_model, X_test, y_test, current_features, \"Test\")\n",
    "        print_progress(f\"Model loaded: Val AUPRC = {iter_metrics['val']['auprc']:.4f}, Test AUPRC = {iter_metrics['test']['auprc']:.4f}\")\n",
    "    else:\n",
    "        print_progress(f\"Step 2.{iteration}.1: Training model with {len(current_features)} features...\")\n",
    "\n",
    "        iter_model = train_xgboost_model(\n",
    "            X_train, y_train, X_val, y_val,\n",
    "            current_features, scale_pos_weight\n",
    "        )\n",
    "\n",
    "        iter_metrics = {\n",
    "            'train': evaluate_model(iter_model, X_train, y_train, current_features, \"Train\"),\n",
    "            'val': evaluate_model(iter_model, X_val, y_val, current_features, \"Val\"),\n",
    "            'test': evaluate_model(iter_model, X_test, y_test, current_features, \"Test\")  # Track for post-hoc analysis\n",
    "        }\n",
    "        iter_metrics['train_val_gap'] = iter_metrics['train']['auprc'] - iter_metrics['val']['auprc']\n",
    "        print(f\"  Test AUPRC: {iter_metrics['test']['auprc']:.4f} (tracking only - not used for decisions)\")\n",
    "\n",
    "        save_checkpoint(checkpoint_name, {\n",
    "            'model': iter_model,\n",
    "            'metrics': iter_metrics\n",
    "        })\n",
    "\n",
    "    # =========================================================================\n",
    "    # Step 2.2: Compute SHAP\n",
    "    # =========================================================================\n",
    "    checkpoint_name = f\"step2_iter{iteration}_shap\"\n",
    "\n",
    "    if checkpoint_exists(checkpoint_name) and not START_FRESH:\n",
    "        print_progress(f\"Step 2.{iteration}.2: Loading SHAP values from checkpoint...\")\n",
    "        shap_checkpoint = load_checkpoint(checkpoint_name)\n",
    "        iter_importance_df = shap_checkpoint['importance_df']\n",
    "        print_progress(f\"SHAP loaded for {len(iter_importance_df)} features\")\n",
    "    else:\n",
    "        print_progress(f\"Step 2.{iteration}.2: Computing SHAP values (this is the slow step)...\")\n",
    "\n",
    "        iter_importance_df = compute_shap_values(iter_model, X_val, y_val, current_features)\n",
    "\n",
    "        save_checkpoint(checkpoint_name, {\n",
    "            'importance_df': iter_importance_df\n",
    "        })\n",
    "\n",
    "    # =========================================================================\n",
    "    # Step 2.3: Identify Removal Candidates (ORIGINAL METHODOLOGY)\n",
    "    # =========================================================================\n",
    "    # From CRC_ITER1_MODEL-PREVALENCE.py:\n",
    "    # Feature must meet AT LEAST 2 of 3 criteria:\n",
    "    #   1. Near-zero SHAP importance (< ZERO_SHAP_THRESHOLD)\n",
    "    #   2. Negative-biased ratio (< NEG_BIAS_RATIO_THRESHOLD)\n",
    "    #   3. Bottom percentile by SHAP (< BOTTOM_PERCENTILE)\n",
    "    #\n",
    "    # Cluster-specific removal caps:\n",
    "    #   - Singleton (1): max 1 removal\n",
    "    #   - Small (2-3): max 2 removals, leave at least 1\n",
    "    #   - Medium (4-7): max 2 removals, leave at least 3\n",
    "    #   - Large (8+): max 3 removals, leave at least 5\n",
    "    #   - High-importance clusters (top 20%): max 1 removal\n",
    "    # =========================================================================\n",
    "    print_progress(f\"Step 2.{iteration}.3: Identifying removal candidates (original methodology)...\")\n",
    "\n",
    "    n_features = len(current_features)\n",
    "\n",
    "    # Get cluster assignments for current features\n",
    "    current_selection = selection_df[selection_df['Feature'].isin(current_features)].copy()\n",
    "\n",
    "    # Merge SHAP importance with cluster info\n",
    "    iter_importance_with_cluster = iter_importance_df.merge(\n",
    "        current_selection[['Feature', 'Cluster']],\n",
    "        on='Feature',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Calculate features meeting each criterion\n",
    "    # -------------------------------------------------------------------------\n",
    "    zero_importance_features = set(\n",
    "        iter_importance_df[iter_importance_df['SHAP_Combined'] < ZERO_SHAP_THRESHOLD]['Feature']\n",
    "    )\n",
    "    neg_biased_features = set(\n",
    "        iter_importance_df[iter_importance_df['SHAP_Ratio'] < NEG_BIAS_RATIO_THRESHOLD]['Feature']\n",
    "    )\n",
    "    importance_percentile = iter_importance_df['SHAP_Combined'].quantile(BOTTOM_PERCENTILE / 100)\n",
    "    bottom_features = set(\n",
    "        iter_importance_df[iter_importance_df['SHAP_Combined'] < importance_percentile]['Feature']\n",
    "    )\n",
    "\n",
    "    # Feature must meet AT LEAST 2 of 3 criteria\n",
    "    features_meeting_two_criteria = (\n",
    "        (zero_importance_features & neg_biased_features) |\n",
    "        (zero_importance_features & bottom_features) |\n",
    "        (neg_biased_features & bottom_features)\n",
    "    )\n",
    "    features_meeting_all = zero_importance_features & neg_biased_features & bottom_features\n",
    "\n",
    "    # Remove clinical protected from candidates\n",
    "    features_flagged = features_meeting_two_criteria.copy()\n",
    "\n",
    "    print(f\"  Removal criteria:\")\n",
    "    print(f\"    Near-zero SHAP (<{ZERO_SHAP_THRESHOLD}): {len(zero_importance_features)}\")\n",
    "    print(f\"    Negative-biased (<{NEG_BIAS_RATIO_THRESHOLD}): {len(neg_biased_features)}\")\n",
    "    print(f\"    Bottom {BOTTOM_PERCENTILE}% (threshold={importance_percentile:.6f}): {len(bottom_features)}\")\n",
    "    print(f\"    Meeting ALL 3 criteria: {len(features_meeting_all)}\")\n",
    "    print(f\"    Meeting 2+ criteria: {len(features_meeting_two_criteria)}\")\n",
    "    print(f\"    Flagged (excl. clinical): {len(features_flagged)}\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Analyze flagged features by cluster with removal caps\n",
    "    # -------------------------------------------------------------------------\n",
    "    # IMPORTANT: Use ORIGINAL cluster sizes from Phase 1 (stored in selection_df)\n",
    "    # not current sizes. This matches the original methodology which reads from\n",
    "    # the cluster CSV file each iteration.\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # Get original cluster sizes from selection_df (created in Phase 1)\n",
    "    original_cluster_sizes = selection_df.groupby('Cluster')['Cluster_Size'].first().to_dict()\n",
    "\n",
    "    cluster_removal_candidates = {}\n",
    "\n",
    "    for cluster_id in iter_importance_with_cluster['Cluster'].dropna().unique():\n",
    "        cluster_features = iter_importance_with_cluster[iter_importance_with_cluster['Cluster'] == cluster_id]\n",
    "\n",
    "        # Use ORIGINAL cluster size, not current size\n",
    "        cluster_size = original_cluster_sizes.get(cluster_id, len(cluster_features))\n",
    "\n",
    "        # Find flagged features in this cluster\n",
    "        flagged_in_cluster = list(set(cluster_features['Feature'].tolist()) & features_flagged)\n",
    "\n",
    "        if not flagged_in_cluster:\n",
    "            continue\n",
    "\n",
    "        # Get SHAP scores for flagged features\n",
    "        flagged_with_scores = []\n",
    "        for feat in flagged_in_cluster:\n",
    "            feat_row = iter_importance_df[iter_importance_df['Feature'] == feat]\n",
    "            if len(feat_row) > 0:\n",
    "                shap_val = feat_row['SHAP_Combined'].values[0]\n",
    "                ratio_val = feat_row['SHAP_Ratio'].values[0]\n",
    "                meets_all = feat in features_meeting_all\n",
    "                flagged_with_scores.append((feat, shap_val, ratio_val, meets_all))\n",
    "\n",
    "        # Sort by: (1) meets all criteria first, then (2) SHAP score ascending\n",
    "        flagged_with_scores.sort(key=lambda x: (not x[3], x[1]))\n",
    "\n",
    "        cluster_removal_candidates[cluster_id] = {\n",
    "            'size': cluster_size,  # Original size from Phase 1\n",
    "            'current_size': len(cluster_features),  # Current size for logging\n",
    "            'flagged': flagged_with_scores,\n",
    "            'max_shap': cluster_features['SHAP_Combined'].max()\n",
    "        }\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Apply cluster removal rules (from original methodology)\n",
    "    # -------------------------------------------------------------------------\n",
    "    removals_by_priority = []\n",
    "    shap_80th = iter_importance_df['SHAP_Combined'].quantile(0.80)\n",
    "\n",
    "    for cluster_id, cluster_info in cluster_removal_candidates.items():\n",
    "        cluster_size = cluster_info['size']\n",
    "        flagged_list = cluster_info['flagged']\n",
    "        max_shap = cluster_info['max_shap']\n",
    "\n",
    "        # Cluster-specific removal caps\n",
    "        if cluster_size == 1:\n",
    "            max_remove = 1  # Singleton - can remove if flagged\n",
    "        elif cluster_size <= 3:\n",
    "            max_remove = min(2, cluster_size - 1)  # Small - max 2, leave at least 1\n",
    "        elif cluster_size <= 7:\n",
    "            max_remove = min(2, cluster_size - 3) if cluster_size > 3 else 0  # Medium\n",
    "        else:\n",
    "            max_remove = min(3, cluster_size - 5) if cluster_size > 5 else 0  # Large\n",
    "\n",
    "        # Additional protection for high-importance clusters\n",
    "        if max_shap > shap_80th:\n",
    "            max_remove = min(max_remove, 1)\n",
    "\n",
    "        # Select worst features up to limit\n",
    "        for i, (feat, shap_val, ratio_val, meets_all) in enumerate(flagged_list[:max_remove]):\n",
    "            priority_score = shap_val * 1000 + (0 if meets_all else 1)\n",
    "            removals_by_priority.append((feat, shap_val, ratio_val, cluster_id, cluster_size, priority_score))\n",
    "\n",
    "    # Sort by priority score and apply global limit (iteration-aware)\n",
    "    removals_by_priority.sort(key=lambda x: x[5])\n",
    "    final_removals = removals_by_priority[:max_removals_this_iter]\n",
    "\n",
    "    features_to_remove = [feat for feat, _, _, _, _, _ in final_removals]\n",
    "\n",
    "    # Summary statistics\n",
    "    print(f\"\\n  Cluster-aware removal:\")\n",
    "    print(f\"    Total flagged across clusters: {len(removals_by_priority)}\")\n",
    "    print(f\"    After global cap ({max_removals_this_iter}): {len(features_to_remove)}\")\n",
    "\n",
    "    if features_to_remove:\n",
    "        print(f\"\\n  Features to remove ({len(features_to_remove)}):\")\n",
    "        for feat, shap_val, ratio_val, cluster_id, cluster_size, _ in final_removals[:10]:\n",
    "            print(f\"    - {feat:<45} SHAP={shap_val:.6f}, Ratio={ratio_val:.3f}, Cluster {cluster_id} (size={cluster_size})\")\n",
    "        if len(final_removals) > 10:\n",
    "            print(f\"    ... and {len(final_removals) - 10} more\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # Step 2.4: Validation Gate\n",
    "    # =========================================================================\n",
    "    print_progress(f\"Step 2.{iteration}.4: Checking validation gates...\")\n",
    "\n",
    "    # Track metrics for post-hoc analysis (NOT used for stopping decisions)\n",
    "    val_change = iter_metrics['val']['auprc'] - prev_metrics['val']['auprc']\n",
    "    test_change = iter_metrics['test']['auprc'] - prev_metrics.get('test', {}).get('auprc', iter_metrics['test']['auprc'])\n",
    "    val_drop_from_baseline = (baseline_metrics['val']['auprc'] - iter_metrics['val']['auprc']) / baseline_metrics['val']['auprc']\n",
    "\n",
    "    print(f\"  Val AUPRC:  {iter_metrics['val']['auprc']:.4f} (change: {val_change:+.4f})\")\n",
    "    print(f\"  Test AUPRC: {iter_metrics['test']['auprc']:.4f} (change: {test_change:+.4f})\")\n",
    "    print(f\"  Train-Val Gap: {iter_metrics['train_val_gap']:.4f}\")\n",
    "    print(f\"  Features remaining: {len(current_features)}\")\n",
    "\n",
    "    # Simple stop conditions - run until we hit limits\n",
    "    # Metrics are tracked but NOT used to influence stopping\n",
    "    if len(current_features) - len(features_to_remove) < MIN_FEATURES_THRESHOLD:\n",
    "        stop_reason = f\"Would go below {MIN_FEATURES_THRESHOLD} features\"\n",
    "    elif len(features_to_remove) == 0:\n",
    "        if len(current_features) > MIN_FEATURES_THRESHOLD + 5:\n",
    "            # Force-remove the single lowest-SHAP feature (excluding clinical)\n",
    "            lowest_features = iter_importance_df.nsmallest(5, 'SHAP_Combined')['Feature'].tolist()\n",
    "            features_to_remove = lowest_features\n",
    "            print(f\"  No features meet 2-of-3 criteria -- force-removing {len(lowest_features)} lowest: {lowest_features}\")\n",
    "        else:\n",
    "            stop_reason = \"No features meet removal criteria\"\n",
    "\n",
    "    # =========================================================================\n",
    "    # Step 2.5: Log & Checkpoint\n",
    "    # =========================================================================\n",
    "\n",
    "    # Update tracking CSV - includes test for post-hoc sweet spot analysis\n",
    "    update_tracking_csv({\n",
    "        'phase': 'phase2',\n",
    "        'iteration': iteration,\n",
    "        'n_features': len(current_features),\n",
    "        'n_removed': len(features_to_remove) if stop_reason is None else 0,\n",
    "        'train_auprc': iter_metrics['train']['auprc'],\n",
    "        'val_auprc': iter_metrics['val']['auprc'],\n",
    "        'test_auprc': iter_metrics['test']['auprc'],\n",
    "        'train_val_gap': iter_metrics['train_val_gap'],\n",
    "        'val_change_from_prev': val_change,\n",
    "        'test_change_from_prev': test_change,\n",
    "        'val_drop_from_baseline': val_drop_from_baseline,\n",
    "        'stop_triggered': stop_reason is not None,\n",
    "        'stop_reason': stop_reason if stop_reason else '',\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    })\n",
    "\n",
    "    if stop_reason:\n",
    "        print(f\"\\n>>> STOP CONDITION TRIGGERED: {stop_reason}\")\n",
    "        print(f\">>> Keeping features from iteration {iteration}\")\n",
    "\n",
    "        # Save final checkpoint\n",
    "        save_checkpoint(f\"step2_iter{iteration}_complete\", {\n",
    "            'current_features': current_features,\n",
    "            'metrics': iter_metrics,\n",
    "            'features_removed': [],\n",
    "            'stop_triggered': True,\n",
    "            'stop_reason': stop_reason\n",
    "        })\n",
    "        break\n",
    "    else:\n",
    "        # Remove features and continue\n",
    "        print(f\"\\n>>> Removing {len(features_to_remove)} features:\")\n",
    "        for feat in features_to_remove[:10]:\n",
    "            shap_val = iter_importance_df[iter_importance_df['Feature'] == feat]['SHAP_Combined'].values[0]\n",
    "            ratio_val = iter_importance_df[iter_importance_df['Feature'] == feat]['SHAP_Ratio'].values[0]\n",
    "            print(f\"    - {feat} (SHAP={shap_val:.6f}, Ratio={ratio_val:.3f})\")\n",
    "        if len(features_to_remove) > 10:\n",
    "            print(f\"    ... and {len(features_to_remove) - 10} more\")\n",
    "\n",
    "        # Update feature list\n",
    "        previous_features = current_features.copy()\n",
    "        current_features = [f for f in current_features if f not in features_to_remove]\n",
    "\n",
    "        # Save checkpoint\n",
    "        save_checkpoint(f\"step2_iter{iteration}_complete\", {\n",
    "            'current_features': current_features,\n",
    "            'previous_features': previous_features,\n",
    "            'metrics': iter_metrics,\n",
    "            'features_removed': features_to_remove,\n",
    "            'stop_triggered': False,\n",
    "            'stop_reason': None\n",
    "        })\n",
    "\n",
    "        # Update previous metrics for next iteration comparison\n",
    "        prev_metrics = iter_metrics.copy()\n",
    "\n",
    "        iter_elapsed = time.time() - iter_start_time\n",
    "        print_progress(f\"Iteration {iteration} complete in {iter_elapsed:.1f}s. Features: {len(previous_features)} → {len(current_features)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b3b4cc4-d30f-4e82-b29e-fa8c457e1878",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Compile Feature Lists by Iteration\n",
    "\n",
    "Creates a comprehensive record of which features were used at each iteration of the feature selection process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72faad8f-a905-418e-a9e3-8276c9dfb5c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def compile_features_by_iteration():\n",
    "    \"\"\"\n",
    "    Extract and compile the feature list from each iteration checkpoint\n",
    "    into a single comprehensive JSON file.\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"COMPILING FEATURE LISTS BY ITERATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    all_iterations_features = {}\n",
    "    \n",
    "    # Phase 1: Initial features\n",
    "    if checkpoint_exists(\"step1_1_data\"):\n",
    "        cp = load_checkpoint(\"step1_1_data\")\n",
    "        all_iterations_features[\"phase1_initial\"] = {\n",
    "            \"description\": \"All features after data load (before any selection)\",\n",
    "            \"n_features\": len(cp['feature_cols']),\n",
    "            \"features\": sorted(cp['feature_cols'])\n",
    "        }\n",
    "        print(f\"  ✓ Phase 1 Initial: {len(cp['feature_cols'])} features\")\n",
    "    \n",
    "    # Phase 1: After clustering\n",
    "    if checkpoint_exists(\"step1_7_phase1_complete\"):\n",
    "        cp = load_checkpoint(\"step1_7_phase1_complete\")\n",
    "        all_iterations_features[\"phase1_clustered\"] = {\n",
    "            \"description\": \"Features after cluster-based reduction (Phase 1 complete)\",\n",
    "            \"n_features\": len(cp['phase1_features']),\n",
    "            \"features\": sorted(cp['phase1_features'])\n",
    "        }\n",
    "        print(f\"  ✓ Phase 1 Clustered: {len(cp['phase1_features'])} features\")\n",
    "    \n",
    "    # Phase 2: Each iteration\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        iteration += 1\n",
    "        checkpoint_name = f\"step2_iter{iteration}_complete\"\n",
    "        \n",
    "        if not checkpoint_exists(checkpoint_name):\n",
    "            break\n",
    "        \n",
    "        cp = load_checkpoint(checkpoint_name)\n",
    "        \n",
    "        # Features going INTO this iteration (before removal)\n",
    "        if 'previous_features' in cp:\n",
    "            input_features = cp['previous_features']\n",
    "        elif iteration == 1 and checkpoint_exists(\"step1_7_phase1_complete\"):\n",
    "            phase1_cp = load_checkpoint(\"step1_7_phase1_complete\")\n",
    "            input_features = phase1_cp['phase1_features']\n",
    "        else:\n",
    "            input_features = cp['current_features']\n",
    "        \n",
    "        # Features AFTER this iteration (after removal)\n",
    "        output_features = cp['current_features']\n",
    "        features_removed = cp.get('features_removed', [])\n",
    "        \n",
    "        all_iterations_features[f\"phase2_iter{iteration}_input\"] = {\n",
    "            \"description\": f\"Features at START of iteration {iteration} (before removal)\",\n",
    "            \"n_features\": len(input_features),\n",
    "            \"features\": sorted(input_features)\n",
    "        }\n",
    "        \n",
    "        all_iterations_features[f\"phase2_iter{iteration}_output\"] = {\n",
    "            \"description\": f\"Features at END of iteration {iteration} (after removal)\",\n",
    "            \"n_features\": len(output_features),\n",
    "            \"n_removed\": len(features_removed),\n",
    "            \"features_removed\": sorted(features_removed) if features_removed else [],\n",
    "            \"features\": sorted(output_features)\n",
    "        }\n",
    "        \n",
    "        stop_triggered = cp.get('stop_triggered', False)\n",
    "        stop_reason = cp.get('stop_reason', '')\n",
    "        \n",
    "        status = \"STOPPED\" if stop_triggered else \"continued\"\n",
    "        print(f\"  ✓ Phase 2 Iter {iteration}: {len(input_features)} → {len(output_features)} features ({len(features_removed)} removed) [{status}]\")\n",
    "        \n",
    "        if stop_triggered:\n",
    "            all_iterations_features[f\"phase2_iter{iteration}_output\"][\"stop_reason\"] = stop_reason\n",
    "            break\n",
    "    \n",
    "    # Add summary\n",
    "    all_iterations_features[\"_summary\"] = {\n",
    "        \"total_iterations\": iteration,\n",
    "        \"initial_features\": all_iterations_features.get(\"phase1_initial\", {}).get(\"n_features\", 0),\n",
    "        \"final_features\": len(output_features) if 'output_features' in dir() else 0,\n",
    "        \"generated_at\": datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    # Save to JSON\n",
    "    output_path = os.path.join(OUTPUT_DIR, \"lucem_novis_standard_features_by_iteration.json\")\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(all_iterations_features, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n✓ Saved comprehensive feature list: {output_path}\")\n",
    "    \n",
    "    # Also save a simpler CSV format for quick reference\n",
    "    csv_rows = []\n",
    "    for key, data in all_iterations_features.items():\n",
    "        if key.startswith(\"_\"):\n",
    "            continue\n",
    "        csv_rows.append({\n",
    "            \"stage\": key,\n",
    "            \"n_features\": data.get(\"n_features\", 0),\n",
    "            \"n_removed\": data.get(\"n_removed\", 0),\n",
    "            \"description\": data.get(\"description\", \"\")\n",
    "        })\n",
    "    \n",
    "    csv_path = os.path.join(OUTPUT_DIR, \"lucem_novis_standard_features_by_iteration_summary.csv\")\n",
    "    pd.DataFrame(csv_rows).to_csv(csv_path, index=False)\n",
    "    print(f\"✓ Saved summary CSV: {csv_path}\")\n",
    "    \n",
    "    return all_iterations_features\n",
    "\n",
    "# Run the compilation\n",
    "features_by_iteration = compile_features_by_iteration()\n",
    "\n",
    "# Display a quick summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE COUNT PROGRESSION\")\n",
    "print(\"=\"*70)\n",
    "for key in sorted(features_by_iteration.keys()):\n",
    "    if key.startswith(\"_\"):\n",
    "        continue\n",
    "    data = features_by_iteration[key]\n",
    "    n = data.get(\"n_features\", 0)\n",
    "    removed = data.get(\"n_removed\", 0)\n",
    "    removed_str = f\" (-{removed})\" if removed > 0 else \"\"\n",
    "    print(f\"  {key:<30} {n:>4} features{removed_str}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74bcc342-8a2c-4813-a025-c4d33c0e6415",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "# Final Results\n",
    "---\n",
    " \n",
    "Train final model with selected features and evaluate performance.\n",
    "**This is the first and only time the test set is evaluated** - all feature selection\n",
    "decisions were made using validation metrics only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e5f178c-de2e-4626-ba14-3c96421b8b5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print_stage(\"FINAL RESULTS\")\n",
    "\n",
    "final_features = current_features\n",
    "\n",
    "print(f\"\\nFeature reduction summary:\")\n",
    "print(f\"  Initial features: {len(feature_cols)}\")\n",
    "print(f\"  After Phase 1 (clustering): {len(phase1_features)}\")\n",
    "print(f\"  After Phase 2 (winnowing): {len(final_features)}\")\n",
    "print(f\"  Total reduction: {len(feature_cols) - len(final_features)} features ({(len(feature_cols) - len(final_features))/len(feature_cols)*100:.1f}%)\")\n",
    "\n",
    "if stop_reason:\n",
    "    print(f\"\\nStop reason: {stop_reason}\")\n",
    "\n",
    "# Train final model\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"FINAL MODEL PERFORMANCE\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "final_model = train_xgboost_model(\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    final_features, scale_pos_weight\n",
    ")\n",
    "\n",
    "print(\"\\nFinal model metrics (train/val):\")\n",
    "final_metrics = {\n",
    "    'train': evaluate_model(final_model, X_train, y_train, final_features, \"Train\"),\n",
    "    'val': evaluate_model(final_model, X_val, y_val, final_features, \"Val\")\n",
    "}\n",
    "\n",
    "# TEST SET EVALUATION - Final model\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL MODEL TEST SET EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "print(\"Note: Test AUPRC was tracked at every iteration in lucem_novis_standard_iteration_tracking.csv\")\n",
    "print(\"Use that file to find the optimal stopping point (sweet spot).\")\n",
    "final_metrics['test'] = evaluate_model(final_model, X_test, y_test, final_features, \"Test\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"COMPARISON: BASELINE vs FINAL\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Metric':<20} {'Baseline':>12} {'Final':>12} {'Change':>12}\")\n",
    "print(\"-\"*56)\n",
    "print(f\"{'Features':<20} {len(feature_cols):>12} {len(final_features):>12} {len(final_features) - len(feature_cols):>+12}\")\n",
    "print(f\"{'Val AUPRC':<20} {baseline_metrics['val']['auprc']:>12.4f} {final_metrics['val']['auprc']:>12.4f} {final_metrics['val']['auprc'] - baseline_metrics['val']['auprc']:>+12.4f}\")\n",
    "print(f\"{'Test AUPRC':<20} {'--':>12} {final_metrics['test']['auprc']:>12.4f}\")\n",
    "print(f\"{'Train-Val Gap':<20} {baseline_metrics['train_val_gap']:>12.4f} {final_metrics['train']['auprc'] - final_metrics['val']['auprc']:>12.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c7086c7-55ba-43b9-85d6-a68326c93e1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Lift Analysis: Overall and By Quarter\n",
    "\n",
    "**Lift** = AUPRC / baseline positive rate. A lift of 10x means the model's precision-recall\n",
    "performance is 10x better than random guessing. Evaluating by quarter checks whether\n",
    "the model degrades over time (temporal stability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4cfac36-87e9-48f9-bd3e-182c11d09493",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Lift Analysis: Overall and By Quarter ---\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LIFT ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Overall lift (all splits)\n",
    "for split_name, mask in [('Train', df_pandas['SPLIT'] == 'train'),\n",
    "                         ('Val', df_pandas['SPLIT'] == 'val'),\n",
    "                         ('Test', df_pandas['SPLIT'] == 'test')]:\n",
    "    X_split = df_pandas.loc[mask]\n",
    "    y_split = df_pandas.loc[mask, 'FUTURE_CRC_EVENT']\n",
    "    y_pred = final_model.predict_proba(X_split[final_features])[:, 1]\n",
    "    base_rate = y_split.mean()\n",
    "    auprc = average_precision_score(y_split, y_pred)\n",
    "    lift = auprc / base_rate if base_rate > 0 else 0\n",
    "    print(f\"  {split_name:<6} Base rate: {base_rate:.4f} ({base_rate*100:.2f}%)  AUPRC: {auprc:.4f}  Lift: {lift:.1f}x\")\n",
    "\n",
    "# By-quarter lift on TEST set only\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"LIFT BY QUARTER (Test Set)\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "test_df = df_pandas[df_pandas['SPLIT'] == 'test'].copy()\n",
    "test_df['quarter'] = test_df['END_DTTM'].dt.to_period('Q').astype(str)\n",
    "test_df['y_pred'] = final_model.predict_proba(test_df[final_features])[:, 1]\n",
    "\n",
    "quarters = sorted(test_df['quarter'].unique())\n",
    "\n",
    "print(f\"  {'Quarter':<10} {'N':>8} {'Events':>8} {'Base Rate':>10} {'AUPRC':>8} {'Lift':>8}\")\n",
    "print(\"  \" + \"-\"*54)\n",
    "\n",
    "for q in quarters:\n",
    "    q_mask = test_df['quarter'] == q\n",
    "    y_q = test_df.loc[q_mask, 'FUTURE_CRC_EVENT']\n",
    "    pred_q = test_df.loc[q_mask, 'y_pred']\n",
    "    n = len(y_q)\n",
    "    n_events = int(y_q.sum())\n",
    "    base_rate = y_q.mean()\n",
    "\n",
    "    if n_events < 2:\n",
    "        print(f\"  {q:<10} {n:>8,} {n_events:>8} {base_rate:>10.4f} {'--':>8} {'--':>8}  (too few events)\")\n",
    "        continue\n",
    "\n",
    "    auprc = average_precision_score(y_q, pred_q)\n",
    "    lift = auprc / base_rate if base_rate > 0 else 0\n",
    "    print(f\"  {q:<10} {n:>8,} {n_events:>8} {base_rate:>10.4f} {auprc:>8.4f} {lift:>7.1f}x\")\n",
    "\n",
    "# Summary stats across quarters\n",
    "q_lifts = []\n",
    "for q in quarters:\n",
    "    q_mask = test_df['quarter'] == q\n",
    "    y_q = test_df.loc[q_mask, 'FUTURE_CRC_EVENT']\n",
    "    pred_q = test_df.loc[q_mask, 'y_pred']\n",
    "    if y_q.sum() >= 2:\n",
    "        auprc = average_precision_score(y_q, pred_q)\n",
    "        base_rate = y_q.mean()\n",
    "        q_lifts.append(auprc / base_rate if base_rate > 0 else 0)\n",
    "\n",
    "if len(q_lifts) >= 2:\n",
    "    print(f\"\\n  Quarterly lift range: {min(q_lifts):.1f}x - {max(q_lifts):.1f}x\")\n",
    "    print(f\"  Quarterly lift std:   {np.std(q_lifts):.2f}\")\n",
    "    if min(q_lifts) / max(q_lifts) > 0.7:\n",
    "        print(\"  ✓ Temporal stability looks good (min/max ratio > 0.7)\")\n",
    "    else:\n",
    "        print(\"  ⚠ Possible temporal instability (min/max ratio ≤ 0.7)\")\n",
    "\n",
    "# Save lift-by-quarter results to checkpoint\n",
    "lift_by_quarter = {}\n",
    "for q in quarters:\n",
    "    q_mask = test_df['quarter'] == q\n",
    "    y_q = test_df.loc[q_mask, 'FUTURE_CRC_EVENT']\n",
    "    pred_q = test_df.loc[q_mask, 'y_pred']\n",
    "    n_events = int(y_q.sum())\n",
    "    base_rate = float(y_q.mean())\n",
    "    if n_events >= 2:\n",
    "        auprc = float(average_precision_score(y_q, pred_q))\n",
    "        lift = auprc / base_rate if base_rate > 0 else 0\n",
    "    else:\n",
    "        auprc = None\n",
    "        lift = None\n",
    "    lift_by_quarter[q] = {\n",
    "        'n': int(q_mask.sum()),\n",
    "        'events': n_events,\n",
    "        'base_rate': base_rate,\n",
    "        'auprc': auprc,\n",
    "        'lift': lift\n",
    "    }\n",
    "\n",
    "save_checkpoint('lift_by_quarter', {'lift_by_quarter': lift_by_quarter, 'q_lifts_summary': {'min': min(q_lifts) if q_lifts else None, 'max': max(q_lifts) if q_lifts else None, 'std': float(np.std(q_lifts)) if len(q_lifts) >= 2 else None}})\n",
    "print(f\"\\n✓ Lift-by-quarter saved to checkpoint\")\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bac39533-63f2-4d2d-9c64-40a41be60ba8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Feature Dictionary\n",
    " Describes each surviving feature: what it measures, how it was derived, and its lookback window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2207e0f4-b755-44db-b40f-37f7810cbb99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- Feature Dictionary ---\n",
    "\n",
    "FEATURE_DICTIONARY = {\n",
    "    # === DEMOGRAPHICS (Book 0) ===\n",
    "    'AGE_GROUP':            ('Demographics', 'Age in 5-year bands (45-49, 50-54, ...75+)', 'Ordinal (0-5)', 'Point-in-time'),\n",
    "    'HAS_PCP_AT_END':       ('Demographics', 'Has established primary care provider at observation date', 'Binary flag', 'Point-in-time'),\n",
    "    'IS_FEMALE':            ('Demographics', 'Patient gender is female', 'Binary flag', 'Static'),\n",
    "    'IS_MARRIED_PARTNER':   ('Demographics', 'Married or in significant relationship', 'Binary flag', 'Static'),\n",
    "    'RACE_ASIAN':           ('Demographics', 'Patient race is Asian', 'Binary flag', 'Static'),\n",
    "    'RACE_CAUCASIAN':       ('Demographics', 'Patient race is Caucasian/White', 'Binary flag', 'Static'),\n",
    "\n",
    "    # === TEMPORAL (Book 0) === EXCLUDED (prevalence bias) ===\n",
    "\n",
    "    # === ICD-10 DIAGNOSES (Book 2) ===\n",
    "    'icd_ANEMIA_FLAG_12MO':          ('ICD-10', 'Any anemia diagnosis (D50-D64)', 'Binary flag', '12 months'),\n",
    "    'icd_BLEED_CNT_12MO':            ('ICD-10', 'Count of GI bleeding encounters (K62.5, K92.1, K92.2)', 'Count', '12 months'),\n",
    "    'icd_COMBINED_COMORBIDITY_12MO': ('ICD-10', 'Charlson Comorbidity Index from ICD-10 codes', 'Composite score (0-6+)', '12 months'),\n",
    "    'icd_IRON_DEF_ANEMIA_FLAG_12MO': ('ICD-10', 'Iron deficiency anemia specifically (D50)', 'Binary flag', '12 months'),\n",
    "    'icd_MALIGNANCY_FLAG_EVER':      ('ICD-10', 'History of any prior non-CRC malignancy (Z85)', 'Binary flag', 'Ever'),\n",
    "    'icd_SYMPTOM_BURDEN_12MO':       ('ICD-10', 'Sum of 6 GI symptom flags (bleeding, pain, bowel changes, weight loss, anemia, other)', 'Composite score (0-6)', '12 months'),\n",
    "    'icd_chronic_gi_pattern':        ('ICD-10', 'Chronic GI condition: IBD, diverticular disease, or GI complexity >= 2', 'Binary flag', 'Ever/24 months'),\n",
    "\n",
    "    # === LABS (Book 4) ===\n",
    "    'lab_ALBUMIN_DROP_15PCT_FLAG':          ('Labs', 'Albumin dropped >15% vs 6-month prior value', 'Binary flag', '6 months'),\n",
    "    'lab_ALBUMIN_VALUE':                    ('Labs', 'Latest serum albumin (g/dL)', 'Raw value', '2 years, most recent'),\n",
    "    'lab_ALK_PHOS_VALUE':                   ('Labs', 'Latest alkaline phosphatase (U/L)', 'Raw value', '2 years, most recent'),\n",
    "    'lab_ANEMIA_GRADE':                     ('Labs', 'WHO anemia grade from hemoglobin/hematocrit', 'Ordinal (0=normal, 1=mild, 2=moderate, 3=severe)', 'Most recent'),\n",
    "    'lab_ANEMIA_SEVERITY_SCORE':            ('Labs', 'Composite: WHO grade (0-3) + iron deficiency (+2) + microcytosis (+1)', 'Composite score (0-6)', 'Most recent'),\n",
    "    'lab_AST_VALUE':                        ('Labs', 'Latest AST liver enzyme (U/L)', 'Raw value', '2 years, most recent'),\n",
    "    'lab_ESR_VALUE':                        ('Labs', 'Latest erythrocyte sedimentation rate (mm/hr)', 'Raw value', '3 years, most recent'),\n",
    "    'lab_HEMOGLOBIN_ACCELERATING_DECLINE':  ('Labs', 'Hemoglobin declining >0.5 g/dL/month AND rate accelerating', 'Binary flag', '6 months'),\n",
    "    'lab_HEMOGLOBIN_VALUE':                 ('Labs', 'Latest hemoglobin (g/dL)', 'Raw value', '2 years, most recent'),\n",
    "    'lab_IRON_SATURATION_PCT':              ('Labs', 'Serum iron saturation (iron/TIBC * 100)', 'Derived ratio (%)', '3 years, most recent'),\n",
    "    'lab_PLATELETS_ACCELERATING_RISE':      ('Labs', 'Platelets >450 AND rate of rise accelerating', 'Binary flag', '6 months'),\n",
    "    'lab_PLATELETS_VALUE':                  ('Labs', 'Latest platelet count (K/uL)', 'Raw value', '2 years, most recent'),\n",
    "    'lab_THROMBOCYTOSIS_FLAG':              ('Labs', 'Platelets >450 K/uL', 'Binary flag', 'Most recent'),\n",
    "    'lab_comprehensive_iron_deficiency':    ('Labs', 'Iron deficiency by ICD D50 OR lab criteria (low Hgb + low MCV or low ferritin)', 'Binary flag', 'ICD 12mo / labs 2yrs'),\n",
    "\n",
    "    # === OUTPATIENT MEDICATIONS (Book 5.1) ===\n",
    "    'out_med_broad_abx_recency': ('Outpatient Meds', 'Days since last broad-spectrum antibiotic Rx, binned', 'Ordinal/Recency', '12 months'),\n",
    "    'out_med_ibd_meds_recency':  ('Outpatient Meds', 'Days since last IBD medication (5-ASA, immunosuppressants, biologics)', 'Ordinal/Recency', '12 months'),\n",
    "    'out_med_ppi_use_flag':      ('Outpatient Meds', 'Any proton pump inhibitor use', 'Binary flag', '12 months'),\n",
    "\n",
    "    # === INPATIENT MEDICATIONS (Book 5.2) ===\n",
    "    'inp_med_inp_gi_hospitalization': ('Inpatient Meds', 'Any hospitalization with GI-related diagnosis', 'Binary flag', '12 months'),\n",
    "    'inp_med_inp_ibd_meds_recency':  ('Inpatient Meds', 'Days since last inpatient IBD medication', 'Ordinal/Recency', '12 months'),\n",
    "    'inp_med_inp_obstruction_pattern': ('Inpatient Meds', 'Obstruction/ileus pattern: laxatives + opioids in same admission', 'Binary flag', '12 months'),\n",
    "    'inp_med_inp_opioid_use_flag':   ('Inpatient Meds', 'Any opioid administered during hospitalization', 'Binary flag', '12 months'),\n",
    "\n",
    "    # === VISIT HISTORY (Book 6) === EXCLUDED (Lucem Novis variant) ===\n",
    "\n",
    "    # === PROCEDURES (Book 7) ===\n",
    "    'proc_blood_transfusion_count_12mo': ('Procedures', 'RBC transfusion count (objective bleeding indicator)', 'Count', '12 months'),\n",
    "    'proc_high_imaging_intensity_flag':  ('Procedures', '>=2 imaging studies (CT/MRI/US)', 'Binary flag', '12 months'),\n",
    "    'proc_mri_abd_pelvis_count_12mo':    ('Procedures', 'Abdominal/pelvic MRI count', 'Count', '12 months'),\n",
    "\n",
    "    # === VITALS (Book 1) ===\n",
    "    'vit_BMI':                     ('Vitals', 'Body Mass Index (kg/m2)', 'Raw value', 'Most recent, 12mo'),\n",
    "    'vit_BMI_CHANGE_6M':           ('Vitals', 'BMI change over 6 months', 'Rate of change', '6 months'),\n",
    "    'vit_CACHEXIA_RISK_SCORE':     ('Vitals', 'Wasting risk from weight loss + low BMI combination', 'Ordinal (0=none, 1=moderate, 2=high)', '6-12 months'),\n",
    "    'vit_MAX_WEIGHT_LOSS_PCT_60D': ('Vitals', 'Maximum % weight loss in any 60-day window', 'Rate of change (%)', '6-12 months'),\n",
    "    'vit_PULSE':                   ('Vitals', 'Latest heart rate (bpm)', 'Raw value', 'Most recent, 12mo'),\n",
    "    'vit_PULSE_PRESSURE':          ('Vitals', 'Systolic BP minus Diastolic BP (mmHg)', 'Derived value', 'Most recent, 12mo'),\n",
    "    'vit_RECENCY_WEIGHT':          ('Vitals', 'Days since last weight measurement', 'Recency (days)', '12 months'),\n",
    "    'vit_SBP_VARIABILITY_6M':      ('Vitals', 'Systolic BP standard deviation (instability measure)', 'Derived value (mmHg)', '6 months'),\n",
    "    'vit_WEIGHT_TRAJECTORY_SLOPE': ('Vitals', 'Linear regression slope of weight over time (lbs/month)', 'Rate of change', '6-12 months')\n",
    "}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"FEATURE DICTIONARY ({len(final_features)} Final Features)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Group by domain\n",
    "from collections import defaultdict\n",
    "by_domain = defaultdict(list)\n",
    "for feat in sorted(final_features):\n",
    "    if feat in FEATURE_DICTIONARY:\n",
    "        domain, desc, ftype, lookback = FEATURE_DICTIONARY[feat]\n",
    "        by_domain[domain].append((feat, desc, ftype, lookback))\n",
    "    else:\n",
    "        by_domain['Unknown'].append((feat, '(not in dictionary)', '', ''))\n",
    "\n",
    "domain_order = ['Demographics', 'Temporal', 'ICD-10', 'Labs', 'Outpatient Meds',\n",
    "                'Inpatient Meds', 'Visits', 'Procedures', 'Vitals', 'Unknown']\n",
    "\n",
    "for domain in domain_order:\n",
    "    if domain not in by_domain:\n",
    "        continue\n",
    "    features = by_domain[domain]\n",
    "    print(f\"\\n{'─'*70}\")\n",
    "    print(f\"  {domain.upper()} ({len(features)} features)\")\n",
    "    print(f\"{'─'*70}\")\n",
    "    for feat, desc, ftype, lookback in features:\n",
    "        print(f\"\\n  {feat}\")\n",
    "        print(f\"    {desc}\")\n",
    "        print(f\"    Type: {ftype}  |  Window: {lookback}\")\n",
    "\n",
    "# Summary by type\n",
    "type_counts = defaultdict(int)\n",
    "for feat in final_features:\n",
    "    if feat in FEATURE_DICTIONARY:\n",
    "        ftype = FEATURE_DICTIONARY[feat][2]\n",
    "        if 'Binary' in ftype:\n",
    "            type_counts['Binary flags'] += 1\n",
    "        elif 'Raw' in ftype:\n",
    "            type_counts['Raw values'] += 1\n",
    "        elif 'Count' in ftype:\n",
    "            type_counts['Counts'] += 1\n",
    "        elif 'Composite' in ftype or 'Ordinal' in ftype:\n",
    "            type_counts['Composite/Ordinal'] += 1\n",
    "        elif 'Derived' in ftype or 'Ratio' in ftype or 'ratio' in ftype:\n",
    "            type_counts['Derived ratios'] += 1\n",
    "        elif 'Rate' in ftype or 'change' in ftype:\n",
    "            type_counts['Rates of change'] += 1\n",
    "        elif 'Recency' in ftype:\n",
    "            type_counts['Recency measures'] += 1\n",
    "        else:\n",
    "            type_counts['Other'] += 1\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"FEATURE TYPE SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "for ftype, count in sorted(type_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {ftype:<25} {count:>3}\")\n",
    "print(f\"  {'TOTAL':<25} {sum(type_counts.values()):>3}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82e47f47-cd17-44b3-8051-de14dd1c7071",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfd7992f-0275-4238-8716-74adc3b03d92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Save Final Outputs\n",
    " \n",
    "Outputs saved to `feature_selection_outputs/`:\n",
    "- `lucem_novis_standard_final_features.txt` - Feature names, one per line\n",
    "- `lucem_novis_standard_final_features.py` - Python list for easy import\n",
    "- `lucem_novis_standard_final_model.pkl` - Trained XGBoost model\n",
    "- `lucem_novis_standard_feature_selection_summary.json` - Summary metrics and feature list\n",
    "- `lucem_novis_standard_iteration_tracking.csv` - Metrics at each iteration\n",
    "- `lucem_novis_standard_cv_stability_report.json` - Cross-validation stability analysis (added after CV section runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "271653d8-2a3a-4ec8-8636-d2f470980412",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save final feature list\n",
    "final_features_path = os.path.join(OUTPUT_DIR, \"lucem_novis_standard_final_features.txt\")\n",
    "with open(final_features_path, 'w') as f:\n",
    "    f.write(f\"# Final Feature List\\n\")\n",
    "    f.write(f\"# Generated: {datetime.now()}\\n\")\n",
    "    f.write(f\"# Initial features: {len(feature_cols)}\\n\")\n",
    "    f.write(f\"# Final features: {len(final_features)}\\n\")\n",
    "    f.write(f\"# Stop reason: {stop_reason}\\n\\n\")\n",
    "    for feat in sorted(final_features):\n",
    "        f.write(f\"{feat}\\n\")\n",
    "\n",
    "print(f\"✓ Saved: {final_features_path}\")\n",
    "\n",
    "# Save final feature list as Python list\n",
    "final_features_py_path = os.path.join(OUTPUT_DIR, \"lucem_novis_standard_final_features.py\")\n",
    "with open(final_features_py_path, 'w') as f:\n",
    "    f.write(f\"# Final Feature List for CRC Prediction Model\\n\")\n",
    "    f.write(f\"# Generated: {datetime.now()}\\n\\n\")\n",
    "    f.write(f\"FINAL_FEATURES = [\\n\")\n",
    "    for feat in sorted(final_features):\n",
    "        f.write(f\"    '{feat}',\\n\")\n",
    "    f.write(f\"]\\n\")\n",
    "\n",
    "print(f\"✓ Saved: {final_features_py_path}\")\n",
    "\n",
    "# Save model\n",
    "final_model_path = os.path.join(OUTPUT_DIR, \"lucem_novis_standard_final_model.pkl\")\n",
    "with open(final_model_path, 'wb') as f:\n",
    "    pickle.dump(final_model, f)\n",
    "\n",
    "print(f\"✓ Saved: {final_model_path}\")\n",
    "\n",
    "# Save summary JSON\n",
    "summary = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'initial_features': len(feature_cols),\n",
    "    'phase1_features': len(phase1_features),\n",
    "    'final_features': len(final_features),\n",
    "    'stop_reason': stop_reason,\n",
    "    'baseline_val_auprc': baseline_metrics['val']['auprc'],\n",
    "    'final_val_auprc': final_metrics['val']['auprc'],\n",
    "    'final_test_auprc': final_metrics['test']['auprc'],  # Test evaluated only at final model\n",
    "    'feature_list': final_features\n",
    "}\n",
    "\n",
    "summary_path = os.path.join(OUTPUT_DIR, \"lucem_novis_standard_feature_selection_summary.json\")\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"✓ Saved: {summary_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ff3fce2-9589-4fe5-ab4a-9b7cb56f28db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Visualize Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a7b87c1-7dac-44f8-925c-9712445e352c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load tracking data\n",
    "tracking_path = os.path.join(OUTPUT_DIR, \"lucem_novis_standard_iteration_tracking.csv\")\n",
    "if os.path.exists(tracking_path):\n",
    "    tracking_df = pd.read_csv(tracking_path)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "    # Feature count over iterations\n",
    "    axes[0, 0].plot(range(len(tracking_df)), tracking_df['n_features'], 'bo-', linewidth=2, markersize=8)\n",
    "    axes[0, 0].set_xlabel('Step')\n",
    "    axes[0, 0].set_ylabel('Number of Features')\n",
    "    axes[0, 0].set_title('Feature Count Over Iterations')\n",
    "    axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "    # Validation AUPRC (test held out until final evaluation)\n",
    "    axes[0, 1].plot(range(len(tracking_df)), tracking_df['val_auprc'], 'go-', linewidth=2, markersize=8, label='Val')\n",
    "    axes[0, 1].plot(range(len(tracking_df)), tracking_df['train_auprc'], 'bo-', linewidth=2, markersize=8, label='Train', alpha=0.5)\n",
    "    axes[0, 1].axhline(y=baseline_metrics['val']['auprc'] * (1 - MAX_VAL_AUPRC_DROP), color='g', linestyle='--', alpha=0.5, label='5% drop threshold')\n",
    "    axes[0, 1].set_xlabel('Step')\n",
    "    axes[0, 1].set_ylabel('AUPRC')\n",
    "    axes[0, 1].set_title('AUPRC Over Iterations (Test Held Out)')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "    # Train-Val Gap\n",
    "    axes[1, 0].plot(range(len(tracking_df)), tracking_df['train_val_gap'], 'mo-', linewidth=2, markersize=8)\n",
    "    axes[1, 0].axhline(y=baseline_metrics['train_val_gap'] + MAX_GAP_INCREASE, color='r', linestyle='--', alpha=0.5, label='Gap threshold')\n",
    "    axes[1, 0].set_xlabel('Step')\n",
    "    axes[1, 0].set_ylabel('Train-Val Gap')\n",
    "    axes[1, 0].set_title('Overfitting Indicator (Train-Val Gap)')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "    # Val drop from baseline\n",
    "    axes[1, 1].plot(range(len(tracking_df)), tracking_df['val_drop_from_baseline'] * 100, 'co-', linewidth=2, markersize=8)\n",
    "    axes[1, 1].axhline(y=MAX_VAL_AUPRC_DROP * 100, color='r', linestyle='--', alpha=0.5, label='5% threshold')\n",
    "    axes[1, 1].set_xlabel('Step')\n",
    "    axes[1, 1].set_ylabel('Val AUPRC Drop (%)')\n",
    "    axes[1, 1].set_title('Validation Performance Drop from Baseline')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'lucem_novis_standard_iteration_progress.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"✓ Saved: {OUTPUT_DIR}/lucem_novis_standard_iteration_progress.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4f9040a-6e4b-4b94-9552-2bea8fca9f42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69cdd1a8-fb1f-490b-add4-b418a2780e57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "total_elapsed = time.time() - PIPELINE_START_TIME\n",
    "hours, remainder = divmod(total_elapsed, 3600)\n",
    "minutes, seconds = divmod(remainder, 60)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FEATURE SELECTION COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\"\"\n",
    "Summary:\n",
    "  - Initial features: {len(feature_cols)}\n",
    "  - After Phase 1 (clustering): {len(phase1_features)}\n",
    "  - Final features: {len(final_features)}\n",
    "  - Reduction: {(len(feature_cols) - len(final_features))/len(feature_cols)*100:.1f}%\n",
    "\n",
    "Performance:\n",
    "  - Baseline Val AUPRC: {baseline_metrics['val']['auprc']:.4f}\n",
    "  - Final Val AUPRC: {final_metrics['val']['auprc']:.4f}\n",
    "  - Final Test AUPRC: {final_metrics['test']['auprc']:.4f}\n",
    "\n",
    "Stop Reason: {stop_reason}\n",
    "\n",
    "Total Pipeline Time: {int(hours)}h {int(minutes)}m {int(seconds)}s\n",
    "\n",
    "Outputs saved to: {OUTPUT_DIR}/\n",
    "  - lucem_novis_standard_final_features.txt\n",
    "  - lucem_novis_standard_final_features.py\n",
    "  - lucem_novis_standard_final_model.pkl\n",
    "  - lucem_novis_standard_feature_selection_summary.json\n",
    "  - lucem_novis_standard_iteration_tracking.csv\n",
    "  - lucem_novis_standard_iteration_progress.png\n",
    "  - lucem_novis_standard_threshold_analysis.png\n",
    "\n",
    "Checkpoints saved to: {CHECKPOINT_DIR}/\n",
    "  (Can be used to resume if notebook is interrupted)\n",
    "\"\"\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b700383-02c1-43be-a5f5-e680e7ff43e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "# Cross-Validation Stability Analysis\n",
    "---\n",
    " \n",
    "This section runs a simplified feature selection on the remaining CV folds to assess\n",
    "stability. Features that are consistently selected across folds are more robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9aa3e744-0a27-436a-8945-230dd7545bd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print_stage(\"CV STABILITY ANALYSIS (PHASE 3)\")\n",
    "print_progress(f\"Running feature selection on {N_CV_FOLDS - 1} additional folds...\")\n",
    "print_progress(f\"Stability threshold: features must appear in ≥{CV_FEATURE_THRESHOLD*100:.0f}% of folds\")\n",
    "print_progress(\"This validates that feature selection is robust across different train/val splits\")\n",
    "\n",
    "# Track features selected in each fold\n",
    "# Fold 0 is the main pipeline result\n",
    "fold_selected_features = {0: set(final_features)}\n",
    "\n",
    "# Test set indices (fixed across folds)\n",
    "test_mask = df_pandas['SPLIT'] == 'test'\n",
    "X_test = df_pandas.loc[test_mask].copy()\n",
    "y_test = df_pandas.loc[test_mask, 'FUTURE_CRC_EVENT'].copy()\n",
    "\n",
    "# Run on remaining folds (1 and 2)\n",
    "for fold_idx in range(1, N_CV_FOLDS):\n",
    "    fold_start_time = time.time()\n",
    "    print_progress(f\"\")\n",
    "    print_progress(f\"{'='*50}\")\n",
    "    print_progress(f\"CV FOLD {fold_idx + 1} / {N_CV_FOLDS}\")\n",
    "    print_progress(f\"{'='*50}\")\n",
    "\n",
    "    # Get train/val split for this fold (convert lists to sets for efficient lookup)\n",
    "    # Use string key because JSON converts int keys to strings\n",
    "    fold_train_patients = set(cv_fold_assignments[str(fold_idx)]['train_patients'])\n",
    "    fold_val_patients = set(cv_fold_assignments[str(fold_idx)]['val_patients'])\n",
    "\n",
    "    # Create masks\n",
    "    trainval_mask = df_pandas['SPLIT'].isin(['train', 'val'])\n",
    "    fold_train_mask = trainval_mask & df_pandas['PAT_ID'].isin(fold_train_patients)\n",
    "    fold_val_mask = trainval_mask & df_pandas['PAT_ID'].isin(fold_val_patients)\n",
    "\n",
    "    X_fold_train = df_pandas.loc[fold_train_mask].copy()\n",
    "    y_fold_train = df_pandas.loc[fold_train_mask, 'FUTURE_CRC_EVENT'].copy()\n",
    "    X_fold_val = df_pandas.loc[fold_val_mask].copy()\n",
    "    y_fold_val = df_pandas.loc[fold_val_mask, 'FUTURE_CRC_EVENT'].copy()\n",
    "\n",
    "    print_progress(f\"Train: {len(y_fold_train):,} obs, {int(y_fold_train.sum()):,} events\")\n",
    "    print_progress(f\"Val: {len(y_fold_val):,} obs, {int(y_fold_val.sum()):,} events\")\n",
    "\n",
    "    # Calculate scale_pos_weight for this fold\n",
    "    n_pos_fold = (y_fold_train == 1).sum()\n",
    "    if n_pos_fold == 0:\n",
    "        raise ValueError(f\"No positive cases in fold {fold_idx} training data. Check SGKF stratification.\")\n",
    "    fold_scale_pos_weight = (y_fold_train == 0).sum() / n_pos_fold\n",
    "\n",
    "    # Train baseline model on this fold (using clustering features from main pipeline)\n",
    "    print_progress(f\"Training model on {len(phase1_features)} cluster representatives...\")\n",
    "    fold_model = train_xgboost_model(\n",
    "        X_fold_train, y_fold_train, X_fold_val, y_fold_val,\n",
    "        phase1_features, fold_scale_pos_weight\n",
    "    )\n",
    "\n",
    "    # Compute SHAP on this fold's validation set\n",
    "    print_progress(\"Computing SHAP values for this fold...\")\n",
    "    fold_importance_df = compute_shap_values(fold_model, X_fold_val, y_fold_val, phase1_features)\n",
    "\n",
    "    # Identify removal candidates (same logic as main pipeline)\n",
    "    zero_threshold = 0.0002\n",
    "    ratio_threshold = 0.2\n",
    "    bottom_pct = 0.15\n",
    "\n",
    "    zero_importance = set(fold_importance_df[fold_importance_df['SHAP_Combined'] < zero_threshold]['Feature'])\n",
    "    neg_biased = set(fold_importance_df[fold_importance_df['SHAP_Ratio'] < ratio_threshold]['Feature'])\n",
    "    importance_cutoff = fold_importance_df['SHAP_Combined'].quantile(bottom_pct)\n",
    "    bottom_features = set(fold_importance_df[fold_importance_df['SHAP_Combined'] < importance_cutoff]['Feature'])\n",
    "\n",
    "    # Features meeting 2+ criteria\n",
    "    candidates = (\n",
    "        (zero_importance & neg_biased) |\n",
    "        (zero_importance & bottom_features) |\n",
    "        (neg_biased & bottom_features)\n",
    "    )\n",
    "\n",
    "    # Never remove protected features\n",
    "    median_ratio = fold_importance_df['SHAP_Ratio'].median()\n",
    "    protected = set(fold_importance_df[fold_importance_df['SHAP_Ratio'] >= median_ratio]['Feature'])\n",
    "    candidates = candidates - protected\n",
    "\n",
    "    # Remove candidates from phase1_features to get this fold's selection\n",
    "    fold_features = [f for f in phase1_features if f not in candidates]\n",
    "\n",
    "    print_progress(f\"Features selected: {len(fold_features)}\")\n",
    "    fold_selected_features[fold_idx] = set(fold_features)\n",
    "\n",
    "    # Quick validation\n",
    "    fold_val_metrics = evaluate_model(fold_model, X_fold_val, y_fold_val, phase1_features, f\"Fold {fold_idx + 1} Val\")\n",
    "\n",
    "    fold_elapsed = time.time() - fold_start_time\n",
    "    print_progress(f\"Fold {fold_idx + 1} complete in {fold_elapsed:.1f}s\")\n",
    "\n",
    "# Compute stability statistics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STABILITY ANALYSIS RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Count how many folds each feature appears in\n",
    "feature_fold_counts = {}\n",
    "all_features = set()\n",
    "for fold_idx, features in fold_selected_features.items():\n",
    "    all_features.update(features)\n",
    "\n",
    "for feat in all_features:\n",
    "    count = sum(1 for fold_features in fold_selected_features.values() if feat in fold_features)\n",
    "    feature_fold_counts[feat] = count\n",
    "\n",
    "# Categorize by stability\n",
    "stable_features = [f for f, count in feature_fold_counts.items() if count >= N_CV_FOLDS * CV_FEATURE_THRESHOLD]\n",
    "unstable_features = [f for f, count in feature_fold_counts.items() if count < N_CV_FOLDS * CV_FEATURE_THRESHOLD]\n",
    "\n",
    "print(f\"\\nFeature stability summary:\")\n",
    "print(f\"  Total unique features selected across folds: {len(all_features)}\")\n",
    "print(f\"  Stable features (≥{int(N_CV_FOLDS * CV_FEATURE_THRESHOLD)}/{N_CV_FOLDS} folds): {len(stable_features)}\")\n",
    "print(f\"  Unstable features (<{int(N_CV_FOLDS * CV_FEATURE_THRESHOLD)}/{N_CV_FOLDS} folds): {len(unstable_features)}\")\n",
    "\n",
    "# Show unstable features\n",
    "if unstable_features:\n",
    "    print(f\"\\n  Unstable features (may be overfitting artifacts):\")\n",
    "    for feat in sorted(unstable_features)[:20]:\n",
    "        count = feature_fold_counts[feat]\n",
    "        in_final = \"✓\" if feat in final_features else \" \"\n",
    "        print(f\"    [{in_final}] {feat}: {count}/{N_CV_FOLDS} folds\")\n",
    "    if len(unstable_features) > 20:\n",
    "        print(f\"    ... and {len(unstable_features) - 20} more\")\n",
    "\n",
    "# Check overlap with final features from main pipeline\n",
    "final_stable = [f for f in final_features if f in stable_features]\n",
    "final_unstable = [f for f in final_features if f in unstable_features]\n",
    "\n",
    "print(f\"\\n  Final features from main pipeline: {len(final_features)}\")\n",
    "print(f\"    - Stable: {len(final_stable)} ({len(final_stable)/len(final_features)*100:.1f}%)\")\n",
    "print(f\"    - Unstable: {len(final_unstable)} ({len(final_unstable)/len(final_features)*100:.1f}%)\")\n",
    "\n",
    "# Save stability report\n",
    "stability_report = {\n",
    "    'n_folds': N_CV_FOLDS,\n",
    "    'threshold': CV_FEATURE_THRESHOLD,\n",
    "    'stable_features': sorted(stable_features),\n",
    "    'unstable_features': sorted(unstable_features),\n",
    "    'feature_fold_counts': feature_fold_counts,\n",
    "    'final_stable': sorted(final_stable),\n",
    "    'final_unstable': sorted(final_unstable)\n",
    "}\n",
    "\n",
    "stability_path = os.path.join(OUTPUT_DIR, \"lucem_novis_standard_cv_stability_report.json\")\n",
    "with open(stability_path, 'w') as f:\n",
    "    json.dump(stability_report, f, indent=2)\n",
    "print(f\"\\n✓ Saved stability report: {stability_path}\")\n",
    "\n",
    "# Optionally update final features to only include stable ones\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"RECOMMENDATION\")\n",
    "print(\"=\"*70)\n",
    "if len(final_unstable) > 0:\n",
    "    print(f\"Consider reviewing the {len(final_unstable)} unstable features in the final set.\")\n",
    "    print(\"These may be overfitting to the specific train/val split.\")\n",
    "    print(f\"For a more robust model, use only the {len(final_stable)} stable features.\")\n",
    "else:\n",
    "    print(\"All final features are stable across CV folds. Feature selection is robust.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Phase 4: Automated Feature Selection (Parsimony-Aware)\n",
    "---\n",
    "\n",
    "### What This Section Does\n",
    "\n",
    "Automatically selects the optimal iteration and feature set using a **parsimony-aware** approach:\n",
    "\n",
    "1. **Load checkpoint artifacts** from Phases 1-3\n",
    "2. **Filter iterations** by overfitting guard (train-val gap < 0.02)\n",
    "3. **Apply 10% parsimony tolerance** — among iterations within 10% of the best validation AUPRC, pick the one with the **fewest features**\n",
    "4. **CV stability filter** — remove features appearing in < 3/5 cross-validation folds\n",
    "5. **Clinical add-backs** — restore clinically important features if they're CV-stable\n",
    "\n",
    "### Why Parsimony Tolerance?\n",
    "\n",
    "With 250:1 class imbalance, validation AUPRC oscillates due to noise — each positive\n",
    "case has outsized impact on the metric. Single-iteration differences of 0.01 are noise,\n",
    "not signal. Rather than chasing the highest val AUPRC (which rewards feature bloat), we\n",
    "pick the **simplest model** whose performance is statistically indistinguishable from the best.\n",
    "\n",
    "**Literature support:** Published CRC risk models typically use 8-30 features (Li 2025,\n",
    "Hornbrook 2020, JMIR 2025). EPV (events per variable) ≥ 20 is recommended for ML methods\n",
    "with variable selection (Austin & Steyerberg 2017). Our 3,092 events at ~27 features give\n",
    "EPV ≈ 115, well above all recommended thresholds."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PHASE 4 CONFIGURATION\n",
    "# ============================================================================\n",
    "# These parameters control the automated feature selection logic.\n",
    "# They are independent of the Phase 1-3 winnowing parameters above.\n",
    "# ============================================================================\n",
    "\n",
    "# --- Overfitting Guard ---\n",
    "# Iterations where |train_val_gap| exceeds this threshold are excluded.\n",
    "# A large gap indicates the model has memorized training data patterns\n",
    "# that don't generalize to the validation set.\n",
    "PHASE4_GAP_THRESHOLD = 0.08\n",
    "\n",
    "# --- Parsimony Tolerance ---\n",
    "# Instead of picking the iteration with the absolute best val AUPRC,\n",
    "# we find all iterations within PARSIMONY_TOLERANCE of the best,\n",
    "# then pick the one with the FEWEST features.\n",
    "#\n",
    "# Rationale: With 250:1 class imbalance, val AUPRC standard deviation\n",
    "# across iterations is ~0.007. A 10% tolerance (e.g., best 0.133 →\n",
    "# threshold 0.120) is roughly a 2-SD band — iterations within this\n",
    "# band are statistically indistinguishable.\n",
    "PARSIMONY_TOLERANCE_PCT = 10  # percent\n",
    "\n",
    "# --- CV Stability ---\n",
    "# Features must appear in at least this many of 5 CV folds to be \"stable.\"\n",
    "# Unstable features (1-2 folds) are sensitive to the specific data split\n",
    "# and risk fitting to artifacts rather than generalizable signal.\n",
    "PHASE4_CV_STABILITY_MIN_FOLDS = 3  # out of 5\n",
    "\n",
    "# --- Clinical Must-Keep Features ---\n",
    "# Features with strong clinical rationale for CRC risk prediction that\n",
    "# may have been removed by the statistical process (SHAP winnowing).\n",
    "# Only added back if they pass the CV stability filter (>= 3/5 folds).\n",
    "#\n",
    "# lab_HEMOGLOBIN_ACCELERATING_DECLINE:\n",
    "#   The second derivative of hemoglobin — measures whether hemoglobin\n",
    "#   decline is ACCELERATING. This is a hallmark of occult GI bleeding,\n",
    "#   one of the strongest early CRC signals. Showed 10.9x CRC risk\n",
    "#   elevation in Book 4 feature engineering EDA. It was removed during\n",
    "#   SHAP winnowing (low average importance because it's rare) but is\n",
    "#   clinically indispensable.\n",
    "PHASE4_CLINICAL_MUST_KEEP = [\n",
    "    'lab_HEMOGLOBIN_ACCELERATING_DECLINE',\n",
    "]\n",
    "\n",
    "# --- Output ---\n",
    "# Final feature list is saved to this Spark table for downstream use\n",
    "# by the training pipeline and calibration steps.\n",
    "PHASE4_OUTPUT_TABLE = f\"{trgt_cat}.clncl_ds.herald_lucem_novis_std_final_features\"\n",
    "\n",
    "# Print configuration for audit trail\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 4 CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"  Overfitting guard:     |gap| < {PHASE4_GAP_THRESHOLD}\")\n",
    "print(f\"  Parsimony tolerance:   {PARSIMONY_TOLERANCE_PCT}% below best val AUPRC\")\n",
    "print(f\"  CV stability minimum:  {PHASE4_CV_STABILITY_MIN_FOLDS}/5 folds\")\n",
    "print(f\"  Clinical must-keep:    {PHASE4_CLINICAL_MUST_KEEP}\")\n",
    "print(f\"  Output table:          {PHASE4_OUTPUT_TABLE}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PHASE 4: AUTOMATED FEATURE SELECTION\n",
    "# ============================================================================\n",
    "# This cell loads the artifacts from Phases 1-3 and applies the parsimony-\n",
    "# aware selection logic to choose the final feature set.\n",
    "#\n",
    "# Loading from files (not in-memory variables) makes Phase 4 independently\n",
    "# runnable even if the kernel was restarted between phases.\n",
    "# ============================================================================\n",
    "\n",
    "print_stage(\"PHASE 4: AUTOMATED FEATURE SELECTION\")\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Step 4.1: Load checkpoint artifacts\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# lucem_novis_standard_iteration_tracking.csv   — val AUPRC, train-val gap per iteration\n",
    "# lucem_novis_standard_features_by_iteration.json — feature lists at each winnowing step\n",
    "# lucem_novis_standard_cv_stability_report.json  — how many CV folds each feature survived\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "print_progress(\"Step 4.1: Loading checkpoint artifacts...\")\n",
    "\n",
    "tracking_path = os.path.join(OUTPUT_DIR, \"lucem_novis_standard_iteration_tracking.csv\")\n",
    "iter_df = pd.read_csv(tracking_path)\n",
    "print_progress(f\"  lucem_novis_standard_iteration_tracking.csv: {len(iter_df)} rows\")\n",
    "\n",
    "features_path = os.path.join(OUTPUT_DIR, \"lucem_novis_standard_features_by_iteration.json\")\n",
    "with open(features_path, 'r') as f:\n",
    "    features_by_iteration = json.load(f)\n",
    "print_progress(f\"  lucem_novis_standard_features_by_iteration.json: {len(features_by_iteration)} entries\")\n",
    "\n",
    "stability_path = os.path.join(OUTPUT_DIR, \"lucem_novis_standard_cv_stability_report.json\")\n",
    "with open(stability_path, 'r') as f:\n",
    "    cv_stability = json.load(f)\n",
    "feature_fold_counts = cv_stability['feature_fold_counts']\n",
    "print_progress(f\"  lucem_novis_standard_cv_stability_report.json: {len(feature_fold_counts)} features tracked\")\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Step 4.2: Identify the canonical pipeline run\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# The lucem_novis_standard_iteration_tracking.csv may contain multiple runs (from debugging or\n",
    "# parameter changes). The canonical run is the LAST one — identified by\n",
    "# finding the last \"phase1,baseline\" row and taking everything after it.\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "print_progress(\"\")\n",
    "print_progress(\"Step 4.2: Identifying canonical pipeline run...\")\n",
    "\n",
    "# Find the last phase1 baseline row (marks start of a fresh run)\n",
    "phase1_mask = (iter_df['phase'] == 'phase1') & (iter_df['iteration'] == 'baseline')\n",
    "phase1_baseline_rows = iter_df[phase1_mask]\n",
    "\n",
    "if len(phase1_baseline_rows) == 0:\n",
    "    # No phase1 baseline — treat all phase2 rows as the canonical run\n",
    "    print_progress(\"  No phase1 baseline found; using all phase2 rows\")\n",
    "    canonical_start_idx = 0\n",
    "else:\n",
    "    # Use the LAST baseline row as the start of the canonical run\n",
    "    canonical_start_idx = phase1_baseline_rows.index[-1]\n",
    "    print_progress(f\"  Found {len(phase1_baseline_rows)} pipeline run(s)\")\n",
    "    print_progress(f\"  Using canonical run starting at row index {canonical_start_idx}\")\n",
    "\n",
    "# Filter to phase2 iterations from the canonical run only\n",
    "canonical_df = iter_df.loc[canonical_start_idx:].copy()\n",
    "phase2_df = canonical_df[canonical_df['phase'] == 'phase2'].copy()\n",
    "phase2_df['iteration'] = phase2_df['iteration'].astype(int)\n",
    "phase2_df = phase2_df.sort_values('iteration').reset_index(drop=True)\n",
    "\n",
    "print_progress(f\"  Canonical run: {len(phase2_df)} Phase 2 iterations\")\n",
    "print_progress(f\"  Feature range: {phase2_df['n_features'].max()} → {phase2_df['n_features'].min()}\")\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Step 4.3: Apply overfitting guard (gap filter)\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Remove iterations where |train - val AUPRC| is too large.\n",
    "# A positive gap means training >> validation (memorization).\n",
    "# A large negative gap means validation >> training (unusual but possible\n",
    "# with noisy validation at extreme imbalance).\n",
    "# We filter on ABSOLUTE gap to catch both directions.\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "print_progress(\"\")\n",
    "print_progress(f\"Step 4.3: Overfitting guard — |train_val_gap| < {PHASE4_GAP_THRESHOLD}...\")\n",
    "\n",
    "phase2_df['abs_gap'] = phase2_df['train_val_gap'].abs()\n",
    "qualified_df = phase2_df[phase2_df['abs_gap'] < PHASE4_GAP_THRESHOLD].copy()\n",
    "n_excluded = len(phase2_df) - len(qualified_df)\n",
    "\n",
    "print_progress(f\"  {len(qualified_df)} of {len(phase2_df)} iterations pass \"\n",
    "               f\"({n_excluded} excluded)\")\n",
    "\n",
    "# Show excluded iterations for transparency\n",
    "if n_excluded > 0:\n",
    "    excluded = phase2_df[phase2_df['abs_gap'] >= PHASE4_GAP_THRESHOLD]\n",
    "    for _, row in excluded.iterrows():\n",
    "        print_progress(f\"    ✗ iter {int(row['iteration']):2d}: \"\n",
    "                       f\"gap = {row['train_val_gap']:+.4f}, \"\n",
    "                       f\"features = {int(row['n_features'])}\")\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Step 4.4: Apply parsimony tolerance\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Find the best val AUPRC among qualifying iterations. Then find ALL\n",
    "# iterations within PARSIMONY_TOLERANCE_PCT of that best. Among those,\n",
    "# pick the one with the FEWEST features.\n",
    "#\n",
    "# This is inspired by the \"1-SE rule\" in cross-validation: when multiple\n",
    "# models are statistically equivalent, prefer the simplest one.\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "print_progress(\"\")\n",
    "print_progress(f\"Step 4.4: Parsimony tolerance — \"\n",
    "               f\"within {PARSIMONY_TOLERANCE_PCT}% of best val AUPRC...\")\n",
    "\n",
    "# Find the best validation performance\n",
    "best_val_auprc = qualified_df['val_auprc'].max()\n",
    "best_iter_row = qualified_df.loc[qualified_df['val_auprc'].idxmax()]\n",
    "\n",
    "# Calculate the tolerance threshold\n",
    "tolerance_threshold = best_val_auprc * (1 - PARSIMONY_TOLERANCE_PCT / 100)\n",
    "\n",
    "print_progress(f\"  Best val AUPRC:      {best_val_auprc:.4f} \"\n",
    "               f\"(iter {int(best_iter_row['iteration'])}, \"\n",
    "               f\"{int(best_iter_row['n_features'])} features)\")\n",
    "print_progress(f\"  Tolerance threshold: {tolerance_threshold:.4f} \"\n",
    "               f\"(= {best_val_auprc:.4f} × {1 - PARSIMONY_TOLERANCE_PCT/100:.2f})\")\n",
    "\n",
    "# Find all iterations within the tolerance band\n",
    "within_tolerance = qualified_df[\n",
    "    qualified_df['val_auprc'] >= tolerance_threshold\n",
    "].copy()\n",
    "\n",
    "print_progress(f\"  {len(within_tolerance)} iterations within tolerance band:\")\n",
    "print_progress(\"\")\n",
    "\n",
    "# Display all qualifying iterations sorted by feature count (ascending)\n",
    "# so the reader can see why we pick the one with fewest features\n",
    "within_sorted = within_tolerance.sort_values('n_features')\n",
    "for _, row in within_sorted.iterrows():\n",
    "    is_selected = row['n_features'] == within_sorted['n_features'].min()\n",
    "    marker = \" ← SELECTED\" if is_selected else \"\"\n",
    "    print_progress(f\"    iter {int(row['iteration']):2d}: \"\n",
    "                   f\"{int(row['n_features']):3d} features, \"\n",
    "                   f\"val = {row['val_auprc']:.4f}, \"\n",
    "                   f\"gap = {row['train_val_gap']:+.5f}\"\n",
    "                   f\"{marker}\")\n",
    "\n",
    "# Select the most parsimonious iteration\n",
    "selected_row = within_tolerance.loc[within_tolerance['n_features'].idxmin()]\n",
    "selected_iteration = int(selected_row['iteration'])\n",
    "selected_val_auprc = selected_row['val_auprc']\n",
    "selected_gap = selected_row['train_val_gap']\n",
    "\n",
    "print_progress(\"\")\n",
    "print_progress(f\"  ✓ Selected iteration {selected_iteration}: \"\n",
    "               f\"{int(selected_row['n_features'])} features, \"\n",
    "               f\"val AUPRC = {selected_val_auprc:.4f}, \"\n",
    "               f\"gap = {selected_gap:+.5f}\")\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Step 4.5: Retrieve the feature list for the selected iteration\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# The lucem_novis_standard_features_by_iteration.json stores features as:\n",
    "#   phase2_iter{N}_input  → features used to TRAIN the model at iteration N\n",
    "#   phase2_iter{N}_output → features AFTER removal at iteration N\n",
    "#\n",
    "# We want _input because n_features and val_auprc in the tracking CSV\n",
    "# correspond to the model trained on the INPUT features.\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "iter_key = f\"phase2_iter{selected_iteration}_input\"\n",
    "selected_features = features_by_iteration[iter_key]['features']\n",
    "\n",
    "print_progress(\"\")\n",
    "print_progress(f\"Step 4.5: Retrieved {len(selected_features)} features \"\n",
    "               f\"from '{iter_key}'\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 4.6: Apply CV stability filter\n",
    "# ============================================================================\n",
    "# Remove features that appeared in fewer than PHASE4_CV_STABILITY_MIN_FOLDS\n",
    "# out of 5 cross-validation folds. Such features are sensitive to the\n",
    "# specific train/val partition and risk fitting to data artifacts.\n",
    "# ============================================================================\n",
    "\n",
    "print_progress(\"\")\n",
    "print_progress(f\"Step 4.6: CV stability filter — \"\n",
    "               f\"require ≥{PHASE4_CV_STABILITY_MIN_FOLDS}/5 folds...\")\n",
    "\n",
    "# Classify each selected feature as stable or unstable\n",
    "stable_features = []\n",
    "unstable_removed = []\n",
    "\n",
    "for feat in selected_features:\n",
    "    fold_count = feature_fold_counts.get(feat, 0)\n",
    "    if fold_count >= PHASE4_CV_STABILITY_MIN_FOLDS:\n",
    "        stable_features.append(feat)\n",
    "    else:\n",
    "        unstable_removed.append((feat, fold_count))\n",
    "\n",
    "print_progress(f\"  {len(stable_features)} features are CV-stable \"\n",
    "               f\"(≥{PHASE4_CV_STABILITY_MIN_FOLDS}/5 folds)\")\n",
    "\n",
    "if unstable_removed:\n",
    "    print_progress(f\"  {len(unstable_removed)} features removed as unstable:\")\n",
    "    for feat, folds in unstable_removed:\n",
    "        print_progress(f\"    ✗ {feat} ({folds}/5 folds)\")\n",
    "else:\n",
    "    print_progress(\"  No features removed (all are CV-stable)\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 4.7: Clinical add-backs\n",
    "# ============================================================================\n",
    "# Add back clinically important features that were removed during SHAP\n",
    "# winnowing but are CV-stable. These features have strong clinical\n",
    "# rationale for CRC risk prediction.\n",
    "#\n",
    "# A feature is only added back if:\n",
    "#   1. It's in PHASE4_CLINICAL_MUST_KEEP\n",
    "#   2. It's NOT already in the selected feature set\n",
    "#   3. It IS CV-stable (≥ PHASE4_CV_STABILITY_MIN_FOLDS folds)\n",
    "# ============================================================================\n",
    "\n",
    "print_progress(\"\")\n",
    "print_progress(\"Step 4.7: Clinical add-backs...\")\n",
    "\n",
    "clinical_added = []\n",
    "clinical_skipped = []\n",
    "\n",
    "for feat in PHASE4_CLINICAL_MUST_KEEP:\n",
    "    if feat in stable_features:\n",
    "        # Already in the set — no action needed\n",
    "        print_progress(f\"  ✓ {feat} — already in selected features\")\n",
    "        continue\n",
    "\n",
    "    fold_count = feature_fold_counts.get(feat, 0)\n",
    "    if fold_count >= PHASE4_CV_STABILITY_MIN_FOLDS:\n",
    "        # CV-stable and not in set — add it back\n",
    "        stable_features.append(feat)\n",
    "        clinical_added.append((feat, fold_count))\n",
    "        print_progress(f\"  + {feat} — added back \"\n",
    "                       f\"({fold_count}/5 folds, CV-stable)\")\n",
    "    else:\n",
    "        # Not CV-stable — skip with explanation\n",
    "        clinical_skipped.append((feat, fold_count))\n",
    "        print_progress(f\"  ✗ {feat} — NOT added \"\n",
    "                       f\"({fold_count}/5 folds, below stability threshold)\")\n",
    "\n",
    "if not PHASE4_CLINICAL_MUST_KEEP:\n",
    "    print_progress(\"  (no clinical must-keep features configured)\")\n",
    "\n",
    "# ============================================================================\n",
    "# Final feature set summary\n",
    "# ============================================================================\n",
    "\n",
    "final_phase4_features = sorted(stable_features)\n",
    "n_final = len(final_phase4_features)\n",
    "\n",
    "# Calculate EPV (events per variable)\n",
    "# n_pos comes from the training data (already computed earlier in the pipeline)\n",
    "train_mask = df_pandas['SPLIT'] == 'train'\n",
    "n_pos = int(df_pandas.loc[train_mask, 'FUTURE_CRC_EVENT'].sum())\n",
    "epv = n_pos / n_final if n_final > 0 else 0\n",
    "\n",
    "print_progress(\"\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"PHASE 4 RESULT: {n_final} FINAL FEATURES\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"  Selected iteration:     {selected_iteration}\")\n",
    "print(f\"  Iteration features:     {len(selected_features)}\")\n",
    "print(f\"  After CV stability:     {len(selected_features) - len(unstable_removed)}\")\n",
    "print(f\"  After clinical adds:    {n_final}\")\n",
    "print(f\"  Events per variable:    {epv:.0f} \"\n",
    "      f\"({n_pos:,} events / {n_final} features)\")\n",
    "print(f\"  Val AUPRC at selection: {selected_val_auprc:.4f}\")\n",
    "print()\n",
    "\n",
    "# Group features by domain for readability\n",
    "domain_map = {}\n",
    "for feat in final_phase4_features:\n",
    "    if feat.startswith('icd_'):\n",
    "        domain = 'ICD-10 Diagnoses'\n",
    "    elif feat.startswith('lab_'):\n",
    "        domain = 'Laboratory'\n",
    "    elif feat.startswith('inp_med_'):\n",
    "        domain = 'Inpatient Medications'\n",
    "    elif feat.startswith('out_med_'):\n",
    "        domain = 'Outpatient Medications'\n",
    "    elif feat.startswith('visit_'):\n",
    "        domain = 'Visit History'\n",
    "    elif feat.startswith('proc_'):\n",
    "        domain = 'Procedures'\n",
    "    elif feat.startswith('vit_'):\n",
    "        domain = 'Vitals'\n",
    "    elif feat in ('months_since_cohort_entry', 'quarters_since_study_start'):\n",
    "        domain = 'Temporal'\n",
    "    else:\n",
    "        domain = 'Demographics'\n",
    "    domain_map.setdefault(domain, []).append(feat)\n",
    "\n",
    "print(\"Features by domain:\")\n",
    "for domain in sorted(domain_map.keys()):\n",
    "    features = domain_map[domain]\n",
    "    print(f\"  {domain} ({len(features)}):\")\n",
    "    for feat in features:\n",
    "        fold_count = feature_fold_counts.get(feat, '?')\n",
    "        source = \"clinical add-back\" if feat in [f for f, _ in clinical_added] else \"winnowing\"\n",
    "        print(f\"    {feat}  (folds: {fold_count}/5, source: {source})\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 4.8: Save final feature list to Spark table\n",
    "# ============================================================================\n",
    "# The output table contains one row per feature with metadata useful for\n",
    "# downstream documentation and model cards.\n",
    "# ============================================================================\n",
    "\n",
    "print_progress(\"\")\n",
    "print_progress(f\"Step 4.8: Saving to {PHASE4_OUTPUT_TABLE}...\")\n",
    "\n",
    "# Build the output dataframe\n",
    "feature_records = []\n",
    "for feat in final_phase4_features:\n",
    "    # Determine domain\n",
    "    if feat.startswith('icd_'):\n",
    "        domain = 'icd_diagnoses'\n",
    "    elif feat.startswith('lab_'):\n",
    "        domain = 'laboratory'\n",
    "    elif feat.startswith('inp_med_'):\n",
    "        domain = 'inpatient_medications'\n",
    "    elif feat.startswith('out_med_'):\n",
    "        domain = 'outpatient_medications'\n",
    "    elif feat.startswith('visit_'):\n",
    "        domain = 'visit_history'\n",
    "    elif feat.startswith('proc_'):\n",
    "        domain = 'procedures'\n",
    "    elif feat.startswith('vit_'):\n",
    "        domain = 'vitals'\n",
    "    elif feat in ('months_since_cohort_entry', 'quarters_since_study_start'):\n",
    "        domain = 'temporal'\n",
    "    else:\n",
    "        domain = 'demographics'\n",
    "\n",
    "    source = 'clinical_add_back' if feat in [f for f, _ in clinical_added] else 'winnowing'\n",
    "    fold_count = feature_fold_counts.get(feat, None)\n",
    "\n",
    "    feature_records.append({\n",
    "        'feature_name': feat,\n",
    "        'domain': domain,\n",
    "        'cv_fold_count': fold_count,\n",
    "        'selection_source': source,\n",
    "        'selected_iteration': selected_iteration,\n",
    "        'parsimony_tolerance_pct': PARSIMONY_TOLERANCE_PCT,\n",
    "        'val_auprc_at_selection': float(selected_val_auprc),\n",
    "    })\n",
    "\n",
    "features_output_df = pd.DataFrame(feature_records)\n",
    "\n",
    "# Write to Spark table\n",
    "spark_features_df = spark.createDataFrame(features_output_df)\n",
    "spark_features_df.write.mode(\"overwrite\").saveAsTable(PHASE4_OUTPUT_TABLE)\n",
    "\n",
    "print_progress(f\"  ✓ Saved {n_final} features to {PHASE4_OUTPUT_TABLE}\")\n",
    "\n",
    "# Also save locally for reference\n",
    "local_path = os.path.join(OUTPUT_DIR, \"lucem_novis_standard_phase4_final_features.json\")\n",
    "with open(local_path, 'w') as f:\n",
    "    json.dump({\n",
    "        'selected_iteration': selected_iteration,\n",
    "        'parsimony_tolerance_pct': PARSIMONY_TOLERANCE_PCT,\n",
    "        'gap_threshold': PHASE4_GAP_THRESHOLD,\n",
    "        'cv_stability_min_folds': PHASE4_CV_STABILITY_MIN_FOLDS,\n",
    "        'best_val_auprc': float(best_val_auprc),\n",
    "        'selected_val_auprc': float(selected_val_auprc),\n",
    "        'n_features': n_final,\n",
    "        'epv': float(epv),\n",
    "        'features': final_phase4_features,\n",
    "        'clinical_added': [f for f, _ in clinical_added],\n",
    "        'unstable_removed': [f for f, _ in unstable_removed],\n",
    "    }, f, indent=2)\n",
    "print_progress(f\"  ✓ Also saved to {local_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 4 Conclusion\n",
    "\n",
    "Phase 4 applied an automated, parsimony-aware feature selection:\n",
    "\n",
    "1. **Overfitting guard**: Excluded iterations with |train-val gap| ≥ 0.02\n",
    "2. **Parsimony tolerance**: Found all iterations within 10% of the best val AUPRC, then selected the one with the **fewest features** (Occam's razor)\n",
    "3. **CV stability filter**: Removed features appearing in < 3/5 cross-validation folds\n",
    "4. **Clinical add-backs**: Restored `lab_HEMOGLOBIN_ACCELERATING_DECLINE` (accelerating hemoglobin decline — hallmark of occult GI bleeding)\n",
    "\n",
    "The result is a compact, robust feature set with high EPV (events per variable), aligned with published CRC risk model standards of 8-30 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Phase 5: Production Model Training\n",
    "---\n",
    "\n",
    "### What This Section Does\n",
    "\n",
    "Retrains an XGBoost model using the **Phase 4 final feature set** with **production-ready\n",
    "hyperparameters**. The Phase 1-3 winnowing used ultra-conservative parameters (max_depth=2,\n",
    "gamma=2.0) to prevent overfitting during feature elimination. Now that features are finalized,\n",
    "we relax the parameters to allow the model to learn more complex patterns.\n",
    "\n",
    "**Production vs. Winnowing parameters:**\n",
    "\n",
    "| Parameter | Winnowing (Phases 1-3) | Production (Phase 5) | Rationale |\n",
    "|-----------|----------------------|---------------------|-----------|\n",
    "| max_depth | 2-3 | 4 | Allow deeper interactions |\n",
    "| gamma | 2.0 | 1.0 | Less aggressive pruning |\n",
    "| subsample | 0.3-0.5 | 0.6 | Use more data per tree |\n",
    "| colsample_bytree | 0.3-0.5 | 0.6 | Use more features per tree |\n",
    "| reg_alpha (L1) | 5.0 | 2.0 | Lighter regularization |\n",
    "| reg_lambda (L2) | 50.0 | 10.0 | Lighter regularization |\n",
    "| learning_rate | 0.005 | 0.005 | Same (slow learning) |\n",
    "\n",
    "**Metrics reported:**\n",
    "- **AUPRC** (primary) — most informative metric at 250:1 imbalance\n",
    "- **AUROC** — discrimination ability\n",
    "- **Brier score** — calibration quality\n",
    "- **Lift @ top 1%** — clinical utility (how many × baseline CRC rate in highest-risk patients)\n",
    "- **SHAP importances** — feature contribution ranking"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PHASE 5: PRODUCTION MODEL TRAINING & EVALUATION\n",
    "# ============================================================================\n",
    "# Train the final model with production-ready XGBoost parameters on the\n",
    "# Phase 4 feature set. Evaluate on train, validation, and test splits.\n",
    "# ============================================================================\n",
    "\n",
    "print_stage(\"PHASE 5: PRODUCTION MODEL TRAINING\")\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Step 5.1: Production hyperparameters\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# These are relaxed from the ultra-conservative winnowing parameters.\n",
    "# The winnowing phase needed conservative params to prevent overfitting\n",
    "# during iterative feature removal. Now that features are finalized,\n",
    "# we can allow more model complexity.\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "PRODUCTION_PARAMS = {\n",
    "    'max_depth': 4,              # Deeper trees (was 2-3 in winnowing)\n",
    "    'min_child_weight': 50,      # Same — require substantial leaf support\n",
    "    'gamma': 1.0,                # Relaxed (was 2.0) — less aggressive pruning\n",
    "    'subsample': 0.6,            # More data per tree (was 0.3-0.5)\n",
    "    'colsample_bytree': 0.6,    # More features per tree (was 0.3-0.5)\n",
    "    'colsample_bylevel': 0.5,   # Same per-level sampling\n",
    "    'reg_alpha': 2.0,            # Lighter L1 (was 5.0)\n",
    "    'reg_lambda': 10.0,          # Lighter L2 (was 50.0)\n",
    "    'learning_rate': 0.005,      # Same slow learning rate\n",
    "    'n_estimators': 3000,        # More trees allowed (early stopping will control)\n",
    "    'early_stopping_rounds': 150,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'aucpr',      # Monitor AUPRC during training\n",
    "    'tree_method': 'hist',       # Fast histogram-based method\n",
    "    'random_state': RANDOM_SEED,\n",
    "    'verbosity': 0,\n",
    "}\n",
    "\n",
    "print(\"Production XGBoost Parameters:\")\n",
    "for param, value in PRODUCTION_PARAMS.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print()\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Step 5.2: Prepare data splits\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Use df_pandas (loaded in Phase 1) with the Phase 4 final feature set.\n",
    "# The SPLIT column defines train/val/test partitions.\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "print_progress(\"Step 5.2: Preparing data splits...\")\n",
    "\n",
    "# Extract features + target for each split\n",
    "train_mask = df_pandas['SPLIT'] == 'train'\n",
    "val_mask = df_pandas['SPLIT'] == 'val'\n",
    "test_mask = df_pandas['SPLIT'] == 'test'\n",
    "\n",
    "X_train = df_pandas.loc[train_mask, final_phase4_features].copy()\n",
    "y_train = df_pandas.loc[train_mask, 'FUTURE_CRC_EVENT'].copy()\n",
    "\n",
    "X_val = df_pandas.loc[val_mask, final_phase4_features].copy()\n",
    "y_val = df_pandas.loc[val_mask, 'FUTURE_CRC_EVENT'].copy()\n",
    "\n",
    "X_test = df_pandas.loc[test_mask, final_phase4_features].copy()\n",
    "y_test = df_pandas.loc[test_mask, 'FUTURE_CRC_EVENT'].copy()\n",
    "\n",
    "# Compute scale_pos_weight from training data\n",
    "n_neg_train = int((y_train == 0).sum())\n",
    "n_pos_train = int((y_train == 1).sum())\n",
    "prod_scale_pos_weight = n_neg_train / n_pos_train\n",
    "\n",
    "print_progress(f\"  Train: {len(y_train):>8,} obs, {n_pos_train:>5,} events \"\n",
    "               f\"({n_pos_train/len(y_train)*100:.3f}%)\")\n",
    "print_progress(f\"  Val:   {len(y_val):>8,} obs, {int(y_val.sum()):>5,} events \"\n",
    "               f\"({y_val.mean()*100:.3f}%)\")\n",
    "print_progress(f\"  Test:  {len(y_test):>8,} obs, {int(y_test.sum()):>5,} events \"\n",
    "               f\"({y_test.mean()*100:.3f}%)\")\n",
    "print_progress(f\"  scale_pos_weight: {prod_scale_pos_weight:.1f}\")\n",
    "print_progress(f\"  Features: {len(final_phase4_features)}\")\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Step 5.3: Train production model\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "print_progress(\"\")\n",
    "print_progress(\"Step 5.3: Training production XGBoost model...\")\n",
    "train_start = time.time()\n",
    "\n",
    "prod_model = XGBClassifier(\n",
    "    scale_pos_weight=prod_scale_pos_weight,\n",
    "    **PRODUCTION_PARAMS\n",
    ")\n",
    "\n",
    "prod_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "train_time = time.time() - train_start\n",
    "n_trees = prod_model.best_iteration + 1 if prod_model.best_iteration else prod_model.n_estimators\n",
    "\n",
    "print_progress(f\"  ✓ Training complete in {train_time:.1f}s\")\n",
    "print_progress(f\"  Trees: {n_trees} \"\n",
    "               f\"(best iteration: {prod_model.best_iteration})\")\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Step 5.4: Evaluate on all splits\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Compute AUPRC, AUROC, Brier score, and Lift @ top 1% for each split.\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "print_progress(\"\")\n",
    "print_progress(\"Step 5.4: Evaluating on all splits...\")\n",
    "\n",
    "def evaluate_split(model, X, y, split_name):\n",
    "    \"\"\"Compute all metrics for one data split.\"\"\"\n",
    "    y_proba = model.predict_proba(X)[:, 1]\n",
    "\n",
    "    # Core metrics\n",
    "    auprc = average_precision_score(y, y_proba)\n",
    "    auroc = roc_auc_score(y, y_proba)\n",
    "    brier = brier_score_loss(y, y_proba)\n",
    "\n",
    "    # Lift @ top 1%: What's the CRC rate in the highest-risk 1% of patients,\n",
    "    # compared to the baseline rate?\n",
    "    n_top = max(1, int(len(y) * 0.01))\n",
    "    top_indices = np.argsort(y_proba)[::-1][:n_top]\n",
    "    top_positive_rate = y.iloc[top_indices].mean()\n",
    "    baseline_rate = y.mean()\n",
    "    lift_1pct = top_positive_rate / baseline_rate if baseline_rate > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'split': split_name,\n",
    "        'auprc': auprc,\n",
    "        'auroc': auroc,\n",
    "        'brier': brier,\n",
    "        'lift_1pct': lift_1pct,\n",
    "        'n_obs': len(y),\n",
    "        'n_events': int(y.sum()),\n",
    "        'baseline_rate': baseline_rate,\n",
    "        'top_1pct_rate': top_positive_rate,\n",
    "    }\n",
    "\n",
    "# Evaluate each split\n",
    "train_metrics = evaluate_split(prod_model, X_train, y_train, 'Train')\n",
    "val_metrics = evaluate_split(prod_model, X_val, y_val, 'Validation')\n",
    "test_metrics = evaluate_split(prod_model, X_test, y_test, 'Test')\n",
    "\n",
    "# Display results in a clean table\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"PRODUCTION MODEL PERFORMANCE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Metric':<20} {'Train':>10} {'Validation':>12} {'Test':>10}\")\n",
    "print(\"-\" * 52)\n",
    "print(f\"{'AUPRC':<20} {train_metrics['auprc']:>10.4f} \"\n",
    "      f\"{val_metrics['auprc']:>12.4f} {test_metrics['auprc']:>10.4f}\")\n",
    "print(f\"{'AUROC':<20} {train_metrics['auroc']:>10.4f} \"\n",
    "      f\"{val_metrics['auroc']:>12.4f} {test_metrics['auroc']:>10.4f}\")\n",
    "print(f\"{'Brier Score':<20} {train_metrics['brier']:>10.4f} \"\n",
    "      f\"{val_metrics['brier']:>12.4f} {test_metrics['brier']:>10.4f}\")\n",
    "print(f\"{'Lift @ Top 1%':<20} {train_metrics['lift_1pct']:>10.1f}x \"\n",
    "      f\"{val_metrics['lift_1pct']:>11.1f}x {test_metrics['lift_1pct']:>9.1f}x\")\n",
    "print(\"-\" * 52)\n",
    "print(f\"{'Observations':<20} {train_metrics['n_obs']:>10,} \"\n",
    "      f\"{val_metrics['n_obs']:>12,} {test_metrics['n_obs']:>10,}\")\n",
    "print(f\"{'Events':<20} {train_metrics['n_events']:>10,} \"\n",
    "      f\"{val_metrics['n_events']:>12,} {test_metrics['n_events']:>10,}\")\n",
    "print(f\"{'Baseline Rate':<20} {train_metrics['baseline_rate']:>10.4f} \"\n",
    "      f\"{val_metrics['baseline_rate']:>12.4f} {test_metrics['baseline_rate']:>10.4f}\")\n",
    "\n",
    "# Overfitting check\n",
    "train_val_gap = train_metrics['auprc'] - val_metrics['auprc']\n",
    "print()\n",
    "print(f\"Train-Val Gap: {train_val_gap:+.4f}\", end=\"\")\n",
    "if abs(train_val_gap) < 0.02:\n",
    "    print(\"  (acceptable — no significant overfitting)\")\n",
    "elif train_val_gap > 0:\n",
    "    print(\"  (⚠ training exceeds validation — possible overfitting)\")\n",
    "else:\n",
    "    print(\"  (validation exceeds training — unusual but not problematic)\")\n",
    "\n",
    "# Save metrics\n",
    "prod_metrics_path = os.path.join(OUTPUT_DIR, \"lucem_novis_standard_phase5_production_metrics.json\")\n",
    "with open(prod_metrics_path, 'w') as f:\n",
    "    json.dump({\n",
    "        'train': train_metrics,\n",
    "        'validation': val_metrics,\n",
    "        'test': test_metrics,\n",
    "        'train_val_gap': float(train_val_gap),\n",
    "        'n_trees': n_trees,\n",
    "        'n_features': len(final_phase4_features),\n",
    "        'production_params': {k: v for k, v in PRODUCTION_PARAMS.items()\n",
    "                              if k != 'random_state'},\n",
    "    }, f, indent=2, default=str)\n",
    "print_progress(f\"  Metrics saved to {prod_metrics_path}\")\n",
    "\n",
    "# Save production model\n",
    "prod_model_path = os.path.join(OUTPUT_DIR, \"lucem_novis_standard_phase5_production_model.json\")\n",
    "prod_model.save_model(prod_model_path)\n",
    "print_progress(f\"  Model saved to {prod_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 5.5: SHAP Feature Importance Analysis\n",
    "# ============================================================================\n",
    "# Compute SHAP values on the validation set to understand feature\n",
    "# contributions in the production model. This serves two purposes:\n",
    "#   1. Verify that the model is using features as expected (sanity check)\n",
    "#   2. Provide interpretable feature rankings for clinical documentation\n",
    "#\n",
    "# We use the validation set (not training) to avoid inflated importance\n",
    "# from memorized patterns.\n",
    "# ============================================================================\n",
    "\n",
    "print_progress(\"\")\n",
    "print_progress(\"Step 5.5: Computing SHAP importance on validation set...\")\n",
    "shap_start = time.time()\n",
    "\n",
    "# TreeExplainer is exact for tree-based models (no approximation needed)\n",
    "explainer = shap.TreeExplainer(prod_model)\n",
    "shap_values = explainer.shap_values(X_val)\n",
    "\n",
    "shap_time = time.time() - shap_start\n",
    "print_progress(f\"  ✓ SHAP computation complete in {shap_time:.1f}s\")\n",
    "\n",
    "# Compute mean absolute SHAP value per feature (standard importance measure)\n",
    "mean_abs_shap = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "# Build importance ranking\n",
    "shap_importance = pd.DataFrame({\n",
    "    'feature': final_phase4_features,\n",
    "    'mean_abs_shap': mean_abs_shap,\n",
    "}).sort_values('mean_abs_shap', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Add rank and cumulative importance\n",
    "shap_importance['rank'] = range(1, len(shap_importance) + 1)\n",
    "total_importance = shap_importance['mean_abs_shap'].sum()\n",
    "shap_importance['pct_of_total'] = (\n",
    "    shap_importance['mean_abs_shap'] / total_importance * 100\n",
    ")\n",
    "shap_importance['cumulative_pct'] = shap_importance['pct_of_total'].cumsum()\n",
    "\n",
    "# Display the ranking\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"SHAP FEATURE IMPORTANCE (Production Model, Validation Set)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Rank':<5} {'Feature':<45} {'Mean |SHAP|':>11} {'% Total':>8} {'Cum %':>6}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for _, row in shap_importance.iterrows():\n",
    "    print(f\"{int(row['rank']):<5} {row['feature']:<45} \"\n",
    "          f\"{row['mean_abs_shap']:>11.6f} \"\n",
    "          f\"{row['pct_of_total']:>7.1f}% \"\n",
    "          f\"{row['cumulative_pct']:>5.1f}%\")\n",
    "\n",
    "# Save SHAP importance\n",
    "shap_path = os.path.join(OUTPUT_DIR, \"lucem_novis_standard_phase5_shap_importance.csv\")\n",
    "shap_importance.to_csv(shap_path, index=False)\n",
    "print_progress(f\"  SHAP importance saved to {shap_path}\")\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# SHAP summary plot (beeswarm)\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Visual summary of how each feature pushes predictions up or down.\n",
    "# Red = high feature value, blue = low feature value.\n",
    "# Features sorted by importance (most important at top).\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "print_progress(\"Generating SHAP summary plot...\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, max(6, len(final_phase4_features) * 0.35)))\n",
    "shap.summary_plot(\n",
    "    shap_values, X_val,\n",
    "    feature_names=final_phase4_features,\n",
    "    show=False,\n",
    "    max_display=len(final_phase4_features),  # Show all features\n",
    ")\n",
    "plt.title(\"SHAP Feature Importance — Production Model\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "\n",
    "shap_plot_path = os.path.join(OUTPUT_DIR, \"lucem_novis_standard_phase5_shap_summary.png\")\n",
    "plt.savefig(shap_plot_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print_progress(f\"  Plot saved to {shap_plot_path}\")\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Final pipeline summary\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"COMPLETE PIPELINE SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\"\"\n",
    "  Phase 1 (Clustering):     167 → {features_by_iteration['phase1_clustered']['n_features']} features\n",
    "  Phase 2 (SHAP Winnowing): {features_by_iteration['phase1_clustered']['n_features']} → {features_by_iteration[f'phase2_iter{phase2_df[\"iteration\"].max()}_output']['n_features']} features ({phase2_df['iteration'].max()} iterations)\n",
    "  Phase 3 (CV Stability):   5-fold cross-validation stability analysis\n",
    "  Phase 4 (Parsimony):      Selected iter {selected_iteration} → {n_final} features\n",
    "  Phase 5 (Production):     Retrained with production params\n",
    "\n",
    "  Final Features:  {n_final}\n",
    "  EPV:             {epv:.0f}\n",
    "  Val AUPRC:       {val_metrics['auprc']:.4f}\n",
    "  Test AUPRC:      {test_metrics['auprc']:.4f}\n",
    "  Lift @ Top 1%:   {test_metrics['lift_1pct']:.1f}x (test)\n",
    "\n",
    "  Output table:    {PHASE4_OUTPUT_TABLE}\n",
    "  Model file:      {prod_model_path}\n",
    "\"\"\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 5 Conclusion\n",
    "\n",
    "The production model was trained on the Phase 4 feature set with relaxed hyperparameters\n",
    "suitable for a final model (max_depth=4, gamma=1.0). Key outputs:\n",
    "\n",
    "- **Performance metrics** on train/val/test splits (AUPRC, AUROC, Brier, Lift)\n",
    "- **SHAP feature importance** ranking with beeswarm visualization\n",
    "- **Production model** saved for downstream calibration and deployment\n",
    "- **Feature list table** saved to Spark for pipeline integration\n",
    "\n",
    "**Next steps:**\n",
    "1. Isotonic calibration (predict_proba → calibrated 0-100 risk score)\n",
    "2. Threshold selection for clinical decision rules\n",
    "3. External validation on held-out time period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cf7a089-94a3-4a8f-9745-3343246c4c3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Utility: Clear Checkpoints (Run Manually if Needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42b91833-ab73-4f5b-ab25-f66078a18ff7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Uncomment and run this cell to clear all checkpoints and start fresh\n",
    "# clear_checkpoints()\n",
    "# print(\"Checkpoints cleared. Re-run notebook to start fresh.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62e60843-8001-4842-8fd1-300ead3ad4c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2f78114-3db5-4ad2-9cfc-b9538156652c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd# Set the option to display all rows\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "df_check_spark = spark.sql('select * from dev.clncl_ds.herald_eda_train_wide')\n",
    "df_check = df_check_spark.toPandas()\n",
    "df_check.isnull().sum()/df_check.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e287620d-82f1-48fd-a4bf-bc73bc9950f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "df_check_spark = spark.sql('select * from dev.clncl_ds.herald_train')\n",
    "df_check = df_check_spark.toPandas()\n",
    "df_check.isnull().sum()/df_check.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f9ef5e1-239d-4b2b-975e-98633960df8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3460118b-1d7a-460e-bbd3-0715ce4dbe79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "V2_Book9_Feature_Selection",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}