You are an expert ML and data science assistant specializing in clinical risk prediction using high-dimensional EHR data, extreme class imbalance, and tree-based models (especially XGBoost). I will describe our current methodology for feature engineering and model training for rare medical outcomes (prevalence ~0.4%), and I want you to help me **redesign and tighten this pipeline** according to a set of explicit requirements.

This prompt will be used both in **production work** and as part of an internal **tutorial** for data scientists who may not yet have deep experience with EHR modeling, temporal validation, or rare-outcome prediction.

Your tasks:

1. Carefully understand the existing process and my constraints.
2. Propose a **revised, detailed methodological pipeline** that:
   - Strictly avoids information leakage (especially in feature selection and composite feature construction).
   - Is optimized for very rare outcomes and XGBoost.
   - Respects clinical interpretability and our current feature engineering patterns.
3. Output:
   - A high-level conceptual description of the new process.
   - A concrete, step-by-step “recipe” I can actually implement (SQL/ETL + Python/XGBoost, logically described).
   - Explicit checks and diagnostics I should run to validate that the new regime is working correctly.
   - Clear explanations of *why* each major design choice is made, so the pipeline can be used as a teaching example.

When you respond, assume the reader is a practicing data scientist/ML engineer in healthcare who understands basic ML concepts, but may be less familiar with temporal-leakage issues, rare-event modeling, and EHR-specific pitfalls. Be explicit, pedagogical, and operational: explain reasoning, give small concrete examples where helpful, and connect steps back to practical modeling and deployment.

---

## Background and Current Setting

We build prediction models for relatively rare clinical outcomes (e.g., incident disease in a defined horizon) using EHR data. Key points:

- Outcome prevalence is typically on the order of **0.4%** (or similarly low).
- We use **XGBoost** as the primary modeling algorithm.
- We have many structured EHR data domains, including but not limited to:
  - Medications (outpatient and inpatient)
  - Labs
  - Vitals
  - Diagnoses (e.g., ICD-10)
  - Procedures
  - Visit history, encounter types, and other utilization variables
- Many fields are **missing or irregularly observed**; some “missingness” is actually informative (e.g., never had a test vs. no data in system vs. new patient).

We already do:

- Temporal controls to enforce that all features are derived from data **prior to a prediction time**.
- Use of **lookback windows** (e.g., 24 months, 12 months) to define historical periods for feature engineering.
- XGBoost-specific handling of imbalance (e.g., `scale_pos_weight`, class weighting/sampling).
- Calibration post-hoc (e.g., Platt or isotonic) using held-out data.

I want to improve the rigor and structure of the process, especially regarding temporal splits, feature selection, feature representation choices, and governance of composite features.

---

## Current Methodology (Existing Regime)

### 1. Feature Engineering Methodology

**1.1 Data Extraction and Temporal Controls**

- For each patient and prediction time, we define a clear **prediction point** (`END_DTTM` or equivalent).
- We define **lookback windows** (e.g., last 24 months, last 12 months, etc.) and only use events that:
  - Occur **before** `END_DTTM`, and
  - Fall within the chosen window(s).
- We enforce **data availability constraints**: only periods where data are known to be reliable are included.
- For training, we sometimes randomize the **exact day of month of END_DTTM per patient** to simulate different calculation days (within constraints) while preserving temporal ordering.

**1.2 Clinical Category Definition**

Across each data domain (meds, labs, vitals, diagnoses, procedures, encounters, etc.), we define clinically meaningful **categories** or groups of events. These may be based on:

- Clinical knowledge and local coding practices.
- Existing institutional groupers or value sets.
- Focused query-driven lists of relevant codes, tests, or concepts.

The key idea is that for each domain we have a set of categories (e.g., specific lab concepts or lab groups, diagnosis groups, procedure groups) that we treat as units for feature engineering.

**1.3 Initial Feature Creation**

For each clinical category in each domain, we generate three main feature types within the lookback window:

- **Binary Flags**:
  - 1/0 indicator of whether the patient **ever had** at least one event in the lookback window for that category.
  - Examples: `category_X_ever_flag`, `category_X_abnormal_flag` (if applicable).

- **Recency Measures (`days_since`)**:
  - `days_since_last_<category_event>` = difference between `END_DTTM` and the most recent event in the lookback window.
  - Examples: `days_since_last_category_X_event`, `days_since_last_abnormal_category_X_event`.

- **Frequency Counts (`count_Xyr`)**:
  - Total number of times events in that category occurred within the window.
  - Examples: `category_X_event_count_1yr`, `category_X_event_count_2yr`.

**1.4 Informative Missingness**

- For `days_since` features, **NULL/absent** is treated as **“never had the event in the window”**, not as generic missingness to impute.
- This is compatible with XGBoost’s ability to route missing values in splits.

---

### 2. Feature Transformation Methodology

**2.1 Pivoting to Wide Format**

- Start from an event-level table (rows = individual events in any domain).
- Transform into a **wide patient-observation × feature matrix**:
  - One row per patient-observation (patient + END_DTTM).
  - Columns for each engineered feature:
    - e.g., `category_X_flag`, `category_X_days_since`, `category_X_count`, etc.

- Use SQL window functions (or equivalent) to compute recency / counts efficiently, constrained by the lookback window.

**2.2 Temporal Calculations**

- Compute:
  - `days_since` as precise date differences (e.g., `DATEDIFF(END_DTTM, event_date)`).
  - `count` as `COUNT(*) OVER (PARTITION BY patient_id, END_DTTM, category)` or equivalent, constrained to the lookback window.

**2.3 Quality Controls**

- Check:
  - **Row count preservation** (no unintended loss of patient-observations).
  - **Temporal integrity**: all events used are strictly before `END_DTTM` and within lookback windows.
  - **Prevalence and usage patterns** of features vs. clinical expectations (sanity checks).

---

### 3. Feature Reduction Methodology

**3.1 Motivation**

- We generate a very large feature set across many domains. To reduce computation and risk of overfitting/noise, we do feature reduction.

**3.2 Multi-Metric Evaluation (Outcome-Aware, Univariate)**

We evaluate each feature using:

- **Risk Ratios**:
  - Compare outcome risk when feature present vs absent.
  - Provides clinically interpretable magnitude of association.

- **Mutual Information (MI)**:
  - Measures (possibly nonlinear) relationship between feature and outcome.
  - Captures patterns beyond simple risk ratios.

- **Impact Scores**:
  - Combine prevalence with effect size (e.g., risk ratio × prevalence).
  - Ensures we capture both:
    - Rare but high-impact features, and
    - Common but moderately predictive features.

These are currently computed in an outcome-aware way. The goal in the new regime is to do this **inside training only** (no leakage).

**3.3 Clinical Prioritization and Filtering**

- **Domain knowledge filters**:
  - Clinical experts specify “must-keep” features that are retained regardless of univariate stats (e.g., well-known risk factors, core utilization measures, key lab abnormalities, etc.).

- **Near-Zero Variance Removal**:
  - Remove features with extremely low prevalence or almost no variation, unless they pass domain-knowledge or “rare but clearly important” checks.
  - We want this refined so we:
    - Do not lose genuinely important rare signals.
    - Still get rid of meaningless almost-always-zero features.

**3.4 Optimal Representation Selection (Flag vs Recency vs Count)**

For each clinical category, we choose which representation(s) to keep:

- **Critical signals**:
  - For categories with extremely high risk associations (e.g., known high-risk diagnoses, procedures, or abnormal findings), we might keep:
    - A flag **and**
    - A recency feature to capture temporal urgency.

- **High-value indicators**:
  - For some categories, a binary “ever had” flag may be sufficient.

- **Very common features**:
  - A simple binary flag might be kept mainly for interpretability.

- **General case**:
  - Among flag / recency / count, we might select the representation with the **highest MI** or best performance in simple screening models, subject to clinical sanity checks.

**3.5 Composite Feature Creation**

We create composite features that combine multiple base features into clinically meaningful patterns, such as:

- Groupings of related medications.
- Combined patterns of symptoms, diagnoses, and treatments.
- Patterns reflecting chronicity and intensity across multiple related categories.
- Decay-style scores using `days_since` (e.g., exponential decay functions) to emphasize recency.

These composites can be more powerful and reduce dimensionality vs. many individual atomic features.

**3.6 Final Validation of Reduced Feature Set**

- Confirm that:
  - Key known risk factors and high-risk patterns are preserved across all domains.
  - The dimensionality has been substantially reduced (e.g., 60%+), where appropriate.
  - Performance on validation is not degraded vs. a more complete feature set.

---

## Current Gaps / Desired Changes (“New Regime”)

I want you to **redesign this pipeline** with the following explicit requirements and improvements:

### A. Temporal Splitting and Leakage Control

1. **Train/validation/test splits must be purely temporal**:
   - Train on older data, validate on intermediate time, test on the most recent time.
   - If multiple prediction times per patient, ensure that:
     - All training observations for that patient occur **before** any test observations in real time.

2. **All outcome-aware feature-selection steps (risk ratios, MI, impact, representation choice, composite tuning) must operate *only on training data***:
   - For each temporal split or CV fold:
     - Compute univariate metrics **only using that fold’s training set**.
     - Decide which features to keep/remove based only on training.
   - No information from validation/test periods may influence:
     - Feature filtering thresholds,
     - Composite feature definitions,
     - Representation choices.

3. At final model training time (after hyperparameter tuning), the univariate metrics and selections can be recomputed on the **full training period only** (excluding the held-out test period).

### B. Handling Rare Outcomes and Feature Reduction

4. **Relax over-aggressive prefiltering**, especially for rare but important features:
   - Let XGBoost’s regularization and tree structure handle a lot of feature selection.
   - Use near-zero variance removal only when:
     - There are truly negligible numbers of positive cases with that feature, or
     - The feature is structurally meaningless/noisy.
   - Define concrete thresholds and rules:
     - Example: keep any feature with at least N positive-outcome patients having that feature (N tuned to dataset size), regardless of overall prevalence.

5. **Use XGBoost’s strengths explicitly**:
   - Document and propose a hyperparameter strategy suitable for:
     - High-dimensional, sparse features,
     - Extreme class imbalance,
     - Strong regularization (e.g., `min_child_weight`, `gamma`, `lambda`, `alpha`, `max_depth`, `subsample`, `colsample_bytree`).
   - Keep dimensionality reduction, but bias towards **retaining more features** and relying on model regularization.

### C. Missingness, Data Density, and `days_since` Encoding

6. Improve representation of “never observed” vs “not measured yet” vs “no data in system” by:
   - Adding meta-features such as:
     - `history_length_days` (time from first recorded event to `END_DTTM`),
     - `num_encounters_last_X_years`,
     - `num_lab_events_last_X_years`,
     - `num_med_events_last_X_years`,
     - or analogous utilization/coverage measures per domain.
   - These should be used so the model can distinguish between:
     - Long, dense EHR history with many opportunities to observe,
     - Short or sparse history where absence is less meaningful.

7. Refine `days_since` encoding:
   - For each category, create:
     - An explicit **“ever had” binary flag** (0/1) indicating at least one event in the window.
     - A `days_since_last_event` feature that is:
       - Defined only when “ever had” = 1, or
       - Provided with missing values that XGBoost can route appropriately.
   - Ensure clear semantics:
     - `ever_event_flag = 0` → patient truly never had the event within the lookback.
     - `ever_event_flag = 1` and `days_since` meaningful positive number.
   - Document how this is implemented in SQL/ETL and passed into the model.

### D. Composite Features and Governance

8. **Governance for composite features**:
   - Composites should be:
     - Pre-specified clinically (e.g., based on literature or expert consensus), OR
     - Generated using clear, data-driven rules **in training data only**, and then fixed.
   - Avoid “fishing” for composites directly on all data including future test periods.

9. Avoid **unnecessary double-counting**:
   - If a composite is defined (e.g., a composite capturing a cluster of related categories and their recency), consider whether:
     - Some or all of its components can be dropped from the model, or
     - They should be retained only when they have independent value.
   - Provide clear rules for when to:
     - Keep both composite and components,
     - Replace components with the composite,
     - Or keep only components.

### E. Evaluation, Calibration, and Thresholding

10. Propose a **full evaluation protocol** tailored for rare outcomes:
    - Primary metrics:
      - PR-AUC,
      - Recall at fixed precision (e.g., PPV >= X%),
      - Brier score,
      - Calibration plots, etc.
    - Secondary metrics:
      - ROC-AUC, etc., but not as the sole decision driver.

11. **Calibration**:
    - Use temporally-separated calibration sets or cross-validation.
    - Apply:
      - Logistic/Platt scaling or isotonic regression to XGBoost scores.
      - Evaluate calibration across deciles or risk bins.

12. **Threshold selection**:
    - Provide a principled approach to choose operating thresholds:
      - Based on clinical utility (e.g., decision curve analysis, expected benefit/harm),
      - Or constraints (e.g., target PPV or sensitivity).

---

## What I Want From You

Given everything above:

1. **Design a revised end-to-end methodology** that explicitly incorporates:
   - Temporal splitting before any outcome-aware feature selection.
   - Within-training-only computation of all outcome-linked statistics used to decide which features to keep.
   - Refined near-zero variance logic for rare features.
   - Explicit meta-features for data density/coverage and refined `days_since` encoding with “ever had” flags.
   - A governance framework for composite features.
   - XGBoost hyperparameter and training strategy appropriate for this setting.
   - Proper evaluation and calibration procedures for rare outcomes.

2. Present your answer in the following structure:
   - **Section 1: High-Level Overview**
     - A concise narrative of the new regime, in order, from raw data to calibrated model.
   - **Section 2: Detailed Step-by-Step Pipeline**
     - For each major stage (cohort building, temporal splits, feature engineering, feature reduction, composite construction, model training, calibration, evaluation), provide:
       - Inputs,
       - Operations,
       - Outputs,
       - Key parameters/choices,
       - Where exactly to enforce “train-only” computations.
   - **Section 3: Implementation Notes**
     - Practical advice for:
       - SQL/ETL design (especially for `days_since`, counts, and meta-features),
       - Python/XGBoost training loop structure (including CV, hyperparameter tuning, and embedding feature-selection logic into the pipeline).
   - **Section 4: Diagnostics and Sanity Checks**
     - Specific checks I should run (with examples) to:
       - Detect leakage,
       - Confirm that rare but important features are retained,
       - Verify calibration quality,
       - Confirm temporal validity (no future data used).
   - **Section 5: Optional Enhancements**
     - Any further, realistic improvements that fit this paradigm (e.g., survival-style framing if needed, dynamic prediction, robust calibration under drift), but keep them clearly labeled as optional.

3. Be **very explicit** about:
   - Where data is split in time,
   - Which computations are allowed to “see” which parts of the data,
   - How to avoid subtle leakage in:
     - Univariate feature selection,
     - Composite feature construction,
     - Threshold selection and calibration.

If you need to make reasonable assumptions (e.g., approximate sample sizes, lookback lengths, or typical numbers of features), state them briefly and proceed. Your output should be detailed enough that I can directly translate it into concrete code and SQL changes in our existing pipeline.